{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM2mwxbOyw6V",
        "outputId": "b4363215-64bb-47af-bc4d-31317cb1447a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New Zealand/Wave'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pip==23.3.1 #needs pip < 24.1 for whatever\n",
        "!pip install fairseq\n",
        "!git clone https://github.com/facebookresearch/fairseq.git"
      ],
      "metadata": {
        "id": "EhHp46HnznQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9b217d-629c-4da0-e09c-81d46f86bce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip==23.3.1\n",
            "  Downloading pip-23.3.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-23.3.1\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.11)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.26.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2024.9.11)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.6)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.5.1+cu121)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.12.2)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (5.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->fairseq) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (3.0.2)\n",
            "Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11288619 sha256=ea7cb98fb43f6578eea4c8ab58d5b2f73cee52d7e3a8f71f225c766494a04222\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=f3eab9924a28f90b538d6e3d167279e4e8cc32642e36f13ee29f15785be5f1b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-3.0.0 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-3.0.0 sacrebleu-2.4.3\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 35385, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 35385 (delta 10), reused 38 (delta 9), pack-reused 35337 (from 1)\u001b[K\n",
            "Receiving objects: 100% (35385/35385), 25.47 MiB | 36.28 MiB/s, done.\n",
            "Resolving deltas: 100% (25538/25538), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!apt-get install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MNHhBw2n0RxS",
        "outputId": "057cb68a-f6ad-49fc-8470-16bc0719e247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #down load vocoder\n",
        "# !wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/g_00500000\n",
        "# !wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iWdwqxV-71pt",
        "outputId": "a2602391-d994-422d-ef85-d9b8c5a8aebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-11 21:23:37--  https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/g_00500000\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.244.202.25, 18.244.202.62, 18.244.202.103, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.244.202.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53590413 (51M) [binary/octet-stream]\n",
            "Saving to: ‘g_00500000’\n",
            "\n",
            "g_00500000          100%[===================>]  51.11M   100MB/s    in 0.5s    \n",
            "\n",
            "2024-12-11 21:23:38 (100 MB/s) - ‘g_00500000’ saved [53590413/53590413]\n",
            "\n",
            "--2024-12-11 21:23:38--  https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/config.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.244.202.25, 18.244.202.62, 18.244.202.103, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.244.202.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1403 (1.4K) [application/json]\n",
            "Saving to: ‘config.json’\n",
            "\n",
            "config.json         100%[===================>]   1.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-11 21:23:38 (937 MB/s) - ‘config.json’ saved [1403/1403]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download hubert base + quantizer\n",
        "!wget https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\n",
        "!wget https://dl.fbaipublicfiles.com/textless_nlp/gslm/hubert/km100/km.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q-Qf5kX5DOEr",
        "outputId": "f4ff93fc-0640-4883-d82e-bf2ef8c86691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-11 21:23:38--  https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.244.202.25, 18.244.202.62, 18.244.202.103, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.244.202.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1136468879 (1.1G) [application/zip]\n",
            "Saving to: ‘hubert_base_ls960.pt’\n",
            "\n",
            "hubert_base_ls960.p 100%[===================>]   1.06G   277MB/s    in 4.1s    \n",
            "\n",
            "2024-12-11 21:23:42 (267 MB/s) - ‘hubert_base_ls960.pt’ saved [1136468879/1136468879]\n",
            "\n",
            "--2024-12-11 21:23:42--  https://dl.fbaipublicfiles.com/textless_nlp/gslm/hubert/km100/km.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.244.202.25, 18.244.202.62, 18.244.202.103, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.244.202.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 308469 (301K) [application/octet-stream]\n",
            "Saving to: ‘km.bin’\n",
            "\n",
            "km.bin              100%[===================>] 301.24K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-12-11 21:23:42 (6.85 MB/s) - ‘km.bin’ saved [308469/308469]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify transcriptions.txt to be filtered\n",
        "import re\n",
        "\n",
        "def clean_text_file(input_file, output_file):\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            # Split the line into ID and text/transcription parts\n",
        "            parts = line.strip().split('\\t', maxsplit=1)\n",
        "            if len(parts) == 2:\n",
        "                identifier, text = parts\n",
        "                # Remove unwanted characters from the text\n",
        "                cleaned_text = re.sub(r'[\\-/%\\.]', '', text)  # Remove '-', '/', '%', '.'\n",
        "                cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Normalize spaces\n",
        "                # Append .wav to the identifier\n",
        "                identifier_wav = f\"{identifier}.wav\"\n",
        "                # Write to output file in the desired format\n",
        "                outfile.write(f\"{identifier_wav}\\t{cleaned_text}\\n\")\n",
        "\n",
        "# Input and output file names\n",
        "input_filename =  \"/content/drive/MyDrive/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New Zealand/ProsodyLabeling/text.txt\"\n",
        "output_filename = \"/content/drive/MyDrive/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New Zealand/ProsodyLabeling/filtered_transcriptions\"\n",
        "\n",
        "# Run the function\n",
        "clean_text_file(input_filename, output_filename)\n"
      ],
      "metadata": {
        "id": "-tJSsSIU0crA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Function to convert audio\n",
        "def convert_audio(input_file, output_file, target_sample_rate=16000):\n",
        "    # Load the .wav file\n",
        "    audio = AudioSegment.from_wav(input_file)\n",
        "\n",
        "    # Set the target sample rate and export\n",
        "    audio = audio.set_frame_rate(target_sample_rate)\n",
        "    audio.export(output_file, format=\"wav\")\n",
        "\n",
        "# Directory with .wav files\n",
        "input_dir = data_dir\n",
        "output_dir = '/content/drive/MyDrive/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New Zealand/16hz_wave'\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all files in the directory and convert\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.wav'):\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Convert to 16 kHz\n",
        "        convert_audio(input_path, output_path)\n",
        "        print(f\"Converted {filename} to 16 kHz\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8PBP-QzM0fzl",
        "outputId": "3aeaca21-502c-4973-d16f-c7e01f8f3736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 000004.wav to 16 kHz\n",
            "Converted 000003.wav to 16 kHz\n",
            "Converted 000001.wav to 16 kHz\n",
            "Converted 000002.wav to 16 kHz\n",
            "Converted 000009.wav to 16 kHz\n",
            "Converted 000019.wav to 16 kHz\n",
            "Converted 000005.wav to 16 kHz\n",
            "Converted 000006.wav to 16 kHz\n",
            "Converted 000015.wav to 16 kHz\n",
            "Converted 000018.wav to 16 kHz\n",
            "Converted 000020.wav to 16 kHz\n",
            "Converted 000008.wav to 16 kHz\n",
            "Converted 000012.wav to 16 kHz\n",
            "Converted 000010.wav to 16 kHz\n",
            "Converted 000017.wav to 16 kHz\n",
            "Converted 000016.wav to 16 kHz\n",
            "Converted 000014.wav to 16 kHz\n",
            "Converted 000011.wav to 16 kHz\n",
            "Converted 000007.wav to 16 kHz\n",
            "Converted 000013.wav to 16 kHz\n",
            "Converted 000023.wav to 16 kHz\n",
            "Converted 000030.wav to 16 kHz\n",
            "Converted 000025.wav to 16 kHz\n",
            "Converted 000035.wav to 16 kHz\n",
            "Converted 000032.wav to 16 kHz\n",
            "Converted 000034.wav to 16 kHz\n",
            "Converted 000028.wav to 16 kHz\n",
            "Converted 000029.wav to 16 kHz\n",
            "Converted 000021.wav to 16 kHz\n",
            "Converted 000022.wav to 16 kHz\n",
            "Converted 000024.wav to 16 kHz\n",
            "Converted 000031.wav to 16 kHz\n",
            "Converted 000026.wav to 16 kHz\n",
            "Converted 000036.wav to 16 kHz\n",
            "Converted 000027.wav to 16 kHz\n",
            "Converted 000033.wav to 16 kHz\n",
            "Converted 000054.wav to 16 kHz\n",
            "Converted 000049.wav to 16 kHz\n",
            "Converted 000046.wav to 16 kHz\n",
            "Converted 000050.wav to 16 kHz\n",
            "Converted 000037.wav to 16 kHz\n",
            "Converted 000043.wav to 16 kHz\n",
            "Converted 000042.wav to 16 kHz\n",
            "Converted 000044.wav to 16 kHz\n",
            "Converted 000048.wav to 16 kHz\n",
            "Converted 000051.wav to 16 kHz\n",
            "Converted 000040.wav to 16 kHz\n",
            "Converted 000047.wav to 16 kHz\n",
            "Converted 000052.wav to 16 kHz\n",
            "Converted 000053.wav to 16 kHz\n",
            "Converted 000039.wav to 16 kHz\n",
            "Converted 000041.wav to 16 kHz\n",
            "Converted 000038.wav to 16 kHz\n",
            "Converted 000045.wav to 16 kHz\n",
            "Converted 000057.wav to 16 kHz\n",
            "Converted 000063.wav to 16 kHz\n",
            "Converted 000055.wav to 16 kHz\n",
            "Converted 000066.wav to 16 kHz\n",
            "Converted 000058.wav to 16 kHz\n",
            "Converted 000065.wav to 16 kHz\n",
            "Converted 000064.wav to 16 kHz\n",
            "Converted 000069.wav to 16 kHz\n",
            "Converted 000067.wav to 16 kHz\n",
            "Converted 000068.wav to 16 kHz\n",
            "Converted 000061.wav to 16 kHz\n",
            "Converted 000059.wav to 16 kHz\n",
            "Converted 000062.wav to 16 kHz\n",
            "Converted 000056.wav to 16 kHz\n",
            "Converted 000060.wav to 16 kHz\n",
            "Converted 000071.wav to 16 kHz\n",
            "Converted 000081.wav to 16 kHz\n",
            "Converted 000086.wav to 16 kHz\n",
            "Converted 000083.wav to 16 kHz\n",
            "Converted 000075.wav to 16 kHz\n",
            "Converted 000070.wav to 16 kHz\n",
            "Converted 000073.wav to 16 kHz\n",
            "Converted 000076.wav to 16 kHz\n",
            "Converted 000085.wav to 16 kHz\n",
            "Converted 000078.wav to 16 kHz\n",
            "Converted 000080.wav to 16 kHz\n",
            "Converted 000074.wav to 16 kHz\n",
            "Converted 000084.wav to 16 kHz\n",
            "Converted 000079.wav to 16 kHz\n",
            "Converted 000082.wav to 16 kHz\n",
            "Converted 000077.wav to 16 kHz\n",
            "Converted 000072.wav to 16 kHz\n",
            "Converted 000093.wav to 16 kHz\n",
            "Converted 000101.wav to 16 kHz\n",
            "Converted 000088.wav to 16 kHz\n",
            "Converted 000095.wav to 16 kHz\n",
            "Converted 000091.wav to 16 kHz\n",
            "Converted 000087.wav to 16 kHz\n",
            "Converted 000100.wav to 16 kHz\n",
            "Converted 000099.wav to 16 kHz\n",
            "Converted 000092.wav to 16 kHz\n",
            "Converted 000090.wav to 16 kHz\n",
            "Converted 000098.wav to 16 kHz\n",
            "Converted 000097.wav to 16 kHz\n",
            "Converted 000094.wav to 16 kHz\n",
            "Converted 000089.wav to 16 kHz\n",
            "Converted 000096.wav to 16 kHz\n",
            "Converted 000108.wav to 16 kHz\n",
            "Converted 000105.wav to 16 kHz\n",
            "Converted 000110.wav to 16 kHz\n",
            "Converted 000112.wav to 16 kHz\n",
            "Converted 000107.wav to 16 kHz\n",
            "Converted 000117.wav to 16 kHz\n",
            "Converted 000111.wav to 16 kHz\n",
            "Converted 000106.wav to 16 kHz\n",
            "Converted 000118.wav to 16 kHz\n",
            "Converted 000103.wav to 16 kHz\n",
            "Converted 000104.wav to 16 kHz\n",
            "Converted 000115.wav to 16 kHz\n",
            "Converted 000109.wav to 16 kHz\n",
            "Converted 000114.wav to 16 kHz\n",
            "Converted 000116.wav to 16 kHz\n",
            "Converted 000102.wav to 16 kHz\n",
            "Converted 000113.wav to 16 kHz\n",
            "Converted 000121.wav to 16 kHz\n",
            "Converted 000122.wav to 16 kHz\n",
            "Converted 000132.wav to 16 kHz\n",
            "Converted 000126.wav to 16 kHz\n",
            "Converted 000133.wav to 16 kHz\n",
            "Converted 000123.wav to 16 kHz\n",
            "Converted 000120.wav to 16 kHz\n",
            "Converted 000119.wav to 16 kHz\n",
            "Converted 000131.wav to 16 kHz\n",
            "Converted 000124.wav to 16 kHz\n",
            "Converted 000129.wav to 16 kHz\n",
            "Converted 000130.wav to 16 kHz\n",
            "Converted 000125.wav to 16 kHz\n",
            "Converted 000128.wav to 16 kHz\n",
            "Converted 000127.wav to 16 kHz\n",
            "Converted 000137.wav to 16 kHz\n",
            "Converted 000134.wav to 16 kHz\n",
            "Converted 000145.wav to 16 kHz\n",
            "Converted 000138.wav to 16 kHz\n",
            "Converted 000141.wav to 16 kHz\n",
            "Converted 000142.wav to 16 kHz\n",
            "Converted 000147.wav to 16 kHz\n",
            "Converted 000149.wav to 16 kHz\n",
            "Converted 000136.wav to 16 kHz\n",
            "Converted 000150.wav to 16 kHz\n",
            "Converted 000144.wav to 16 kHz\n",
            "Converted 000140.wav to 16 kHz\n",
            "Converted 000139.wav to 16 kHz\n",
            "Converted 000143.wav to 16 kHz\n",
            "Converted 000148.wav to 16 kHz\n",
            "Converted 000146.wav to 16 kHz\n",
            "Converted 000135.wav to 16 kHz\n",
            "Converted 000163.wav to 16 kHz\n",
            "Converted 000152.wav to 16 kHz\n",
            "Converted 000161.wav to 16 kHz\n",
            "Converted 000165.wav to 16 kHz\n",
            "Converted 000159.wav to 16 kHz\n",
            "Converted 000164.wav to 16 kHz\n",
            "Converted 000153.wav to 16 kHz\n",
            "Converted 000154.wav to 16 kHz\n",
            "Converted 000156.wav to 16 kHz\n",
            "Converted 000158.wav to 16 kHz\n",
            "Converted 000151.wav to 16 kHz\n",
            "Converted 000155.wav to 16 kHz\n",
            "Converted 000162.wav to 16 kHz\n",
            "Converted 000157.wav to 16 kHz\n",
            "Converted 000160.wav to 16 kHz\n",
            "Converted 000173.wav to 16 kHz\n",
            "Converted 000170.wav to 16 kHz\n",
            "Converted 000168.wav to 16 kHz\n",
            "Converted 000166.wav to 16 kHz\n",
            "Converted 000167.wav to 16 kHz\n",
            "Converted 000171.wav to 16 kHz\n",
            "Converted 000182.wav to 16 kHz\n",
            "Converted 000172.wav to 16 kHz\n",
            "Converted 000179.wav to 16 kHz\n",
            "Converted 000180.wav to 16 kHz\n",
            "Converted 000176.wav to 16 kHz\n",
            "Converted 000175.wav to 16 kHz\n",
            "Converted 000181.wav to 16 kHz\n",
            "Converted 000174.wav to 16 kHz\n",
            "Converted 000169.wav to 16 kHz\n",
            "Converted 000177.wav to 16 kHz\n",
            "Converted 000178.wav to 16 kHz\n",
            "Converted 000191.wav to 16 kHz\n",
            "Converted 000195.wav to 16 kHz\n",
            "Converted 000188.wav to 16 kHz\n",
            "Converted 000192.wav to 16 kHz\n",
            "Converted 000187.wav to 16 kHz\n",
            "Converted 000183.wav to 16 kHz\n",
            "Converted 000185.wav to 16 kHz\n",
            "Converted 000196.wav to 16 kHz\n",
            "Converted 000198.wav to 16 kHz\n",
            "Converted 000194.wav to 16 kHz\n",
            "Converted 000184.wav to 16 kHz\n",
            "Converted 000190.wav to 16 kHz\n",
            "Converted 000186.wav to 16 kHz\n",
            "Converted 000189.wav to 16 kHz\n",
            "Converted 000197.wav to 16 kHz\n",
            "Converted 000199.wav to 16 kHz\n",
            "Converted 000193.wav to 16 kHz\n",
            "Converted 000213.wav to 16 kHz\n",
            "Converted 000208.wav to 16 kHz\n",
            "Converted 000212.wav to 16 kHz\n",
            "Converted 000215.wav to 16 kHz\n",
            "Converted 000206.wav to 16 kHz\n",
            "Converted 000201.wav to 16 kHz\n",
            "Converted 000211.wav to 16 kHz\n",
            "Converted 000203.wav to 16 kHz\n",
            "Converted 000200.wav to 16 kHz\n",
            "Converted 000216.wav to 16 kHz\n",
            "Converted 000204.wav to 16 kHz\n",
            "Converted 000205.wav to 16 kHz\n",
            "Converted 000209.wav to 16 kHz\n",
            "Converted 000214.wav to 16 kHz\n",
            "Converted 000207.wav to 16 kHz\n",
            "Converted 000202.wav to 16 kHz\n",
            "Converted 000210.wav to 16 kHz\n",
            "Converted 000220.wav to 16 kHz\n",
            "Converted 000232.wav to 16 kHz\n",
            "Converted 000228.wav to 16 kHz\n",
            "Converted 000219.wav to 16 kHz\n",
            "Converted 000231.wav to 16 kHz\n",
            "Converted 000217.wav to 16 kHz\n",
            "Converted 000226.wav to 16 kHz\n",
            "Converted 000224.wav to 16 kHz\n",
            "Converted 000229.wav to 16 kHz\n",
            "Converted 000230.wav to 16 kHz\n",
            "Converted 000227.wav to 16 kHz\n",
            "Converted 000221.wav to 16 kHz\n",
            "Converted 000223.wav to 16 kHz\n",
            "Converted 000222.wav to 16 kHz\n",
            "Converted 000225.wav to 16 kHz\n",
            "Converted 000218.wav to 16 kHz\n",
            "Converted 000235.wav to 16 kHz\n",
            "Converted 000244.wav to 16 kHz\n",
            "Converted 000234.wav to 16 kHz\n",
            "Converted 000236.wav to 16 kHz\n",
            "Converted 000242.wav to 16 kHz\n",
            "Converted 000238.wav to 16 kHz\n",
            "Converted 000237.wav to 16 kHz\n",
            "Converted 000241.wav to 16 kHz\n",
            "Converted 000246.wav to 16 kHz\n",
            "Converted 000240.wav to 16 kHz\n",
            "Converted 000243.wav to 16 kHz\n",
            "Converted 000233.wav to 16 kHz\n",
            "Converted 000247.wav to 16 kHz\n",
            "Converted 000245.wav to 16 kHz\n",
            "Converted 000249.wav to 16 kHz\n",
            "Converted 000248.wav to 16 kHz\n",
            "Converted 000239.wav to 16 kHz\n",
            "Converted 000263.wav to 16 kHz\n",
            "Converted 000262.wav to 16 kHz\n",
            "Converted 000254.wav to 16 kHz\n",
            "Converted 000260.wav to 16 kHz\n",
            "Converted 000251.wav to 16 kHz\n",
            "Converted 000259.wav to 16 kHz\n",
            "Converted 000252.wav to 16 kHz\n",
            "Converted 000256.wav to 16 kHz\n",
            "Converted 000258.wav to 16 kHz\n",
            "Converted 000255.wav to 16 kHz\n",
            "Converted 000253.wav to 16 kHz\n",
            "Converted 000257.wav to 16 kHz\n",
            "Converted 000250.wav to 16 kHz\n",
            "Converted 000264.wav to 16 kHz\n",
            "Converted 000261.wav to 16 kHz\n",
            "Converted 000268.wav to 16 kHz\n",
            "Converted 000279.wav to 16 kHz\n",
            "Converted 000277.wav to 16 kHz\n",
            "Converted 000269.wav to 16 kHz\n",
            "Converted 000266.wav to 16 kHz\n",
            "Converted 000272.wav to 16 kHz\n",
            "Converted 000280.wav to 16 kHz\n",
            "Converted 000278.wav to 16 kHz\n",
            "Converted 000267.wav to 16 kHz\n",
            "Converted 000276.wav to 16 kHz\n",
            "Converted 000273.wav to 16 kHz\n",
            "Converted 000274.wav to 16 kHz\n",
            "Converted 000281.wav to 16 kHz\n",
            "Converted 000275.wav to 16 kHz\n",
            "Converted 000271.wav to 16 kHz\n",
            "Converted 000270.wav to 16 kHz\n",
            "Converted 000265.wav to 16 kHz\n",
            "Converted 000297.wav to 16 kHz\n",
            "Converted 000291.wav to 16 kHz\n",
            "Converted 000292.wav to 16 kHz\n",
            "Converted 000298.wav to 16 kHz\n",
            "Converted 000293.wav to 16 kHz\n",
            "Converted 000287.wav to 16 kHz\n",
            "Converted 000288.wav to 16 kHz\n",
            "Converted 000285.wav to 16 kHz\n",
            "Converted 000284.wav to 16 kHz\n",
            "Converted 000283.wav to 16 kHz\n",
            "Converted 000296.wav to 16 kHz\n",
            "Converted 000282.wav to 16 kHz\n",
            "Converted 000294.wav to 16 kHz\n",
            "Converted 000286.wav to 16 kHz\n",
            "Converted 000290.wav to 16 kHz\n",
            "Converted 000289.wav to 16 kHz\n",
            "Converted 000295.wav to 16 kHz\n",
            "Converted 000309.wav to 16 kHz\n",
            "Converted 000305.wav to 16 kHz\n",
            "Converted 000304.wav to 16 kHz\n",
            "Converted 000300.wav to 16 kHz\n",
            "Converted 000299.wav to 16 kHz\n",
            "Converted 000301.wav to 16 kHz\n",
            "Converted 000311.wav to 16 kHz\n",
            "Converted 000312.wav to 16 kHz\n",
            "Converted 000307.wav to 16 kHz\n",
            "Converted 000313.wav to 16 kHz\n",
            "Converted 000308.wav to 16 kHz\n",
            "Converted 000303.wav to 16 kHz\n",
            "Converted 000310.wav to 16 kHz\n",
            "Converted 000306.wav to 16 kHz\n",
            "Converted 000314.wav to 16 kHz\n",
            "Converted 000302.wav to 16 kHz\n",
            "Converted 000317.wav to 16 kHz\n",
            "Converted 000326.wav to 16 kHz\n",
            "Converted 000328.wav to 16 kHz\n",
            "Converted 000325.wav to 16 kHz\n",
            "Converted 000321.wav to 16 kHz\n",
            "Converted 000323.wav to 16 kHz\n",
            "Converted 000315.wav to 16 kHz\n",
            "Converted 000322.wav to 16 kHz\n",
            "Converted 000327.wav to 16 kHz\n",
            "Converted 000330.wav to 16 kHz\n",
            "Converted 000329.wav to 16 kHz\n",
            "Converted 000320.wav to 16 kHz\n",
            "Converted 000324.wav to 16 kHz\n",
            "Converted 000319.wav to 16 kHz\n",
            "Converted 000318.wav to 16 kHz\n",
            "Converted 000316.wav to 16 kHz\n",
            "Converted 000333.wav to 16 kHz\n",
            "Converted 000338.wav to 16 kHz\n",
            "Converted 000337.wav to 16 kHz\n",
            "Converted 000332.wav to 16 kHz\n",
            "Converted 000335.wav to 16 kHz\n",
            "Converted 000341.wav to 16 kHz\n",
            "Converted 000334.wav to 16 kHz\n",
            "Converted 000339.wav to 16 kHz\n",
            "Converted 000336.wav to 16 kHz\n",
            "Converted 000331.wav to 16 kHz\n",
            "Converted 000340.wav to 16 kHz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        "# Change to the working directory\n",
        "%cd /content\n",
        "\n",
        "def split_data(data_dir, output_dir, train_ratio=0.8, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Splits the data directory into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to the original data directory (contains .wav and transcriptions.txt).\n",
        "        output_dir (str): Path to the output directory.\n",
        "        train_ratio (float): Proportion of data to use for training (default 0.8).\n",
        "        test_ratio (float): Proportion of data to use for testing (default 0.1).\n",
        "    \"\"\"\n",
        "    # Ensure train_ratio + test_ratio <= 1\n",
        "    if train_ratio + test_ratio > 1:\n",
        "        raise ValueError(\"Train and test ratios must sum to at most 1.\")\n",
        "\n",
        "    # Path to the transcription file\n",
        "    transcription_file = os.path.join(\n",
        "        \"/content/drive/MyDrive/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New Zealand/ProsodyLabeling/filtered_transcriptions\"\n",
        "    )\n",
        "\n",
        "    # Read transcriptions\n",
        "    with open(transcription_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Shuffle the data\n",
        "    random.shuffle(lines)\n",
        "\n",
        "    # Calculate split indices\n",
        "    total_lines = len(lines)\n",
        "    train_idx = int(total_lines * train_ratio)\n",
        "    test_idx = int(total_lines * test_ratio) + train_idx\n",
        "\n",
        "    # Split the dataset\n",
        "    train_lines = lines[:train_idx]\n",
        "    test_lines = lines[train_idx:test_idx]\n",
        "    valid_lines = lines[test_idx:]\n",
        "\n",
        "    # Create output directories\n",
        "    train_dir = os.path.join(output_dir, \"train\")\n",
        "    valid_dir = os.path.join(output_dir, \"valid\")\n",
        "    test_dir = os.path.join(output_dir, \"test\")\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(valid_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Write split transcriptions\n",
        "    with open(os.path.join(train_dir, \"transcriptions.txt\"), \"w\") as f:\n",
        "        f.writelines(train_lines)\n",
        "\n",
        "    with open(os.path.join(valid_dir, \"transcriptions.txt\"), \"w\") as f:\n",
        "        f.writelines(valid_lines)\n",
        "\n",
        "    with open(os.path.join(test_dir, \"transcriptions.txt\"), \"w\") as f:\n",
        "        f.writelines(test_lines)\n",
        "\n",
        "    # Copy corresponding audio files\n",
        "    def copy_files(lines, destination_dir):\n",
        "        for line in lines:\n",
        "            audio_file = line.split(\"\\t\")[0].strip()\n",
        "            source_path = os.path.join(data_dir, audio_file)\n",
        "            dest_path = os.path.join(destination_dir, audio_file)\n",
        "            if os.path.exists(source_path):\n",
        "                copyfile(source_path, dest_path)\n",
        "\n",
        "    copy_files(train_lines, train_dir)\n",
        "    copy_files(valid_lines, valid_dir)\n",
        "    copy_files(test_lines, test_dir)\n",
        "\n",
        "    print(f\"Data successfully split into train ({train_dir}), valid ({valid_dir}), and test ({test_dir})\")\n",
        "\n",
        "# Path to the output directory\n",
        "output_dir = \"processed_data\"\n",
        "\n",
        "data_dir_16hz = '/content/drive/MyDrive/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New Zealand/16hz_wave'\n",
        "\n",
        "# Split the data\n",
        "split_data(data_dir_16hz, output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTr6i1Ou1DCx",
        "outputId": "b0acdc30-9ccf-4ea9-b1c8-52210767dc10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Data successfully split into train (processed_data/train), valid (processed_data/valid), and test (processed_data/test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q6YC8k3U3-V7",
        "outputId": "4933592a-a7a7-444a-83aa-3735e29d2593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.8.30)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: gtts\n",
            "Successfully installed gtts-2.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from gtts import gTTS\n",
        "\n",
        "# Ensure the folder exists\n",
        "output_folder = \"source_wav\"\n",
        "train_output_folder = \"source_wav/train\"\n",
        "test_output_folder = \"source_wav/test\"\n",
        "valid_output_folder = \"source_wav/valid\"\n",
        "os.makedirs(valid_output_folder, exist_ok=True)\n",
        "os.makedirs(test_output_folder, exist_ok=True)\n",
        "os.makedirs(train_output_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "source_transcription_path_train = '/content/processed_data/train/transcriptions.txt'\n",
        "source_transcription_path_test = '/content/processed_data/test/transcriptions.txt'\n",
        "source_transcription_path_valid = '/content/processed_data/valid/transcriptions.txt'\n",
        "# Open the transcription file\n",
        "with open(source_transcription_path_train, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Split the line into filename and text\n",
        "        parts = line.strip().split(\"\\t\")\n",
        "        if len(parts) != 2:\n",
        "            continue  # Skip invalid lines\n",
        "        filename, text = parts\n",
        "        # Generate TTS audio\n",
        "        tts = gTTS(text, lang=\"en\")\n",
        "        # Save the file\n",
        "        output_path = os.path.join(output_folder, 'train' ,filename)\n",
        "        tts.save(output_path)\n",
        "\n",
        "with open(source_transcription_path_test, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Split the line into filename and text\n",
        "        parts = line.strip().split(\"\\t\")\n",
        "        if len(parts) != 2:\n",
        "            continue  # Skip invalid lines\n",
        "        filename, text = parts\n",
        "        # Generate TTS audio\n",
        "        tts = gTTS(text, lang=\"en\")\n",
        "        # Save the file\n",
        "        output_path = os.path.join(output_folder, 'test' ,filename)\n",
        "        tts.save(output_path)\n",
        "\n",
        "with open(source_transcription_path_valid, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Split the line into filename and text\n",
        "        parts = line.strip().split(\"\\t\")\n",
        "        if len(parts) != 2:\n",
        "            continue  # Skip invalid lines\n",
        "        filename, text = parts\n",
        "        # Generate TTS audio\n",
        "        tts = gTTS(text, lang=\"en\")\n",
        "        # Save the file\n",
        "        output_path = os.path.join(output_folder, 'valid' ,filename)\n",
        "        tts.save(output_path)\n",
        "print(f\"Synthetic TTS .wav files saved in '{output_folder}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmFnfsNa3yS-",
        "outputId": "f587b406-be6c-42ea-bbe3-266385270198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic TTS .wav files saved in 'source_wav'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create manifest files\n",
        "!pip install soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WJliNaagANZz",
        "outputId": "703c8d61-984e-45ad-c66b-0c929228cc4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/source_wav/test --dest /content/source_wav/test --ext wav --valid-percent 0\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/source_wav/train --dest /content/source_wav/train --ext wav --valid-percent 0\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/target_wav/test --dest /content/target_wav/test --ext wav --valid-percent 0\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/target_wav/train --dest /content/target_wav/train --ext wav --valid-percent 0\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/source_wav/valid --dest /content/source_wav/valid --ext wav --valid-percent 0\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/target_wav/valid --dest /content/target_wav/valid --ext wav --valid-percent 0"
      ],
      "metadata": {
        "id": "FgdxmI-ZAWMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: take the audio from new zealand/16khz wave and split them into test train and valid folders in target wav and update the tsv file to have this format\n",
        "# /content/source_wav/valid\n",
        "# 000166.wav\t139392\n",
        "# 000046.wav\t139392\n",
        "# 000331.wav\t139968\n",
        "# 000239.wav\t134208\n",
        "# 000151.wav\t146304\n",
        "# 000324.wav\t133056\n",
        "# 000011.wav\t123264\n",
        "# 000153.wav\t113472\n",
        "\n",
        "import soundfile\n",
        "import os\n",
        "import random\n",
        "\n",
        "def update_tsv(input_dir, output_file):\n",
        "    \"\"\"Updates the TSV file with the specified format.\"\"\"\n",
        "\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        for subdir, _, files in os.walk(input_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.wav'):\n",
        "                    filepath = os.path.join(subdir, file)\n",
        "                    # Placeholder for duration, replace with actual duration calculation if needed\n",
        "                    f = soundfile.SoundFile(filepath) # replace your_file.wav with the actual file path\n",
        "\n",
        "\n",
        "                    duration = len(f)\n",
        "                    outfile.write(f\"{filepath}\\t{duration}\\n\")\n",
        "\n",
        "# Example usage\n",
        "input_directory = \"/content/processed_data/train\"  # Replace with the actual directory\n",
        "output_tsv_file = \"/content/target_wav/train.tsv\"  # Replace with desired output path\n",
        "\n",
        "update_tsv(input_directory, output_tsv_file)\n",
        "print(f\"TSV file '{output_tsv_file}' created successfully.\")\n",
        "\n",
        "# Example usage\n",
        "input_directory = \"/content/processed_data/test\"  # Replace with the actual directory\n",
        "output_tsv_file = \"/content/target_wav/test.tsv\"  # Replace with desired output path\n",
        "\n",
        "update_tsv(input_directory, output_tsv_file)\n",
        "print(f\"TSV file '{output_tsv_file}' created successfully.\")\n",
        "\n",
        "# Example usage\n",
        "input_directory = \"/content/processed_data/valid\"  # Replace with the actual directory\n",
        "output_tsv_file = \"/content/target_wav/valid.tsv\"  # Replace with desired output path\n",
        "\n",
        "update_tsv(input_directory, output_tsv_file)\n",
        "print(f\"TSV file '{output_tsv_file}' created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v22Dmuyn2k0B",
        "outputId": "957ea5d9-2e1e-42bf-dff0-c30b860b3152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TSV file '/content/target_wav/train.tsv' created successfully.\n",
            "TSV file '/content/target_wav/test.tsv' created successfully.\n",
            "TSV file '/content/target_wav/valid.tsv' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!MANIFEST=/content/target_wav/train/train.tsv\n",
        "!OUT_QUANTIZED_FILE=/content/target_wav/train.txt\n",
        "!KM_MODEL_PATH=/content/km.bin\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py \\\n",
        "    --feature_type hubert \\\n",
        "    --kmeans_model_path /content/km.bin \\\n",
        "    --acoustic_model_path /content/hubert_base_ls960.pt \\\n",
        "    --layer 6 \\\n",
        "    --manifest_path /content/target_wav/train.tsv \\\n",
        "    --out_quantized_file_path /content/target_wav/train.txt \\\n",
        "    --extension \".wav\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SFNrE2dgCpbe",
        "outputId": "c9fed182-dd42-425b-a6be-e4f2d09a5c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 21:26:58.178242: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 21:26:58.195028: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 21:26:58.216148: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 21:26:58.222503: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 21:26:58.237613: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 21:26:59.296799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/fairseq/fairseq/tasks/multires_hubert_pretraining.py:154: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  dictionaries = [ (Dictionary.load(f\"{label_dir}/dict.{label}.txt\") if label is not \"\" else None ) for label in self.cfg.labels]\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg6\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg6 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1350, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.58: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg5\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg5 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1350, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.57: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg4\n",
            "DEBUG:torio._extension.utils:Successfully loaded FFmpeg4\n",
            "INFO:__main__:Namespace(feature_type='hubert', acoustic_model_path='/content/hubert_base_ls960.pt', layer=6, kmeans_model_path='/content/km.bin', features_path=None, manifest_path='/content/target_wav/train.tsv', out_quantized_file_path='/content/target_wav/train.txt', extension='.wav', channel_id=None, hide_fname=False)\n",
            "INFO:__main__:Extracting hubert acoustic features...\n",
            "/content/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "INFO:fairseq.tasks.hubert_pretraining:current directory is /content\n",
            "INFO:fairseq.tasks.hubert_pretraining:HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
            "INFO:fairseq.models.hubert.hubert:HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "100% 271/271 [00:05<00:00, 48.89it/s]\n",
            "INFO:__main__:Features extracted for 271 utterances.\n",
            "\n",
            "INFO:__main__:Dimensionality of representation = 768\n",
            "INFO:__main__:Loading K-means model from /content/km.bin ...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Writing quantized predictions to /content/target_wav/train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!MANIFEST=/content/target_wav/test/test.tsv\n",
        "!OUT_QUANTIZED_FILE=/content/target_wav/test.txt\n",
        "!KM_MODEL_PATH=/content/km.bin\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py \\\n",
        "    --feature_type hubert \\\n",
        "    --kmeans_model_path /content/km.bin \\\n",
        "    --acoustic_model_path /content/hubert_base_ls960.pt \\\n",
        "    --layer 6 \\\n",
        "    --manifest_path /content/target_wav/test.tsv \\\n",
        "    --out_quantized_file_path /content/target_wav/test.txt \\\n",
        "    --extension \".wav\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA8rVaS-Gm4J",
        "outputId": "4e13d27d-8700-434d-c6c6-96213ecf4b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 21:27:26.360183: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 21:27:26.376988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 21:27:26.397790: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 21:27:26.404092: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 21:27:26.418812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 21:27:27.481178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg6\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg6 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1350, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.58: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg5\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg5 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1350, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.57: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg4\n",
            "DEBUG:torio._extension.utils:Successfully loaded FFmpeg4\n",
            "INFO:__main__:Namespace(feature_type='hubert', acoustic_model_path='/content/hubert_base_ls960.pt', layer=6, kmeans_model_path='/content/km.bin', features_path=None, manifest_path='/content/target_wav/test.tsv', out_quantized_file_path='/content/target_wav/test.txt', extension='.wav', channel_id=None, hide_fname=False)\n",
            "INFO:__main__:Extracting hubert acoustic features...\n",
            "/content/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "INFO:fairseq.tasks.hubert_pretraining:current directory is /content\n",
            "INFO:fairseq.tasks.hubert_pretraining:HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
            "INFO:fairseq.models.hubert.hubert:HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "100% 33/33 [00:01<00:00, 25.75it/s]\n",
            "INFO:__main__:Features extracted for 33 utterances.\n",
            "\n",
            "INFO:__main__:Dimensionality of representation = 768\n",
            "INFO:__main__:Loading K-means model from /content/km.bin ...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Writing quantized predictions to /content/target_wav/test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!MANIFEST=/content/target_wav/valid/valid.tsv\n",
        "!OUT_QUANTIZED_FILE=/content/target_wav/valid.txt\n",
        "!KM_MODEL_PATH=/content/km.bin\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py \\\n",
        "    --feature_type hubert \\\n",
        "    --kmeans_model_path /content/km.bin \\\n",
        "    --acoustic_model_path /content/hubert_base_ls960.pt \\\n",
        "    --layer 6 \\\n",
        "    --manifest_path /content/target_wav/valid.tsv \\\n",
        "    --out_quantized_file_path /content/target_wav/valid.txt \\\n",
        "    --extension \".wav\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNh5HdlCOuaI",
        "outputId": "3fe92eb5-077a-49de-9580-7a3aab9eec1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 21:27:42.335606: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 21:27:42.352680: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 21:27:42.373591: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 21:27:42.379873: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 21:27:42.394602: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 21:27:43.433115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg6\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg6 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1350, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.58: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg5\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg5 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1350, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.57: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg4\n",
            "DEBUG:torio._extension.utils:Successfully loaded FFmpeg4\n",
            "INFO:__main__:Namespace(feature_type='hubert', acoustic_model_path='/content/hubert_base_ls960.pt', layer=6, kmeans_model_path='/content/km.bin', features_path=None, manifest_path='/content/target_wav/valid.tsv', out_quantized_file_path='/content/target_wav/valid.txt', extension='.wav', channel_id=None, hide_fname=False)\n",
            "INFO:__main__:Extracting hubert acoustic features...\n",
            "/content/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "INFO:fairseq.tasks.hubert_pretraining:current directory is /content\n",
            "INFO:fairseq.tasks.hubert_pretraining:HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
            "INFO:fairseq.models.hubert.hubert:HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "100% 34/34 [00:01<00:00, 27.61it/s]\n",
            "INFO:__main__:Features extracted for 34 utterances.\n",
            "\n",
            "INFO:__main__:Dimensionality of representation = 768\n",
            "INFO:__main__:Loading K-means model from /content/km.bin ...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Writing quantized predictions to /content/target_wav/valid.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# $SPLIT1, $SPLIT2, etc. are split names such as train, dev, test, etc.\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/speech_to_speech/preprocessing/prep_s2ut_data.py \\\n",
        "  --source-dir /content/source_wav --target-dir /content/target_wav/ --data-split train test valid \\\n",
        "  --output-root /content/data_root --reduce-unit \\\n",
        "  --vocoder-checkpoint /content/g_00500000 --vocoder-cfg /content/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNov96-C40Z5",
        "outputId": "f81ff4be-ad85-4794-b7e2-49f52b30a8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 21:27:58.081910: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 21:27:58.098627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 21:27:58.119171: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 21:27:58.125792: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 21:27:58.140267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 21:27:59.300381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "Generating manifest...\n",
            "Processing train\n",
            "100% 272/272 [00:00<00:00, 6950.13it/s]\n",
            "Processed 271 samples\n",
            "1 with missing target data (first 3 examples: 000216)\n",
            "Writing manifest to /content/data_root/train.tsv...\n",
            "Processing test\n",
            "100% 34/34 [00:00<00:00, 6938.81it/s]\n",
            "Processed 33 samples\n",
            "1 with missing target data (first 3 examples: 000196)\n",
            "Writing manifest to /content/data_root/test.tsv...\n",
            "Processing valid\n",
            "100% 35/35 [00:00<00:00, 6974.23it/s]\n",
            "Processed 34 samples\n",
            "1 with missing target data (first 3 examples: 000312)\n",
            "Writing manifest to /content/data_root/valid.tsv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-train /content/data_root \\\n",
        "  --config-yaml /content/data_root/config.yaml  \\\n",
        "  --task speech_to_speech --target-is-code --target-code-size 100 --vocoder code_hifigan  \\\n",
        "  --criterion speech_to_unit --label-smoothing 0.2 \\\n",
        "  --arch s2ut_transformer_fisher --share-decoder-input-output-embed \\\n",
        "  --dropout 0.1 --attention-dropout 0.1 --relu-dropout 0.1 \\\n",
        "  --train-subset train  \\\n",
        "  --save-dir /content/models \\\n",
        "  --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-7 --warmup-updates 10000 \\\n",
        "  --optimizer adam --adam-betas \"(0.9,0.98)\" --clip-norm 10.0 \\\n",
        "  --max-update 400000 --max-tokens 20000 --max-target-positions 3000 --update-freq 1 \\\n",
        "  --seed 1 --fp16 --num-workers 8 --max-epoch 80 --save-interval 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u9aMbL54IYk1",
        "outputId": "b8d09c68-20b5-4f4c-834e-6d0622504d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 21:28:05.527077: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 21:28:05.543694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 21:28:05.564198: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 21:28:05.570396: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 21:28:05.585030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 21:28:06.634674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-11 21:28:07 | INFO | numexpr.utils | NumExpr defaulting to 12 threads.\n",
            "2024-12-11 21:28:09 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-12-11 21:28:10 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 20000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 20000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 80, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='speech_to_unit', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='speech_to_speech', num_workers=8, skip_invalid_size_inputs_valid_test=False, max_tokens=20000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=20000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2ut_transformer_fisher', max_epoch=80, max_update=400000, stop_time_hours=0, clip_norm=10.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/content/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=10, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, activation_fn='relu', data='/content/data_root', config_yaml='/content/data_root/config.yaml', max_source_positions=6000, max_target_positions=3000, target_is_code=True, target_code_size=100, n_frames_per_step=1, multitask_config_yaml=None, eval_inference=False, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='code_hifigan', spec_bwd_max_iter=8, infer_target_lang='', label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, no_seed_provided=False, encoder_embed_dim=256, encoder_attention_heads=4, encoder_freezing_updates=0, conv_kernel_sizes='5,5', conv_channels=1024, encoder_ffn_embed_dim=2048, encoder_layers=12, encoder_normalize_before=True, no_scale_embedding=False, speaker_embed_dim=256, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=True, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, quant_noise_pq=0, _name='s2ut_transformer_fisher'), 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='speech_to_unit', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='speech_to_speech', num_workers=8, skip_invalid_size_inputs_valid_test=False, max_tokens=20000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=20000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2ut_transformer_fisher', max_epoch=80, max_update=400000, stop_time_hours=0, clip_norm=10.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/content/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=10, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, activation_fn='relu', data='/content/data_root', config_yaml='/content/data_root/config.yaml', max_source_positions=6000, max_target_positions=3000, target_is_code=True, target_code_size=100, n_frames_per_step=1, multitask_config_yaml=None, eval_inference=False, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='code_hifigan', spec_bwd_max_iter=8, infer_target_lang='', label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, no_seed_provided=False, encoder_embed_dim=256, encoder_attention_heads=4, encoder_freezing_updates=0, conv_kernel_sizes='5,5', conv_channels=1024, encoder_ffn_embed_dim=2048, encoder_layers=12, encoder_normalize_before=True, no_scale_embedding=False, speaker_embed_dim=256, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=True, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, quant_noise_pq=0, _name='speech_to_speech'), 'criterion': {'_name': 'speech_to_unit', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 10000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-12-11 21:28:10 | INFO | fairseq.tasks.speech_to_speech | dictionary size: 104\n",
            "2024-12-11 21:28:11 | INFO | fairseq_cli.train | S2UTTransformerModel(\n",
            "  (encoder): S2STransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (subsample): Conv1dSubsampler(\n",
            "      (conv_layers): ModuleList(\n",
            "        (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
            "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-11): 12 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerUnitDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): StackedEmbedding(104, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=104, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-12-11 21:28:11 | INFO | fairseq_cli.train | task: SpeechToSpeechTask\n",
            "2024-12-11 21:28:11 | INFO | fairseq_cli.train | model: S2UTTransformerModel\n",
            "2024-12-11 21:28:11 | INFO | fairseq_cli.train | criterion: SpeechToUnitMultitaskTaskCriterion\n",
            "2024-12-11 21:28:11 | INFO | fairseq_cli.train | num. shared model params: 42,771,456 (num. trained: 42,771,456)\n",
            "2024-12-11 21:28:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-12-11 21:28:11 | INFO | fairseq.data.audio.speech_to_text_dataset | 'valid' has 0.00% OOV\n",
            "2024-12-11 21:28:11 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToSpeechDataset(split=\"valid\", n_samples=34, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 21:28:11 | INFO | fairseq.data.audio.speech_to_speech_dataset | SpeechToSpeechDataset(split=\"valid\", n_samples=34, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 21:28:11 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-12-11 21:28:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-12-11 21:28:11 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.564 GB ; name = NVIDIA A100-SXM4-40GB                   \n",
            "2024-12-11 21:28:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-12-11 21:28:11 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-12-11 21:28:11 | INFO | fairseq_cli.train | max tokens per device = 20000 and max sentences per device = None\n",
            "2024-12-11 21:28:11 | INFO | fairseq.trainer | Preparing to load checkpoint /content/models/checkpoint_last.pt\n",
            "2024-12-11 21:28:11 | INFO | fairseq.trainer | No existing checkpoint found /content/models/checkpoint_last.pt\n",
            "2024-12-11 21:28:11 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-12-11 21:28:11 | INFO | fairseq.data.audio.speech_to_text_dataset | 'train' has 0.00% OOV\n",
            "2024-12-11 21:28:11 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToSpeechDataset(split=\"train\", n_samples=271, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "    SpecAugmentTransform(time_warp_w=0, freq_mask_n=1, freq_mask_f=27, time_mask_n=1, time_mask_t=100, time_mask_p=1.0)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 21:28:11 | INFO | fairseq.data.audio.speech_to_speech_dataset | SpeechToSpeechDataset(split=\"train\", n_samples=271, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "    SpecAugmentTransform(time_warp_w=0, freq_mask_n=1, freq_mask_f=27, time_mask_n=1, time_mask_t=100, time_mask_p=1.0)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 21:28:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 001:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:13 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-12-11 21:28:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "2024-12-11 21:28:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   7% 1/15 [00:02<00:34,  2.46s/it]2024-12-11 21:28:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:  53% 8/15 [00:03<00:01,  5.14it/s]2024-12-11 21:28:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:  87% 13/15 [00:03<00:00,  7.85it/s]2024-12-11 21:28:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  2.20it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.11 | nll_loss 10.096 | ppl 1094.08 | wps 34049.8 | wpb 1564.7 | bsz 11.3 | num_updates 12\n",
            "2024-12-11 21:28:17 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-12-11 21:28:17 | INFO | train | epoch 001 | loss 10.079 | nll_loss 10.075 | ppl 1078.8 | wps 13916.5 | ups 5.66 | wpb 2462.2 | bsz 17.2 | num_updates 12 | lr 6.9988e-07 | gnorm 19.207 | clip 100 | loss_scale 16 | train_wall 4 | gb_free 37.6 | wall 6\n",
            "2024-12-11 21:28:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 002:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:18 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-12-11 21:28:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  87% 13/15 [00:01<00:00, 10.22it/s]2024-12-11 21:28:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.128 | nll_loss 9.112 | ppl 553.39 | wps 45885.3 | wpb 1564.7 | bsz 11.3 | num_updates 27\n",
            "2024-12-11 21:28:20 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-12-11 21:28:20 | INFO | train | epoch 002 | loss 9.593 | nll_loss 9.587 | ppl 768.98 | wps 16404.2 | ups 6.37 | wpb 2575.8 | bsz 18.1 | num_updates 27 | lr 1.44973e-06 | gnorm 15.491 | clip 100 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 9\n",
            "2024-12-11 21:28:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 003:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:20 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-12-11 21:28:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  93% 14/15 [00:01<00:00, 10.82it/s]2024-12-11 21:28:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.17it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:22 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.293 | nll_loss 8.272 | ppl 309.12 | wps 46388.2 | wpb 1564.7 | bsz 11.3 | num_updates 42\n",
            "2024-12-11 21:28:22 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-12-11 21:28:22 | INFO | train | epoch 003 | loss 8.715 | nll_loss 8.703 | ppl 416.85 | wps 17527.6 | ups 6.8 | wpb 2575.8 | bsz 18.1 | num_updates 42 | lr 2.19958e-06 | gnorm 8.469 | clip 20 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 11\n",
            "2024-12-11 21:28:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 004:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:22 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-12-11 21:28:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  93% 14/15 [00:01<00:00, 10.83it/s]2024-12-11 21:28:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.31it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:24 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.846 | nll_loss 7.816 | ppl 225.37 | wps 45334.4 | wpb 1564.7 | bsz 11.3 | num_updates 57\n",
            "2024-12-11 21:28:24 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-12-11 21:28:24 | INFO | train | epoch 004 | loss 8.131 | nll_loss 8.112 | ppl 276.69 | wps 17614.2 | ups 6.84 | wpb 2575.8 | bsz 18.1 | num_updates 57 | lr 2.94943e-06 | gnorm 4.699 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 13\n",
            "2024-12-11 21:28:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 005:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:24 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-12-11 21:28:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  93% 14/15 [00:01<00:00, 10.97it/s]2024-12-11 21:28:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.28it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:26 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.499 | nll_loss 7.459 | ppl 175.9 | wps 45680.1 | wpb 1564.7 | bsz 11.3 | num_updates 72\n",
            "2024-12-11 21:28:26 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-12-11 21:28:26 | INFO | train | epoch 005 | loss 7.725 | nll_loss 7.695 | ppl 207.28 | wps 17776.8 | ups 6.9 | wpb 2575.8 | bsz 18.1 | num_updates 72 | lr 3.69928e-06 | gnorm 3.186 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 15\n",
            "2024-12-11 21:28:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 006:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:26 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-12-11 21:28:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  87% 13/15 [00:01<00:00, 10.23it/s]2024-12-11 21:28:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  3.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:29 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.251 | nll_loss 7.199 | ppl 146.89 | wps 46247.2 | wpb 1564.7 | bsz 11.3 | num_updates 87\n",
            "2024-12-11 21:28:29 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-12-11 21:28:29 | INFO | train | epoch 006 | loss 7.429 | nll_loss 7.387 | ppl 167.44 | wps 16057.4 | ups 6.23 | wpb 2575.8 | bsz 18.1 | num_updates 87 | lr 4.44913e-06 | gnorm 2.309 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 18\n",
            "2024-12-11 21:28:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 007:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:29 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2024-12-11 21:28:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  87% 13/15 [00:01<00:00, 10.72it/s, loss=8.378, nll_loss=8.354, ppl=327.29, wps=16988.9, ups=6.63, wpb=2562.4, bsz=18, num_updates=100, lr=5.099e-06, gnorm=7.657, clip=30, loss_scale=16, train_wall=12, gb_free=37.7, wall=19]2024-12-11 21:28:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.41it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:31 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.069 | nll_loss 7.007 | ppl 128.61 | wps 46276.9 | wpb 1564.7 | bsz 11.3 | num_updates 102\n",
            "2024-12-11 21:28:31 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2024-12-11 21:28:31 | INFO | train | epoch 007 | loss 7.21 | nll_loss 7.155 | ppl 142.57 | wps 17187.2 | ups 6.67 | wpb 2575.8 | bsz 18.1 | num_updates 102 | lr 5.19898e-06 | gnorm 1.728 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 20\n",
            "2024-12-11 21:28:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 008:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:31 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2024-12-11 21:28:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  87% 13/15 [00:01<00:00, 10.63it/s]2024-12-11 21:28:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:33 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.935 | nll_loss 6.866 | ppl 116.61 | wps 43967.4 | wpb 1564.7 | bsz 11.3 | num_updates 117\n",
            "2024-12-11 21:28:33 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2024-12-11 21:28:33 | INFO | train | epoch 008 | loss 7.052 | nll_loss 6.99 | ppl 127.08 | wps 17046.1 | ups 6.62 | wpb 2575.8 | bsz 18.1 | num_updates 117 | lr 5.94883e-06 | gnorm 1.327 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.6 | wall 22\n",
            "2024-12-11 21:28:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 009:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:33 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2024-12-11 21:28:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  93% 14/15 [00:01<00:00, 10.95it/s]2024-12-11 21:28:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:35 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.836 | nll_loss 6.76 | ppl 108.37 | wps 46113.5 | wpb 1564.7 | bsz 11.3 | num_updates 132\n",
            "2024-12-11 21:28:35 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2024-12-11 21:28:35 | INFO | train | epoch 009 | loss 6.933 | nll_loss 6.862 | ppl 116.33 | wps 17991.3 | ups 6.98 | wpb 2575.8 | bsz 18.1 | num_updates 132 | lr 6.69868e-06 | gnorm 1.053 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 24\n",
            "2024-12-11 21:28:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 010:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:36 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2024-12-11 21:28:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  93% 14/15 [00:01<00:00, 10.84it/s]2024-12-11 21:28:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.18it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:38 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.76 | nll_loss 6.678 | ppl 102.39 | wps 45679.2 | wpb 1564.7 | bsz 11.3 | num_updates 147\n",
            "2024-12-11 21:28:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 147 updates\n",
            "2024-12-11 21:28:38 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint10.pt\n",
            "2024-12-11 21:28:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint10.pt\n",
            "2024-12-11 21:28:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint10.pt (epoch 10 @ 147 updates, score 6.76) (writing took 2.117245173000015 seconds)\n",
            "2024-12-11 21:28:40 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2024-12-11 21:28:40 | INFO | train | epoch 010 | loss 6.834 | nll_loss 6.756 | ppl 108.11 | wps 8888.3 | ups 3.45 | wpb 2575.8 | bsz 18.1 | num_updates 147 | lr 7.44853e-06 | gnorm 0.862 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 29\n",
            "2024-12-11 21:28:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 011:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:40 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2024-12-11 21:28:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  87% 13/15 [00:01<00:00, 10.68it/s]2024-12-11 21:28:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.19it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:42 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.7 | nll_loss 6.613 | ppl 97.86 | wps 41980.1 | wpb 1564.7 | bsz 11.3 | num_updates 162 | best_loss 6.7\n",
            "2024-12-11 21:28:42 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2024-12-11 21:28:42 | INFO | train | epoch 011 | loss 6.764 | nll_loss 6.68 | ppl 102.57 | wps 17027.6 | ups 6.61 | wpb 2575.8 | bsz 18.1 | num_updates 162 | lr 8.19838e-06 | gnorm 0.727 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 31\n",
            "2024-12-11 21:28:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 012:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:42 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2024-12-11 21:28:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  93% 14/15 [00:01<00:00, 10.56it/s]2024-12-11 21:28:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.24it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:44 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.652 | nll_loss 6.558 | ppl 94.24 | wps 44955.5 | wpb 1564.7 | bsz 11.3 | num_updates 177 | best_loss 6.652\n",
            "2024-12-11 21:28:44 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2024-12-11 21:28:44 | INFO | train | epoch 012 | loss 6.706 | nll_loss 6.617 | ppl 98.16 | wps 17267.7 | ups 6.7 | wpb 2575.8 | bsz 18.1 | num_updates 177 | lr 8.94823e-06 | gnorm 0.628 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 33\n",
            "2024-12-11 21:28:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 013:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:44 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2024-12-11 21:28:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  93% 14/15 [00:01<00:00, 10.51it/s]2024-12-11 21:28:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.21it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:47 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.61 | nll_loss 6.512 | ppl 91.26 | wps 42805.9 | wpb 1564.7 | bsz 11.3 | num_updates 192 | best_loss 6.61\n",
            "2024-12-11 21:28:47 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2024-12-11 21:28:47 | INFO | train | epoch 013 | loss 6.654 | nll_loss 6.558 | ppl 94.25 | wps 17023.1 | ups 6.61 | wpb 2575.8 | bsz 18.1 | num_updates 192 | lr 9.69808e-06 | gnorm 0.556 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 36\n",
            "2024-12-11 21:28:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 014:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:47 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2024-12-11 21:28:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  87% 13/15 [00:01<00:00, 10.23it/s, loss=6.815, nll_loss=6.733, ppl=106.41, wps=14841.3, ups=5.77, wpb=2569.9, bsz=17.9, num_updates=200, lr=1.0098e-05, gnorm=0.845, clip=0, loss_scale=16, train_wall=9, gb_free=37.9, wall=37]2024-12-11 21:28:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:49 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.575 | nll_loss 6.47 | ppl 88.63 | wps 44578.6 | wpb 1564.7 | bsz 11.3 | num_updates 207 | best_loss 6.575\n",
            "2024-12-11 21:28:49 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2024-12-11 21:28:49 | INFO | train | epoch 014 | loss 6.615 | nll_loss 6.514 | ppl 91.38 | wps 16933.7 | ups 6.57 | wpb 2575.8 | bsz 18.1 | num_updates 207 | lr 1.04479e-05 | gnorm 0.505 | clip 0 | loss_scale 16 | train_wall 2 | gb_free 37.6 | wall 38\n",
            "2024-12-11 21:28:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 015:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:49 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2024-12-11 21:28:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  93% 14/15 [00:01<00:00, 10.80it/s]2024-12-11 21:28:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.32it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:51 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.541 | nll_loss 6.43 | ppl 86.22 | wps 45649.3 | wpb 1564.7 | bsz 11.3 | num_updates 222 | best_loss 6.541\n",
            "2024-12-11 21:28:51 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2024-12-11 21:28:51 | INFO | train | epoch 015 | loss 6.581 | nll_loss 6.473 | ppl 88.85 | wps 16838.9 | ups 6.54 | wpb 2575.8 | bsz 18.1 | num_updates 222 | lr 1.11978e-05 | gnorm 0.468 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 40\n",
            "2024-12-11 21:28:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 016:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:51 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2024-12-11 21:28:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  93% 14/15 [00:01<00:00, 10.81it/s]2024-12-11 21:28:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.28it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:53 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.51 | nll_loss 6.391 | ppl 83.93 | wps 46786.8 | wpb 1564.7 | bsz 11.3 | num_updates 237 | best_loss 6.51\n",
            "2024-12-11 21:28:53 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2024-12-11 21:28:53 | INFO | train | epoch 016 | loss 6.545 | nll_loss 6.43 | ppl 86.24 | wps 17278.8 | ups 6.71 | wpb 2575.8 | bsz 18.1 | num_updates 237 | lr 1.19476e-05 | gnorm 0.442 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 42\n",
            "2024-12-11 21:28:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 017:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:53 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2024-12-11 21:28:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  87% 13/15 [00:01<00:00, 10.87it/s]2024-12-11 21:28:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.39it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:56 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.477 | nll_loss 6.349 | ppl 81.54 | wps 45483.1 | wpb 1564.7 | bsz 11.3 | num_updates 252 | best_loss 6.477\n",
            "2024-12-11 21:28:56 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2024-12-11 21:28:56 | INFO | train | epoch 017 | loss 6.512 | nll_loss 6.389 | ppl 83.8 | wps 17686.1 | ups 6.87 | wpb 2575.8 | bsz 18.1 | num_updates 252 | lr 1.26975e-05 | gnorm 0.422 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 45\n",
            "2024-12-11 21:28:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 018:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:56 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2024-12-11 21:28:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  93% 14/15 [00:01<00:00, 10.82it/s]2024-12-11 21:28:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.32it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:28:58 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.439 | nll_loss 6.301 | ppl 78.84 | wps 44583.7 | wpb 1564.7 | bsz 11.3 | num_updates 267 | best_loss 6.439\n",
            "2024-12-11 21:28:58 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2024-12-11 21:28:58 | INFO | train | epoch 018 | loss 6.478 | nll_loss 6.346 | ppl 81.34 | wps 17430.5 | ups 6.77 | wpb 2575.8 | bsz 18.1 | num_updates 267 | lr 1.34473e-05 | gnorm 0.412 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.2 | wall 47\n",
            "2024-12-11 21:28:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 019:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:28:58 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2024-12-11 21:28:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  87% 13/15 [00:01<00:00, 10.60it/s]2024-12-11 21:29:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.07it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:00 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.389 | nll_loss 6.235 | ppl 75.32 | wps 45801 | wpb 1564.7 | bsz 11.3 | num_updates 282 | best_loss 6.389\n",
            "2024-12-11 21:29:00 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2024-12-11 21:29:00 | INFO | train | epoch 019 | loss 6.437 | nll_loss 6.294 | ppl 78.46 | wps 17601.7 | ups 6.83 | wpb 2575.8 | bsz 18.1 | num_updates 282 | lr 1.41972e-05 | gnorm 0.415 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 49\n",
            "2024-12-11 21:29:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 020:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:00 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2024-12-11 21:29:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  93% 14/15 [00:01<00:00, 10.76it/s]2024-12-11 21:29:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.19it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:02 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.309 | nll_loss 6.125 | ppl 69.78 | wps 43621.2 | wpb 1564.7 | bsz 11.3 | num_updates 297 | best_loss 6.309\n",
            "2024-12-11 21:29:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 297 updates\n",
            "2024-12-11 21:29:02 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint20.pt\n",
            "2024-12-11 21:29:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint20.pt\n",
            "2024-12-11 21:29:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint20.pt (epoch 20 @ 297 updates, score 6.309) (writing took 2.656565426000043 seconds)\n",
            "2024-12-11 21:29:05 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2024-12-11 21:29:05 | INFO | train | epoch 020 | loss 6.382 | nll_loss 6.219 | ppl 74.52 | wps 7919.5 | ups 3.07 | wpb 2575.8 | bsz 18.1 | num_updates 297 | lr 1.4947e-05 | gnorm 0.439 | clip 0 | loss_scale 16 | train_wall 2 | gb_free 37.6 | wall 54\n",
            "2024-12-11 21:29:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 021:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:05 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2024-12-11 21:29:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  93% 14/15 [00:01<00:00, 10.12it/s, loss=6.494, nll_loss=6.364, ppl=82.36, wps=14292.7, ups=5.55, wpb=2573.4, bsz=18.1, num_updates=300, lr=1.5097e-05, gnorm=0.44, clip=0, loss_scale=16, train_wall=9, gb_free=37.7, wall=55]2024-12-11 21:29:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.14it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:07 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.175 | nll_loss 5.932 | ppl 61.07 | wps 43862.1 | wpb 1564.7 | bsz 11.3 | num_updates 312 | best_loss 6.175\n",
            "2024-12-11 21:29:07 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2024-12-11 21:29:07 | INFO | train | epoch 021 | loss 6.297 | nll_loss 6.097 | ppl 68.47 | wps 15948.8 | ups 6.19 | wpb 2575.8 | bsz 18.1 | num_updates 312 | lr 1.56969e-05 | gnorm 0.492 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 56\n",
            "2024-12-11 21:29:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 022:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:07 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2024-12-11 21:29:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  93% 14/15 [00:01<00:00, 10.48it/s]2024-12-11 21:29:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:10 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6 | nll_loss 5.683 | ppl 51.36 | wps 45234.6 | wpb 1564.7 | bsz 11.3 | num_updates 327 | best_loss 6\n",
            "2024-12-11 21:29:10 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2024-12-11 21:29:10 | INFO | train | epoch 022 | loss 6.16 | nll_loss 5.905 | ppl 59.9 | wps 16727.5 | ups 6.49 | wpb 2575.8 | bsz 18.1 | num_updates 327 | lr 1.64467e-05 | gnorm 0.554 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 59\n",
            "2024-12-11 21:29:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 023:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:10 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2024-12-11 21:29:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  87% 13/15 [00:01<00:00, 10.66it/s]2024-12-11 21:29:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:12 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.772 | nll_loss 5.362 | ppl 41.14 | wps 44941.4 | wpb 1564.7 | bsz 11.3 | num_updates 342 | best_loss 5.772\n",
            "2024-12-11 21:29:12 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2024-12-11 21:29:12 | INFO | train | epoch 023 | loss 5.978 | nll_loss 5.65 | ppl 50.21 | wps 17641.4 | ups 6.85 | wpb 2575.8 | bsz 18.1 | num_updates 342 | lr 1.71966e-05 | gnorm 0.618 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 61\n",
            "2024-12-11 21:29:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 024:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:12 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2024-12-11 21:29:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  87% 13/15 [00:01<00:00, 10.79it/s]2024-12-11 21:29:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.14it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:14 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.504 | nll_loss 4.983 | ppl 31.62 | wps 45079.9 | wpb 1564.7 | bsz 11.3 | num_updates 357 | best_loss 5.504\n",
            "2024-12-11 21:29:14 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2024-12-11 21:29:14 | INFO | train | epoch 024 | loss 5.746 | nll_loss 5.323 | ppl 40.02 | wps 17834.2 | ups 6.92 | wpb 2575.8 | bsz 18.1 | num_updates 357 | lr 1.79464e-05 | gnorm 0.667 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 63\n",
            "2024-12-11 21:29:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 025:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:14 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2024-12-11 21:29:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  87% 13/15 [00:01<00:00, 10.63it/s]2024-12-11 21:29:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:16 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.237 | nll_loss 4.599 | ppl 24.23 | wps 45295.7 | wpb 1564.7 | bsz 11.3 | num_updates 372 | best_loss 5.237\n",
            "2024-12-11 21:29:16 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2024-12-11 21:29:16 | INFO | train | epoch 025 | loss 5.505 | nll_loss 4.983 | ppl 31.63 | wps 17047.5 | ups 6.62 | wpb 2575.8 | bsz 18.1 | num_updates 372 | lr 1.86963e-05 | gnorm 0.727 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 65\n",
            "2024-12-11 21:29:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 026:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:16 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2024-12-11 21:29:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  93% 14/15 [00:01<00:00, 10.30it/s]2024-12-11 21:29:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.15it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:19 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.017 | nll_loss 4.266 | ppl 19.24 | wps 44021.5 | wpb 1564.7 | bsz 11.3 | num_updates 387 | best_loss 5.017\n",
            "2024-12-11 21:29:19 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2024-12-11 21:29:19 | INFO | train | epoch 026 | loss 5.245 | nll_loss 4.609 | ppl 24.4 | wps 16805.7 | ups 6.52 | wpb 2575.8 | bsz 18.1 | num_updates 387 | lr 1.94461e-05 | gnorm 0.745 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 68\n",
            "2024-12-11 21:29:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 027:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:19 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2024-12-11 21:29:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  93% 14/15 [00:01<00:00, 10.79it/s, loss=5.71, nll_loss=5.268, ppl=38.53, wps=17903.3, ups=6.95, wpb=2575.7, bsz=18.1, num_updates=400, lr=2.0096e-05, gnorm=0.653, clip=0, loss_scale=16, train_wall=9, gb_free=37.9, wall=69]2024-12-11 21:29:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.15it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:21 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.847 | nll_loss 3.995 | ppl 15.95 | wps 45986.2 | wpb 1564.7 | bsz 11.3 | num_updates 402 | best_loss 4.847\n",
            "2024-12-11 21:29:21 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2024-12-11 21:29:21 | INFO | train | epoch 027 | loss 5.037 | nll_loss 4.302 | ppl 19.73 | wps 17385.6 | ups 6.75 | wpb 2575.8 | bsz 18.1 | num_updates 402 | lr 2.0196e-05 | gnorm 0.75 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 70\n",
            "2024-12-11 21:29:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 028:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:21 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2024-12-11 21:29:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  93% 14/15 [00:01<00:00, 10.63it/s]2024-12-11 21:29:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.23it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:23 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.736 | nll_loss 3.82 | ppl 14.13 | wps 45785.5 | wpb 1564.7 | bsz 11.3 | num_updates 417 | best_loss 4.736\n",
            "2024-12-11 21:29:23 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2024-12-11 21:29:23 | INFO | train | epoch 028 | loss 4.885 | nll_loss 4.078 | ppl 16.89 | wps 17025.1 | ups 6.61 | wpb 2575.8 | bsz 18.1 | num_updates 417 | lr 2.09458e-05 | gnorm 0.712 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 72\n",
            "2024-12-11 21:29:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 029:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:23 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2024-12-11 21:29:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  93% 14/15 [00:01<00:00, 10.91it/s]2024-12-11 21:29:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.28it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.651 | nll_loss 3.695 | ppl 12.96 | wps 44786.5 | wpb 1564.7 | bsz 11.3 | num_updates 432 | best_loss 4.651\n",
            "2024-12-11 21:29:25 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2024-12-11 21:29:25 | INFO | train | epoch 029 | loss 4.774 | nll_loss 3.915 | ppl 15.08 | wps 17887 | ups 6.94 | wpb 2575.8 | bsz 18.1 | num_updates 432 | lr 2.16957e-05 | gnorm 0.71 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 74\n",
            "2024-12-11 21:29:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 030:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:25 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2024-12-11 21:29:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  87% 13/15 [00:01<00:00, 10.44it/s]2024-12-11 21:29:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:27 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.587 | nll_loss 3.612 | ppl 12.23 | wps 46654.9 | wpb 1564.7 | bsz 11.3 | num_updates 447 | best_loss 4.587\n",
            "2024-12-11 21:29:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 447 updates\n",
            "2024-12-11 21:29:27 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint30.pt\n",
            "2024-12-11 21:29:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint30.pt\n",
            "2024-12-11 21:29:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint30.pt (epoch 30 @ 447 updates, score 4.587) (writing took 2.3281545149999374 seconds)\n",
            "2024-12-11 21:29:30 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2024-12-11 21:29:30 | INFO | train | epoch 030 | loss 4.692 | nll_loss 3.796 | ppl 13.89 | wps 8383.8 | ups 3.25 | wpb 2575.8 | bsz 18.1 | num_updates 447 | lr 2.24455e-05 | gnorm 0.704 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 79\n",
            "2024-12-11 21:29:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 031:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:30 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2024-12-11 21:29:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  93% 14/15 [00:01<00:00, 10.65it/s]2024-12-11 21:29:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:32 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.54 | nll_loss 3.534 | ppl 11.58 | wps 45082.7 | wpb 1564.7 | bsz 11.3 | num_updates 462 | best_loss 4.54\n",
            "2024-12-11 21:29:32 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2024-12-11 21:29:32 | INFO | train | epoch 031 | loss 4.63 | nll_loss 3.707 | ppl 13.06 | wps 16518.8 | ups 6.41 | wpb 2575.8 | bsz 18.1 | num_updates 462 | lr 2.31954e-05 | gnorm 0.695 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 81\n",
            "2024-12-11 21:29:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 032:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:32 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2024-12-11 21:29:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  93% 14/15 [00:01<00:00, 10.49it/s]2024-12-11 21:29:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:34 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.499 | nll_loss 3.479 | ppl 11.15 | wps 46518.6 | wpb 1564.7 | bsz 11.3 | num_updates 477 | best_loss 4.499\n",
            "2024-12-11 21:29:34 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2024-12-11 21:29:34 | INFO | train | epoch 032 | loss 4.581 | nll_loss 3.635 | ppl 12.42 | wps 16925.4 | ups 6.57 | wpb 2575.8 | bsz 18.1 | num_updates 477 | lr 2.39452e-05 | gnorm 0.681 | clip 0 | loss_scale 16 | train_wall 2 | gb_free 37.9 | wall 83\n",
            "2024-12-11 21:29:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 033:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:35 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2024-12-11 21:29:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  93% 14/15 [00:01<00:00, 10.66it/s]2024-12-11 21:29:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.30it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:37 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.475 | nll_loss 3.427 | ppl 10.75 | wps 45645.1 | wpb 1564.7 | bsz 11.3 | num_updates 492 | best_loss 4.475\n",
            "2024-12-11 21:29:37 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2024-12-11 21:29:37 | INFO | train | epoch 033 | loss 4.54 | nll_loss 3.574 | ppl 11.91 | wps 17112.8 | ups 6.64 | wpb 2575.8 | bsz 18.1 | num_updates 492 | lr 2.46951e-05 | gnorm 0.697 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 86\n",
            "2024-12-11 21:29:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 034:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:37 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2024-12-11 21:29:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  87% 13/15 [00:01<00:00, 10.30it/s, loss=4.676, nll_loss=3.773, ppl=13.67, wps=14537.7, ups=5.62, wpb=2585.1, bsz=18.1, num_updates=500, lr=2.5095e-05, gnorm=0.699, clip=0, loss_scale=16, train_wall=9, gb_free=37.9, wall=87]2024-12-11 21:29:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:39 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.449 | nll_loss 3.384 | ppl 10.44 | wps 46185.1 | wpb 1564.7 | bsz 11.3 | num_updates 507 | best_loss 4.449\n",
            "2024-12-11 21:29:39 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2024-12-11 21:29:39 | INFO | train | epoch 034 | loss 4.508 | nll_loss 3.531 | ppl 11.56 | wps 16756.4 | ups 6.51 | wpb 2575.8 | bsz 18.1 | num_updates 507 | lr 2.54449e-05 | gnorm 0.698 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 88\n",
            "2024-12-11 21:29:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 035:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:39 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-12-11 21:29:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  93% 14/15 [00:01<00:00, 10.84it/s]2024-12-11 21:29:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.24it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:41 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.424 | nll_loss 3.363 | ppl 10.29 | wps 45777.2 | wpb 1564.7 | bsz 11.3 | num_updates 522 | best_loss 4.424\n",
            "2024-12-11 21:29:41 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-12-11 21:29:41 | INFO | train | epoch 035 | loss 4.478 | nll_loss 3.487 | ppl 11.21 | wps 17722.8 | ups 6.88 | wpb 2575.8 | bsz 18.1 | num_updates 522 | lr 2.61948e-05 | gnorm 0.703 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 90\n",
            "2024-12-11 21:29:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 036:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:41 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2024-12-11 21:29:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  93% 14/15 [00:01<00:00, 10.99it/s]2024-12-11 21:29:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:43 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.407 | nll_loss 3.324 | ppl 10.01 | wps 46927.3 | wpb 1564.7 | bsz 11.3 | num_updates 537 | best_loss 4.407\n",
            "2024-12-11 21:29:43 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2024-12-11 21:29:43 | INFO | train | epoch 036 | loss 4.454 | nll_loss 3.455 | ppl 10.97 | wps 17623.5 | ups 6.84 | wpb 2575.8 | bsz 18.1 | num_updates 537 | lr 2.69446e-05 | gnorm 0.665 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 92\n",
            "2024-12-11 21:29:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 037:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:43 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2024-12-11 21:29:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  93% 14/15 [00:01<00:00, 10.80it/s]2024-12-11 21:29:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:46 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.393 | nll_loss 3.294 | ppl 9.81 | wps 45797.6 | wpb 1564.7 | bsz 11.3 | num_updates 552 | best_loss 4.393\n",
            "2024-12-11 21:29:46 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2024-12-11 21:29:46 | INFO | train | epoch 037 | loss 4.435 | nll_loss 3.426 | ppl 10.75 | wps 17522.1 | ups 6.8 | wpb 2575.8 | bsz 18.1 | num_updates 552 | lr 2.76945e-05 | gnorm 0.691 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 95\n",
            "2024-12-11 21:29:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 038:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:46 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2024-12-11 21:29:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  93% 14/15 [00:01<00:00, 10.92it/s]2024-12-11 21:29:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.28it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:48 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.383 | nll_loss 3.264 | ppl 9.61 | wps 46553.4 | wpb 1564.7 | bsz 11.3 | num_updates 567 | best_loss 4.383\n",
            "2024-12-11 21:29:48 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2024-12-11 21:29:48 | INFO | train | epoch 038 | loss 4.415 | nll_loss 3.4 | ppl 10.55 | wps 17433.2 | ups 6.77 | wpb 2575.8 | bsz 18.1 | num_updates 567 | lr 2.84443e-05 | gnorm 0.664 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 97\n",
            "2024-12-11 21:29:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 039:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:48 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2024-12-11 21:29:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  93% 14/15 [00:01<00:00, 10.60it/s]2024-12-11 21:29:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.17it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:50 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.359 | nll_loss 3.254 | ppl 9.54 | wps 41887.3 | wpb 1564.7 | bsz 11.3 | num_updates 582 | best_loss 4.359\n",
            "2024-12-11 21:29:50 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2024-12-11 21:29:50 | INFO | train | epoch 039 | loss 4.396 | nll_loss 3.371 | ppl 10.35 | wps 17283 | ups 6.71 | wpb 2575.8 | bsz 18.1 | num_updates 582 | lr 2.91942e-05 | gnorm 0.677 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 99\n",
            "2024-12-11 21:29:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 040:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:50 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2024-12-11 21:29:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  87% 13/15 [00:01<00:00, 10.33it/s]2024-12-11 21:29:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  3.96it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.345 | nll_loss 3.246 | ppl 9.49 | wps 35222.1 | wpb 1564.7 | bsz 11.3 | num_updates 597 | best_loss 4.345\n",
            "2024-12-11 21:29:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 597 updates\n",
            "2024-12-11 21:29:52 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint40.pt\n",
            "2024-12-11 21:29:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint40.pt\n",
            "2024-12-11 21:29:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint40.pt (epoch 40 @ 597 updates, score 4.345) (writing took 2.3331614799999443 seconds)\n",
            "2024-12-11 21:29:55 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2024-12-11 21:29:55 | INFO | train | epoch 040 | loss 4.38 | nll_loss 3.348 | ppl 10.18 | wps 8222.6 | ups 3.19 | wpb 2575.8 | bsz 18.1 | num_updates 597 | lr 2.9944e-05 | gnorm 0.662 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.1 | wall 104\n",
            "2024-12-11 21:29:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 041:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:55 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2024-12-11 21:29:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  93% 14/15 [00:01<00:00, 10.67it/s, loss=4.43, nll_loss=3.419, ppl=10.7, wps=14682, ups=5.7, wpb=2575.6, bsz=18.1, num_updates=600, lr=3.0094e-05, gnorm=0.678, clip=0, loss_scale=16, train_wall=9, gb_free=38.1, wall=104]2024-12-11 21:29:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:57 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.34 | nll_loss 3.229 | ppl 9.38 | wps 45600.2 | wpb 1564.7 | bsz 11.3 | num_updates 612 | best_loss 4.34\n",
            "2024-12-11 21:29:57 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2024-12-11 21:29:57 | INFO | train | epoch 041 | loss 4.371 | nll_loss 3.337 | ppl 10.11 | wps 16981.6 | ups 6.59 | wpb 2575.8 | bsz 18.1 | num_updates 612 | lr 3.06939e-05 | gnorm 0.695 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 106\n",
            "2024-12-11 21:29:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 042:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:57 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2024-12-11 21:29:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  87% 13/15 [00:01<00:00, 10.61it/s]2024-12-11 21:29:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.24it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:29:59 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.335 | nll_loss 3.221 | ppl 9.32 | wps 45271.2 | wpb 1564.7 | bsz 11.3 | num_updates 627 | best_loss 4.335\n",
            "2024-12-11 21:29:59 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2024-12-11 21:29:59 | INFO | train | epoch 042 | loss 4.356 | nll_loss 3.318 | ppl 9.97 | wps 17335.9 | ups 6.73 | wpb 2575.8 | bsz 18.1 | num_updates 627 | lr 3.14437e-05 | gnorm 0.686 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 108\n",
            "2024-12-11 21:29:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 043:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:29:59 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2024-12-11 21:29:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  93% 14/15 [00:01<00:00, 10.87it/s]2024-12-11 21:30:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.21it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:01 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.328 | nll_loss 3.201 | ppl 9.19 | wps 45809.4 | wpb 1564.7 | bsz 11.3 | num_updates 642 | best_loss 4.328\n",
            "2024-12-11 21:30:01 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2024-12-11 21:30:01 | INFO | train | epoch 043 | loss 4.344 | nll_loss 3.301 | ppl 9.86 | wps 17713.8 | ups 6.88 | wpb 2575.8 | bsz 18.1 | num_updates 642 | lr 3.21936e-05 | gnorm 0.677 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 110\n",
            "2024-12-11 21:30:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 044:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:01 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2024-12-11 21:30:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  93% 14/15 [00:01<00:00, 10.88it/s]2024-12-11 21:30:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:04 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.318 | nll_loss 3.185 | ppl 9.09 | wps 46513.5 | wpb 1564.7 | bsz 11.3 | num_updates 657 | best_loss 4.318\n",
            "2024-12-11 21:30:04 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2024-12-11 21:30:04 | INFO | train | epoch 044 | loss 4.332 | nll_loss 3.284 | ppl 9.74 | wps 17386.4 | ups 6.75 | wpb 2575.8 | bsz 18.1 | num_updates 657 | lr 3.29434e-05 | gnorm 0.662 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 113\n",
            "2024-12-11 21:30:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 045:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:04 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2024-12-11 21:30:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  93% 14/15 [00:01<00:00, 10.61it/s]2024-12-11 21:30:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.31it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:06 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.307 | nll_loss 3.184 | ppl 9.09 | wps 45145 | wpb 1564.7 | bsz 11.3 | num_updates 672 | best_loss 4.307\n",
            "2024-12-11 21:30:06 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2024-12-11 21:30:06 | INFO | train | epoch 045 | loss 4.325 | nll_loss 3.276 | ppl 9.68 | wps 17507.6 | ups 6.8 | wpb 2575.8 | bsz 18.1 | num_updates 672 | lr 3.36933e-05 | gnorm 0.659 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.2 | wall 115\n",
            "2024-12-11 21:30:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 046:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:06 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2024-12-11 21:30:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  87% 13/15 [00:01<00:00, 10.12it/s]2024-12-11 21:30:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:08 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.295 | nll_loss 3.171 | ppl 9.01 | wps 45275 | wpb 1564.7 | bsz 11.3 | num_updates 687 | best_loss 4.295\n",
            "2024-12-11 21:30:08 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2024-12-11 21:30:08 | INFO | train | epoch 046 | loss 4.313 | nll_loss 3.253 | ppl 9.53 | wps 16110.9 | ups 6.25 | wpb 2575.8 | bsz 18.1 | num_updates 687 | lr 3.44431e-05 | gnorm 0.659 | clip 0 | loss_scale 16 | train_wall 2 | gb_free 37.5 | wall 117\n",
            "2024-12-11 21:30:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 047:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:08 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2024-12-11 21:30:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  93% 14/15 [00:01<00:00, 10.76it/s, loss=4.335, nll_loss=3.288, ppl=9.77, wps=17873.7, ups=6.91, wpb=2585, bsz=18.2, num_updates=700, lr=3.5093e-05, gnorm=0.672, clip=0, loss_scale=16, train_wall=9, gb_free=37.7, wall=119]2024-12-11 21:30:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.22it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:11 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 4.301 | nll_loss 3.164 | ppl 8.97 | wps 44962 | wpb 1564.7 | bsz 11.3 | num_updates 702 | best_loss 4.301\n",
            "2024-12-11 21:30:11 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2024-12-11 21:30:11 | INFO | train | epoch 047 | loss 4.305 | nll_loss 3.249 | ppl 9.51 | wps 16555.2 | ups 6.43 | wpb 2575.8 | bsz 18.1 | num_updates 702 | lr 3.5193e-05 | gnorm 0.661 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.1 | wall 120\n",
            "2024-12-11 21:30:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 048:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:11 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2024-12-11 21:30:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  93% 14/15 [00:01<00:00, 10.87it/s]2024-12-11 21:30:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:13 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 4.295 | nll_loss 3.158 | ppl 8.93 | wps 44972.4 | wpb 1564.7 | bsz 11.3 | num_updates 717 | best_loss 4.295\n",
            "2024-12-11 21:30:13 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2024-12-11 21:30:13 | INFO | train | epoch 048 | loss 4.299 | nll_loss 3.24 | ppl 9.45 | wps 17471.3 | ups 6.78 | wpb 2575.8 | bsz 18.1 | num_updates 717 | lr 3.59428e-05 | gnorm 0.674 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 122\n",
            "2024-12-11 21:30:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 049:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:13 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2024-12-11 21:30:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  87% 13/15 [00:01<00:00, 10.71it/s]2024-12-11 21:30:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.22it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:15 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 4.291 | nll_loss 3.145 | ppl 8.84 | wps 44287.2 | wpb 1564.7 | bsz 11.3 | num_updates 732 | best_loss 4.291\n",
            "2024-12-11 21:30:15 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2024-12-11 21:30:15 | INFO | train | epoch 049 | loss 4.295 | nll_loss 3.233 | ppl 9.4 | wps 17548.2 | ups 6.81 | wpb 2575.8 | bsz 18.1 | num_updates 732 | lr 3.66927e-05 | gnorm 0.694 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.1 | wall 124\n",
            "2024-12-11 21:30:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 050:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:15 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2024-12-11 21:30:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  93% 14/15 [00:01<00:00, 10.28it/s]2024-12-11 21:30:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.23it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:17 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 4.289 | nll_loss 3.152 | ppl 8.89 | wps 45376.2 | wpb 1564.7 | bsz 11.3 | num_updates 747 | best_loss 4.289\n",
            "2024-12-11 21:30:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 747 updates\n",
            "2024-12-11 21:30:17 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint50.pt\n",
            "2024-12-11 21:30:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint50.pt\n",
            "2024-12-11 21:30:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint50.pt (epoch 50 @ 747 updates, score 4.289) (writing took 2.655896592999966 seconds)\n",
            "2024-12-11 21:30:20 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2024-12-11 21:30:20 | INFO | train | epoch 050 | loss 4.281 | nll_loss 3.213 | ppl 9.27 | wps 7789.8 | ups 3.02 | wpb 2575.8 | bsz 18.1 | num_updates 747 | lr 3.74425e-05 | gnorm 0.669 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.6 | wall 129\n",
            "2024-12-11 21:30:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 051:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:20 | INFO | fairseq.trainer | begin training epoch 51\n",
            "2024-12-11 21:30:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 051:  87% 13/15 [00:01<00:00, 10.26it/s]2024-12-11 21:30:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 051 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.21it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:22 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 4.272 | nll_loss 3.16 | ppl 8.94 | wps 44170 | wpb 1564.7 | bsz 11.3 | num_updates 762 | best_loss 4.272\n",
            "2024-12-11 21:30:22 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
            "2024-12-11 21:30:22 | INFO | train | epoch 051 | loss 4.279 | nll_loss 3.209 | ppl 9.25 | wps 16870 | ups 6.55 | wpb 2575.8 | bsz 18.1 | num_updates 762 | lr 3.81924e-05 | gnorm 0.702 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 131\n",
            "2024-12-11 21:30:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 052:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:22 | INFO | fairseq.trainer | begin training epoch 52\n",
            "2024-12-11 21:30:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 052:  93% 14/15 [00:01<00:00, 10.90it/s]2024-12-11 21:30:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 052 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.34it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:24 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 4.283 | nll_loss 3.135 | ppl 8.79 | wps 47021 | wpb 1564.7 | bsz 11.3 | num_updates 777 | best_loss 4.283\n",
            "2024-12-11 21:30:24 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
            "2024-12-11 21:30:24 | INFO | train | epoch 052 | loss 4.27 | nll_loss 3.201 | ppl 9.2 | wps 17134 | ups 6.65 | wpb 2575.8 | bsz 18.1 | num_updates 777 | lr 3.89422e-05 | gnorm 0.672 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 133\n",
            "2024-12-11 21:30:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 053:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:25 | INFO | fairseq.trainer | begin training epoch 53\n",
            "2024-12-11 21:30:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 053:  93% 14/15 [00:01<00:00, 10.52it/s]2024-12-11 21:30:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 053 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.24it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:27 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 4.274 | nll_loss 3.132 | ppl 8.77 | wps 44551.7 | wpb 1564.7 | bsz 11.3 | num_updates 792 | best_loss 4.274\n",
            "2024-12-11 21:30:27 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
            "2024-12-11 21:30:27 | INFO | train | epoch 053 | loss 4.26 | nll_loss 3.183 | ppl 9.08 | wps 17565.2 | ups 6.82 | wpb 2575.8 | bsz 18.1 | num_updates 792 | lr 3.96921e-05 | gnorm 0.705 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 136\n",
            "2024-12-11 21:30:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 054:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:27 | INFO | fairseq.trainer | begin training epoch 54\n",
            "2024-12-11 21:30:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 054:  87% 13/15 [00:01<00:00, 10.42it/s, loss=4.279, nll_loss=3.211, ppl=9.26, wps=14281, ups=5.56, wpb=2568.2, bsz=17.9, num_updates=800, lr=4.0092e-05, gnorm=0.681, clip=0, loss_scale=16, train_wall=9, gb_free=37.9, wall=137]2024-12-11 21:30:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 054 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.21it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:29 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 4.266 | nll_loss 3.129 | ppl 8.75 | wps 46284.7 | wpb 1564.7 | bsz 11.3 | num_updates 807 | best_loss 4.266\n",
            "2024-12-11 21:30:29 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
            "2024-12-11 21:30:29 | INFO | train | epoch 054 | loss 4.252 | nll_loss 3.176 | ppl 9.04 | wps 17002 | ups 6.6 | wpb 2575.8 | bsz 18.1 | num_updates 807 | lr 4.04419e-05 | gnorm 0.637 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.2 | wall 138\n",
            "2024-12-11 21:30:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 055:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:29 | INFO | fairseq.trainer | begin training epoch 55\n",
            "2024-12-11 21:30:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 055:  87% 13/15 [00:01<00:00, 10.59it/s]2024-12-11 21:30:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 055 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.18it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:31 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 4.262 | nll_loss 3.116 | ppl 8.67 | wps 44615.3 | wpb 1564.7 | bsz 11.3 | num_updates 822 | best_loss 4.262\n",
            "2024-12-11 21:30:31 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
            "2024-12-11 21:30:31 | INFO | train | epoch 055 | loss 4.249 | nll_loss 3.168 | ppl 8.99 | wps 17502.6 | ups 6.8 | wpb 2575.8 | bsz 18.1 | num_updates 822 | lr 4.11918e-05 | gnorm 0.673 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 140\n",
            "2024-12-11 21:30:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 056:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:31 | INFO | fairseq.trainer | begin training epoch 56\n",
            "2024-12-11 21:30:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 056:  93% 14/15 [00:01<00:00, 10.87it/s]2024-12-11 21:30:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 056 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.25it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:33 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 4.258 | nll_loss 3.111 | ppl 8.64 | wps 43197.9 | wpb 1564.7 | bsz 11.3 | num_updates 837 | best_loss 4.258\n",
            "2024-12-11 21:30:33 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
            "2024-12-11 21:30:33 | INFO | train | epoch 056 | loss 4.244 | nll_loss 3.166 | ppl 8.97 | wps 17043 | ups 6.62 | wpb 2575.8 | bsz 18.1 | num_updates 837 | lr 4.19416e-05 | gnorm 0.659 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 142\n",
            "2024-12-11 21:30:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 057:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:33 | INFO | fairseq.trainer | begin training epoch 57\n",
            "2024-12-11 21:30:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 057:  93% 14/15 [00:01<00:00, 10.90it/s]2024-12-11 21:30:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 057 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.25it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:36 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 4.265 | nll_loss 3.11 | ppl 8.63 | wps 46218.8 | wpb 1564.7 | bsz 11.3 | num_updates 852 | best_loss 4.265\n",
            "2024-12-11 21:30:36 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
            "2024-12-11 21:30:36 | INFO | train | epoch 057 | loss 4.236 | nll_loss 3.15 | ppl 8.88 | wps 17850.1 | ups 6.93 | wpb 2575.8 | bsz 18.1 | num_updates 852 | lr 4.26915e-05 | gnorm 0.651 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 145\n",
            "2024-12-11 21:30:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 058:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:36 | INFO | fairseq.trainer | begin training epoch 58\n",
            "2024-12-11 21:30:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 058:  87% 13/15 [00:01<00:00, 10.60it/s]2024-12-11 21:30:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 058 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:38 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 4.249 | nll_loss 3.115 | ppl 8.66 | wps 43571.8 | wpb 1564.7 | bsz 11.3 | num_updates 867 | best_loss 4.249\n",
            "2024-12-11 21:30:38 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
            "2024-12-11 21:30:38 | INFO | train | epoch 058 | loss 4.227 | nll_loss 3.136 | ppl 8.79 | wps 17490.6 | ups 6.79 | wpb 2575.8 | bsz 18.1 | num_updates 867 | lr 4.34413e-05 | gnorm 0.659 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 147\n",
            "2024-12-11 21:30:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 059:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:38 | INFO | fairseq.trainer | begin training epoch 59\n",
            "2024-12-11 21:30:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 059:  87% 13/15 [00:01<00:00, 10.33it/s]2024-12-11 21:30:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 059 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.00it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:40 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 4.248 | nll_loss 3.112 | ppl 8.65 | wps 45100.6 | wpb 1564.7 | bsz 11.3 | num_updates 882 | best_loss 4.248\n",
            "2024-12-11 21:30:40 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
            "2024-12-11 21:30:40 | INFO | train | epoch 059 | loss 4.223 | nll_loss 3.14 | ppl 8.81 | wps 16629.4 | ups 6.46 | wpb 2575.8 | bsz 18.1 | num_updates 882 | lr 4.41912e-05 | gnorm 0.686 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.2 | wall 149\n",
            "2024-12-11 21:30:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 060:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:40 | INFO | fairseq.trainer | begin training epoch 60\n",
            "2024-12-11 21:30:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 060:  87% 13/15 [00:01<00:00, 10.59it/s]2024-12-11 21:30:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 060 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.25it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:42 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 4.256 | nll_loss 3.105 | ppl 8.61 | wps 44404.7 | wpb 1564.7 | bsz 11.3 | num_updates 897 | best_loss 4.256\n",
            "2024-12-11 21:30:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 897 updates\n",
            "2024-12-11 21:30:42 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint60.pt\n",
            "2024-12-11 21:30:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint60.pt\n",
            "2024-12-11 21:30:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint60.pt (epoch 60 @ 897 updates, score 4.256) (writing took 2.303917167999998 seconds)\n",
            "2024-12-11 21:30:45 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
            "2024-12-11 21:30:45 | INFO | train | epoch 060 | loss 4.218 | nll_loss 3.127 | ppl 8.74 | wps 8340.9 | ups 3.24 | wpb 2575.8 | bsz 18.1 | num_updates 897 | lr 4.4941e-05 | gnorm 0.68 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 154\n",
            "2024-12-11 21:30:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 061:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:45 | INFO | fairseq.trainer | begin training epoch 61\n",
            "2024-12-11 21:30:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 061:  93% 14/15 [00:01<00:00, 10.68it/s, loss=4.233, nll_loss=3.148, ppl=8.87, wps=14554.2, ups=5.71, wpb=2549.5, bsz=18, num_updates=900, lr=4.5091e-05, gnorm=0.671, clip=0, loss_scale=16, train_wall=9, gb_free=37.7, wall=154]2024-12-11 21:30:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 061 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:47 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 4.247 | nll_loss 3.1 | ppl 8.57 | wps 44046 | wpb 1564.7 | bsz 11.3 | num_updates 912 | best_loss 4.247\n",
            "2024-12-11 21:30:47 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
            "2024-12-11 21:30:47 | INFO | train | epoch 061 | loss 4.209 | nll_loss 3.115 | ppl 8.66 | wps 17425.6 | ups 6.77 | wpb 2575.8 | bsz 18.1 | num_updates 912 | lr 4.56909e-05 | gnorm 0.66 | clip 0 | loss_scale 16 | train_wall 2 | gb_free 37.7 | wall 156\n",
            "2024-12-11 21:30:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 062:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:47 | INFO | fairseq.trainer | begin training epoch 62\n",
            "2024-12-11 21:30:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 062:  93% 14/15 [00:01<00:00, 10.76it/s]2024-12-11 21:30:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 062 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:49 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 4.237 | nll_loss 3.094 | ppl 8.54 | wps 45880.4 | wpb 1564.7 | bsz 11.3 | num_updates 927 | best_loss 4.237\n",
            "2024-12-11 21:30:49 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
            "2024-12-11 21:30:49 | INFO | train | epoch 062 | loss 4.207 | nll_loss 3.111 | ppl 8.64 | wps 16889.5 | ups 6.56 | wpb 2575.8 | bsz 18.1 | num_updates 927 | lr 4.64407e-05 | gnorm 0.663 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 158\n",
            "2024-12-11 21:30:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 063:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:49 | INFO | fairseq.trainer | begin training epoch 63\n",
            "2024-12-11 21:30:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 063:  87% 13/15 [00:01<00:00, 10.58it/s]2024-12-11 21:30:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 063 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:51 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 4.244 | nll_loss 3.097 | ppl 8.56 | wps 46963.7 | wpb 1564.7 | bsz 11.3 | num_updates 942 | best_loss 4.244\n",
            "2024-12-11 21:30:51 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
            "2024-12-11 21:30:51 | INFO | train | epoch 063 | loss 4.196 | nll_loss 3.102 | ppl 8.59 | wps 17387.7 | ups 6.75 | wpb 2575.8 | bsz 18.1 | num_updates 942 | lr 4.71906e-05 | gnorm 0.662 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 160\n",
            "2024-12-11 21:30:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 064:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:52 | INFO | fairseq.trainer | begin training epoch 64\n",
            "2024-12-11 21:30:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 064:  87% 13/15 [00:01<00:00, 10.72it/s]2024-12-11 21:30:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 064 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.28it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:54 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 4.246 | nll_loss 3.083 | ppl 8.47 | wps 46188.4 | wpb 1564.7 | bsz 11.3 | num_updates 957 | best_loss 4.246\n",
            "2024-12-11 21:30:54 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
            "2024-12-11 21:30:54 | INFO | train | epoch 064 | loss 4.192 | nll_loss 3.094 | ppl 8.54 | wps 17579.6 | ups 6.82 | wpb 2575.8 | bsz 18.1 | num_updates 957 | lr 4.79404e-05 | gnorm 0.667 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.6 | wall 163\n",
            "2024-12-11 21:30:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 065:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:54 | INFO | fairseq.trainer | begin training epoch 65\n",
            "2024-12-11 21:30:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 065:  87% 13/15 [00:01<00:00, 10.32it/s]2024-12-11 21:30:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 065 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:56 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 4.235 | nll_loss 3.095 | ppl 8.54 | wps 45658.7 | wpb 1564.7 | bsz 11.3 | num_updates 972 | best_loss 4.235\n",
            "2024-12-11 21:30:56 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
            "2024-12-11 21:30:56 | INFO | train | epoch 065 | loss 4.191 | nll_loss 3.089 | ppl 8.51 | wps 16607.5 | ups 6.45 | wpb 2575.8 | bsz 18.1 | num_updates 972 | lr 4.86903e-05 | gnorm 0.669 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 165\n",
            "2024-12-11 21:30:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 066:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:56 | INFO | fairseq.trainer | begin training epoch 66\n",
            "2024-12-11 21:30:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 066:  93% 14/15 [00:01<00:00, 10.48it/s]2024-12-11 21:30:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 066 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.30it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:30:58 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 4.239 | nll_loss 3.081 | ppl 8.46 | wps 46000.7 | wpb 1564.7 | bsz 11.3 | num_updates 987 | best_loss 4.239\n",
            "2024-12-11 21:30:58 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
            "2024-12-11 21:30:58 | INFO | train | epoch 066 | loss 4.184 | nll_loss 3.082 | ppl 8.47 | wps 16787.3 | ups 6.52 | wpb 2575.8 | bsz 18.1 | num_updates 987 | lr 4.94401e-05 | gnorm 0.677 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 167\n",
            "2024-12-11 21:30:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 067:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:30:58 | INFO | fairseq.trainer | begin training epoch 67\n",
            "2024-12-11 21:30:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 067:  87% 13/15 [00:01<00:00, 10.96it/s, loss=4.194, nll_loss=3.095, ppl=8.55, wps=17986.9, ups=6.95, wpb=2589, bsz=18.2, num_updates=1000, lr=5.009e-05, gnorm=0.664, clip=0, loss_scale=16, train_wall=9, gb_free=37.9, wall=169]2024-12-11 21:31:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 067 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.17it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:00 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 4.241 | nll_loss 3.084 | ppl 8.48 | wps 46249 | wpb 1564.7 | bsz 11.3 | num_updates 1002 | best_loss 4.241\n",
            "2024-12-11 21:31:00 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
            "2024-12-11 21:31:00 | INFO | train | epoch 067 | loss 4.179 | nll_loss 3.076 | ppl 8.43 | wps 18114.9 | ups 7.03 | wpb 2575.8 | bsz 18.1 | num_updates 1002 | lr 5.019e-05 | gnorm 0.67 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 169\n",
            "2024-12-11 21:31:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 068:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:01 | INFO | fairseq.trainer | begin training epoch 68\n",
            "2024-12-11 21:31:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 068:  93% 14/15 [00:01<00:00, 10.96it/s]2024-12-11 21:31:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 068 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:03 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 4.233 | nll_loss 3.08 | ppl 8.46 | wps 46837 | wpb 1564.7 | bsz 11.3 | num_updates 1017 | best_loss 4.233\n",
            "2024-12-11 21:31:03 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
            "2024-12-11 21:31:03 | INFO | train | epoch 068 | loss 4.173 | nll_loss 3.066 | ppl 8.37 | wps 17800.1 | ups 6.91 | wpb 2575.8 | bsz 18.1 | num_updates 1017 | lr 5.09398e-05 | gnorm 0.68 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 172\n",
            "2024-12-11 21:31:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 069:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:03 | INFO | fairseq.trainer | begin training epoch 69\n",
            "2024-12-11 21:31:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 069:  93% 14/15 [00:01<00:00, 10.73it/s]2024-12-11 21:31:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 069 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:05 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 4.242 | nll_loss 3.074 | ppl 8.42 | wps 46119.6 | wpb 1564.7 | bsz 11.3 | num_updates 1032 | best_loss 4.242\n",
            "2024-12-11 21:31:05 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
            "2024-12-11 21:31:05 | INFO | train | epoch 069 | loss 4.166 | nll_loss 3.062 | ppl 8.35 | wps 17294.8 | ups 6.71 | wpb 2575.8 | bsz 18.1 | num_updates 1032 | lr 5.16897e-05 | gnorm 0.689 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.6 | wall 174\n",
            "2024-12-11 21:31:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 070:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:05 | INFO | fairseq.trainer | begin training epoch 70\n",
            "2024-12-11 21:31:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 070:  93% 14/15 [00:01<00:00, 10.80it/s]2024-12-11 21:31:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 070 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:07 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 4.244 | nll_loss 3.077 | ppl 8.44 | wps 46484.8 | wpb 1564.7 | bsz 11.3 | num_updates 1047 | best_loss 4.244\n",
            "2024-12-11 21:31:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 1047 updates\n",
            "2024-12-11 21:31:07 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint70.pt\n",
            "2024-12-11 21:31:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint70.pt\n",
            "2024-12-11 21:31:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint70.pt (epoch 70 @ 1047 updates, score 4.244) (writing took 2.2314078669999162 seconds)\n",
            "2024-12-11 21:31:09 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
            "2024-12-11 21:31:09 | INFO | train | epoch 070 | loss 4.163 | nll_loss 3.051 | ppl 8.29 | wps 8663.7 | ups 3.36 | wpb 2575.8 | bsz 18.1 | num_updates 1047 | lr 5.24395e-05 | gnorm 0.707 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 178\n",
            "2024-12-11 21:31:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 071:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:09 | INFO | fairseq.trainer | begin training epoch 71\n",
            "2024-12-11 21:31:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 071:  93% 14/15 [00:01<00:00, 10.88it/s]2024-12-11 21:31:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 071 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.00it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:12 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 4.233 | nll_loss 3.077 | ppl 8.44 | wps 45535.2 | wpb 1564.7 | bsz 11.3 | num_updates 1062 | best_loss 4.233\n",
            "2024-12-11 21:31:12 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
            "2024-12-11 21:31:12 | INFO | train | epoch 071 | loss 4.158 | nll_loss 3.043 | ppl 8.24 | wps 16918.1 | ups 6.57 | wpb 2575.8 | bsz 18.1 | num_updates 1062 | lr 5.31894e-05 | gnorm 0.704 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 181\n",
            "2024-12-11 21:31:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 072:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:12 | INFO | fairseq.trainer | begin training epoch 72\n",
            "2024-12-11 21:31:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 072:  87% 13/15 [00:01<00:00, 10.94it/s]2024-12-11 21:31:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 072 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.00it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:14 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 4.23 | nll_loss 3.073 | ppl 8.42 | wps 45268.8 | wpb 1564.7 | bsz 11.3 | num_updates 1077 | best_loss 4.23\n",
            "2024-12-11 21:31:14 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n",
            "2024-12-11 21:31:14 | INFO | train | epoch 072 | loss 4.148 | nll_loss 3.035 | ppl 8.2 | wps 17072.4 | ups 6.63 | wpb 2575.8 | bsz 18.1 | num_updates 1077 | lr 5.39392e-05 | gnorm 0.681 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 183\n",
            "2024-12-11 21:31:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 073:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:14 | INFO | fairseq.trainer | begin training epoch 73\n",
            "2024-12-11 21:31:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 073:  87% 13/15 [00:01<00:00, 10.60it/s]2024-12-11 21:31:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 073 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:16 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 4.228 | nll_loss 3.07 | ppl 8.39 | wps 45038.5 | wpb 1564.7 | bsz 11.3 | num_updates 1092 | best_loss 4.228\n",
            "2024-12-11 21:31:16 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n",
            "2024-12-11 21:31:16 | INFO | train | epoch 073 | loss 4.146 | nll_loss 3.031 | ppl 8.17 | wps 16759.9 | ups 6.51 | wpb 2575.8 | bsz 18.1 | num_updates 1092 | lr 5.46891e-05 | gnorm 0.687 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.6 | wall 185\n",
            "2024-12-11 21:31:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 074:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:16 | INFO | fairseq.trainer | begin training epoch 74\n",
            "2024-12-11 21:31:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 074:  93% 14/15 [00:01<00:00, 10.87it/s, loss=4.16, nll_loss=3.049, ppl=8.28, wps=14954.7, ups=5.72, wpb=2614.4, bsz=18.3, num_updates=1100, lr=5.5089e-05, gnorm=0.689, clip=0, loss_scale=16, train_wall=9, gb_free=37.7, wall=186]2024-12-11 21:31:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 074 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.37it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:18 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 4.236 | nll_loss 3.06 | ppl 8.34 | wps 46405.4 | wpb 1564.7 | bsz 11.3 | num_updates 1107 | best_loss 4.236\n",
            "2024-12-11 21:31:18 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n",
            "2024-12-11 21:31:18 | INFO | train | epoch 074 | loss 4.136 | nll_loss 3.019 | ppl 8.1 | wps 17757.3 | ups 6.89 | wpb 2575.8 | bsz 18.1 | num_updates 1107 | lr 5.54389e-05 | gnorm 0.718 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 187\n",
            "2024-12-11 21:31:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 075:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:18 | INFO | fairseq.trainer | begin training epoch 75\n",
            "2024-12-11 21:31:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 075:  87% 13/15 [00:01<00:00, 10.82it/s]2024-12-11 21:31:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 075 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.44it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:21 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.234 | nll_loss 3.075 | ppl 8.43 | wps 45573 | wpb 1564.7 | bsz 11.3 | num_updates 1122 | best_loss 4.234\n",
            "2024-12-11 21:31:21 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n",
            "2024-12-11 21:31:21 | INFO | train | epoch 075 | loss 4.135 | nll_loss 3.014 | ppl 8.08 | wps 17163.9 | ups 6.66 | wpb 2575.8 | bsz 18.1 | num_updates 1122 | lr 5.61888e-05 | gnorm 0.705 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 190\n",
            "2024-12-11 21:31:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 076:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:21 | INFO | fairseq.trainer | begin training epoch 76\n",
            "2024-12-11 21:31:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 076:  93% 14/15 [00:01<00:00, 11.14it/s]2024-12-11 21:31:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 076 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.40it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:23 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 4.238 | nll_loss 3.076 | ppl 8.43 | wps 46222.5 | wpb 1564.7 | bsz 11.3 | num_updates 1137 | best_loss 4.238\n",
            "2024-12-11 21:31:23 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
            "2024-12-11 21:31:23 | INFO | train | epoch 076 | loss 4.128 | nll_loss 3.006 | ppl 8.03 | wps 18322.8 | ups 7.11 | wpb 2575.8 | bsz 18.1 | num_updates 1137 | lr 5.69386e-05 | gnorm 0.739 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 192\n",
            "2024-12-11 21:31:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 077:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:23 | INFO | fairseq.trainer | begin training epoch 77\n",
            "2024-12-11 21:31:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 077:  93% 14/15 [00:01<00:00, 10.82it/s]2024-12-11 21:31:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 077 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.38it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:25 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 4.232 | nll_loss 3.07 | ppl 8.4 | wps 45606.6 | wpb 1564.7 | bsz 11.3 | num_updates 1152 | best_loss 4.232\n",
            "2024-12-11 21:31:25 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
            "2024-12-11 21:31:25 | INFO | train | epoch 077 | loss 4.124 | nll_loss 2.999 | ppl 8 | wps 17382.7 | ups 6.75 | wpb 2575.8 | bsz 18.1 | num_updates 1152 | lr 5.76885e-05 | gnorm 0.743 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 194\n",
            "2024-12-11 21:31:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 078:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:25 | INFO | fairseq.trainer | begin training epoch 78\n",
            "2024-12-11 21:31:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 078:  93% 14/15 [00:01<00:00, 10.65it/s]2024-12-11 21:31:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 078 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.34it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:27 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 4.237 | nll_loss 3.071 | ppl 8.4 | wps 45527.3 | wpb 1564.7 | bsz 11.3 | num_updates 1167 | best_loss 4.237\n",
            "2024-12-11 21:31:27 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
            "2024-12-11 21:31:27 | INFO | train | epoch 078 | loss 4.123 | nll_loss 3 | ppl 8 | wps 18026 | ups 7 | wpb 2575.8 | bsz 18.1 | num_updates 1167 | lr 5.84383e-05 | gnorm 0.728 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 196\n",
            "2024-12-11 21:31:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 079:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:27 | INFO | fairseq.trainer | begin training epoch 79\n",
            "2024-12-11 21:31:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 079:  93% 14/15 [00:01<00:00, 10.68it/s]2024-12-11 21:31:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 079 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  4.18it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:29 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 4.233 | nll_loss 3.073 | ppl 8.42 | wps 45893.1 | wpb 1564.7 | bsz 11.3 | num_updates 1182 | best_loss 4.233\n",
            "2024-12-11 21:31:29 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
            "2024-12-11 21:31:29 | INFO | train | epoch 079 | loss 4.116 | nll_loss 2.987 | ppl 7.93 | wps 16875.6 | ups 6.55 | wpb 2575.8 | bsz 18.1 | num_updates 1182 | lr 5.91882e-05 | gnorm 0.749 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 198\n",
            "2024-12-11 21:31:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15\n",
            "epoch 080:   0% 0/15 [00:00<?, ?it/s]2024-12-11 21:31:29 | INFO | fairseq.trainer | begin training epoch 80\n",
            "2024-12-11 21:31:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 080:  87% 13/15 [00:01<00:00, 10.44it/s]2024-12-11 21:31:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 080 | valid on 'valid' subset:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  33% 1/3 [00:00<00:00,  3.80it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 21:31:32 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 4.234 | nll_loss 3.069 | ppl 8.39 | wps 41783.4 | wpb 1564.7 | bsz 11.3 | num_updates 1197 | best_loss 4.234\n",
            "2024-12-11 21:31:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 1197 updates\n",
            "2024-12-11 21:31:32 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint80.pt\n",
            "2024-12-11 21:31:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint80.pt\n",
            "2024-12-11 21:31:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint80.pt (epoch 80 @ 1197 updates, score 4.234) (writing took 2.278062627000054 seconds)\n",
            "2024-12-11 21:31:34 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
            "2024-12-11 21:31:34 | INFO | train | epoch 080 | loss 4.105 | nll_loss 2.975 | ppl 7.86 | wps 8467.4 | ups 3.29 | wpb 2575.8 | bsz 18.1 | num_updates 1197 | lr 5.9938e-05 | gnorm 0.707 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 203\n",
            "2024-12-11 21:31:34 | INFO | fairseq_cli.train | done training in 201.3 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate /content/data_root \\\n",
        "  --config-yaml config.yaml  \\\n",
        "  --task speech_to_speech --target-is-code --target-code-size 100 --vocoder code_hifigan \\\n",
        "  --path /content/models/checkpoint_best.pt  --gen-subset test \\\n",
        "  --max-tokens 50000 \\\n",
        "  --beam 10 --max-len-a 1 \\\n",
        "  --results-path /content/results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffAP0Dy8QRw7",
        "outputId": "d5e3ab79-8151-422c-c896-92684d16f3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 21:32:58.188870: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 21:32:58.206194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 21:32:58.227106: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 21:32:58.233438: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 21:32:58.248450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 21:32:59.301531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-11 21:33:01 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-12-11 21:33:03 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': '/content/results'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 50000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 50000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'nbest': 1, 'max_len_a': 1.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='speech_to_speech', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=50000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=50000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, path='/content/models/checkpoint_best.pt', post_process=None, quiet=False, model_overrides='{}', results_path='/content/results', beam=10, nbest=1, max_len_a=1.0, max_len_b=200, min_len=1, match_source_len=False, unnormalized=False, no_early_stop=False, no_beamable_mm=False, lenpen=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, constraints=None, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, print_alignment=None, print_step=False, lm_path=None, lm_weight=0.0, iter_decode_eos_penalty=0.0, iter_decode_max_iter=10, iter_decode_force_max_iter=False, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, retain_iter_history=False, retain_dropout=False, retain_dropout_modules=None, decoding_format=None, no_seed_provided=False, eos_token=None, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, arch='wav2vec2', data='/content/data_root', config_yaml='config.yaml', max_source_positions=6000, max_target_positions=1024, target_is_code=True, target_code_size=100, n_frames_per_step=1, multitask_config_yaml=None, eval_inference=False, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='code_hifigan', spec_bwd_max_iter=8, infer_target_lang='', force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, _name='speech_to_speech'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-12-11 21:33:03 | INFO | fairseq.tasks.speech_to_speech | dictionary size: 104\n",
            "2024-12-11 21:33:03 | INFO | fairseq_cli.generate | loading model(s) from /content/models/checkpoint_best.pt\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "2024-12-11 21:33:04 | INFO | fairseq.data.audio.speech_to_text_dataset | 'test' has 0.00% OOV\n",
            "2024-12-11 21:33:04 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToSpeechDataset(split=\"test\", n_samples=33, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 21:33:04 | INFO | fairseq.data.audio.speech_to_speech_dataset | SpeechToSpeechDataset(split=\"test\", n_samples=33, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 21:33:09 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-12-11 21:33:09 | INFO | fairseq_cli.generate | Translated 33 sentences (3,935 tokens) in 3.8s (8.68 sentences/s, 1035.48 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep \"^D\\-\" /content/results/generate-test.txt | \\\n",
        "  sed 's/^D-//ig' | sort -nk1 | cut -f3 \\\n",
        "  > /content/results/generate-test.unit\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/speech_to_speech/generate_waveform_from_code.py \\\n",
        "  --in-code-file /content/results/generate-test.unit \\\n",
        "  --vocoder /content/g_00500000 --vocoder-cfg /content/config.json  \\\n",
        "  --results-path /content/results --dur-prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etqPBnXrRkgr",
        "outputId": "0cd6e43b-f1f8-43f3-8750-260d1784603a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 21:33:18.007403: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 21:33:18.024273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 21:33:18.046002: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 21:33:18.052397: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 21:33:18.068719: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 21:33:19.169087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "INFO:__main__:Namespace(in_code_file='/content/results/generate-test.unit', vocoder='/content/g_00500000', vocoder_cfg='/content/config.json', results_path='/content/results', dur_prediction=True, speaker_id=-1, cpu=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/content/fairseq/fairseq/models/text_to_speech/vocoder.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(checkpoint_path)\n",
            "Removing weight norm...\n",
            "INFO:fairseq.models.text_to_speech.vocoder:loaded CodeHiFiGAN checkpoint from /content/g_00500000\n",
            "100% 33/33 [00:03<00:00, 10.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hifi gan fine tuning"
      ],
      "metadata": {
        "id": "NY9omTUARe5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/g_00500000\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/config.json\n",
        "!git clone https://github.com/facebookresearch/speech-resynthesis.git\n",
        "%cd speech-resynthesis\n",
        "!pip install torch==1.11.0 numpy librosa scipy tensorboard soundfile amfm_decompy matplotlib ffmpeg tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SRDxVPiRdoF",
        "outputId": "350099ab-57d5-4356-d948-67f3333afc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'speech-resynthesis'\n",
            "/content/speech-resynthesis\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: amfm_decompy in /usr/local/lib/python3.10/dist-packages (1.0.11)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (1.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.12.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4.3)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (24.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.43.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.8.30)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "def process_tsv(tsv_file, audio_dir, output_file, duration_file):\n",
        "  \"\"\"\n",
        "  Processes a TSV file and creates a new file with the desired format.\n",
        "\n",
        "  Args:\n",
        "    tsv_file: Path to the input TSV file.\n",
        "    audio_dir: Directory containing the audio files.\n",
        "    output_file: Path to the output file.\n",
        "    duration_file: Path to the file containing audio durations.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load durations from the duration file\n",
        "  durations = {}\n",
        "  with open(duration_file, 'r') as f:\n",
        "    for line in f:\n",
        "      audio_path, duration = line.strip().split('\\t')\n",
        "\n",
        "      durations[audio_path] = int(duration) / 16000.0  # Assuming 16kHz sample rate\n",
        "\n",
        "  with open(tsv_file, 'r') as f, open(output_file, 'w') as out_f:\n",
        "    for line in f:\n",
        "      filename, hubert_units = line.strip().split('|')\n",
        "      audio_path = os.path.join(audio_dir, f\"{filename}.wav\")\n",
        "      duration = durations.get(audio_path, 0)  # Get duration from the dictionary, default to 0 if not found\n",
        "      linebyline = f\"{'':}'audio': '{audio_path}', 'hubert': '{hubert_units}', 'duration': {duration}\"\n",
        "      out_f.write('{' + linebyline + '}\\n')\n",
        "\n",
        "# Example usage:\n",
        "input_tsv = \"/content/target_wav/train.txt\"\n",
        "audio_dir = \"/content/processed_data/train\"\n",
        "output_file = \"/content/target_wav/train/train.txt\"\n",
        "duration_file = \"/content/target_wav/train.tsv\"\n",
        "\n",
        "process_tsv(input_tsv, audio_dir, output_file, duration_file)\n",
        "\n",
        "input_tsv = \"/content/target_wav/valid.txt\"\n",
        "audio_dir = \"/content/processed_data/valid/\"\n",
        "output_file = \"/content/target_wav/valid/valid.txt\"\n",
        "duration_file = \"/content/target_wav/valid.tsv\"\n",
        "\n",
        "process_tsv(input_tsv, audio_dir, output_file, duration_file)"
      ],
      "metadata": {
        "id": "2y0hMpmrXDBR"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9yL35B71ctK",
        "outputId": "bc11df0d-a004-43ab-96e7-ebde9c95ed08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Collecting librosa\n",
            "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
            "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: librosa\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "Successfully installed librosa-0.10.2.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# an example of training with HuBERT units\n",
        "\n",
        "%cd /content/speech-resynthesis/\n",
        "\n",
        "!python -m torch.distributed.launch --nproc_per_node 1 \\\n",
        "    -m examples.speech_to_speech_translation.train \\\n",
        "    --checkpoint_path checkpoints/lj_hubert100_dur1.0 \\\n",
        "    --config examples/speech_to_speech_translation/configs/hubert100_dw1.0.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWkGTGIJ2JoR",
        "outputId": "641c4544-e812-49d9-bbee-7a47e7d3f387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/speech-resynthesis\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use_env is set by default in torchrun.\n",
            "If your script expects `--local_rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "Initializing Training Process..\n",
            "Batch size per GPU : 16\n",
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:145: UserWarning: \n",
            "NVIDIA A100-SXM4-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
            "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
            "If you want to use the NVIDIA A100-SXM4-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
            "\n",
            "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
            "DurationCodeGenerator(\n",
            "  (conv_pre): Conv1d(128, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "  (ups): ModuleList(\n",
            "    (0): ConvTranspose1d(512, 256, kernel_size=(11,), stride=(5,), padding=(3,))\n",
            "    (1): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(4,), padding=(2,))\n",
            "    (2): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,), padding=(2,))\n",
            "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
            "    (4): ConvTranspose1d(32, 16, kernel_size=(4,), stride=(2,), padding=(1,))\n",
            "  )\n",
            "  (resblocks): ModuleList(\n",
            "    (0): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      )\n",
            "    )\n",
            "    (1): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
            "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "      )\n",
            "    )\n",
            "    (2): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
            "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "      )\n",
            "    )\n",
            "    (3): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      )\n",
            "    )\n",
            "    (4): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
            "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "      )\n",
            "    )\n",
            "    (5): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
            "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "      )\n",
            "    )\n",
            "    (6): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      )\n",
            "    )\n",
            "    (7): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
            "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "      )\n",
            "    )\n",
            "    (8): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
            "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "      )\n",
            "    )\n",
            "    (9): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      )\n",
            "    )\n",
            "    (10): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
            "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "      )\n",
            "    )\n",
            "    (11): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
            "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "      )\n",
            "    )\n",
            "    (12): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "        (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      )\n",
            "    )\n",
            "    (13): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
            "        (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "      )\n",
            "    )\n",
            "    (14): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
            "        (2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_post): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "  (dict): Embedding(100, 128)\n",
            "  (dur_predictor): VariancePredictor(\n",
            "    (conv1): Sequential(\n",
            "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "    (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (conv2): Sequential(\n",
            "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "    (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (proj): Linear(in_features=128, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "checkpoints directory :  checkpoints/lj_hubert100_dur1.0\n",
            "2024-12-11 22:33:49.321484: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 22:33:49.338202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 22:33:49.357997: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 22:33:49.364340: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 22:33:49.379211: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 22:33:50.579451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Epoch: 1\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/speech-resynthesis/examples/speech_to_speech_translation/train.py\", line 309, in <module>\n",
            "    main()\n",
            "  File \"/content/speech-resynthesis/examples/speech_to_speech_translation/train.py\", line 305, in main\n",
            "    train(rank, local_rank, a, h)\n",
            "  File \"/content/speech-resynthesis/examples/speech_to_speech_translation/train.py\", line 131, in train\n",
            "    for i, batch in enumerate(train_loader):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 530, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 570, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/speech-resynthesis/dataset.py\", line 224, in __getitem__\n",
            "    audio, sampling_rate = load_audio(filename)\n",
            "  File \"/content/speech-resynthesis/dataset.py\", line 71, in load_audio\n",
            "    data, sampling_rate = sf.read(full_path, dtype='int16')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/soundfile.py\", line 285, in read\n",
            "    with SoundFile(file, 'r', samplerate, channels,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/soundfile.py\", line 658, in __init__\n",
            "    self._file = self._open(file, mode_int, closefd)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/soundfile.py\", line 1216, in _open\n",
            "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
            "soundfile.LibsndfileError: Error opening 'data/LJSpeech-1.1/wavs_16khz/LJ001-0006.wav': System error.\n",
            "\u001b[0mERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 23190) of binary: /usr/bin/python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check PyTorch documentation for available versions with CUDA 11.3 support\n",
        "# Replace 1.12.1 with a supported version if needed\n",
        "\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio --index-url https://download.pytorch.org/whl/cu113"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxNAvuX35MML",
        "outputId": "539c7697-1988-4418-b1d3-121968271f14"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (11.0.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2024.8.30)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu121\n",
            "    Uninstalling torchaudio-2.5.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.13.2 requires torch>=1.13.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft==0.13.1  # This might not be the latest version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbFF6KXd8dH1",
        "outputId": "3cd3398d-8735-44c6-d727-67f5c197b01a"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft==0.13.1\n",
            "  Downloading peft-0.13.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.13.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.13.1) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.13.1) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.13.1) (6.0.2)\n",
            "Collecting torch>=1.13.0 (from peft==0.13.1)\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.13.1) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.13.1) (4.66.6)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.13.1) (1.1.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.13.1) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.13.1) (0.26.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.1) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.1) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.1) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.13.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.13.1) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch>=1.13.0->peft==0.13.1)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.13.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.13.1) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.13.1) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.13.1) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.13.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.13.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.13.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.13.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.13.1) (2024.8.30)\n",
            "Downloading peft-0.13.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/27/94/3266821f65b92b3138631e9c8e7fe1fb513804ac934485a8d05776e1dd43/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, peft\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.13.2\n",
            "    Uninstalling peft-0.13.2:\n",
            "      Successfully uninstalled peft-0.13.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 peft-0.13.1 torch-2.5.1 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# an example of training with HuBERT units\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "!python -m torch.distributed.launch --nproc_per_node 1 \\\n",
        "    -m examples.speech_to_speech_translation.train \\\n",
        "    --checkpoint_path /content/checkpoints/ \\\n",
        "    --config /content/checkpoints/hubert100_dw1.json \\\n",
        "    --fine_tuning True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcoY7qfVRo5J",
        "outputId": "c337b828-37cc-4abd-e907-8833827417ea"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  main()\n",
            "2024-12-11 23:19:58.834348: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 23:19:58.852527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 23:19:58.874809: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 23:19:58.881356: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 23:19:58.897022: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 23:20:00.121541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Initializing Training Process..\n",
            "usage: train.py [-h] [--group_name GROUP_NAME] [--checkpoint_path CHECKPOINT_PATH]\n",
            "                [--config CONFIG] [--training_epochs TRAINING_EPOCHS]\n",
            "                [--training_steps TRAINING_STEPS] [--stdout_interval STDOUT_INTERVAL]\n",
            "                [--checkpoint_interval CHECKPOINT_INTERVAL] [--summary_interval SUMMARY_INTERVAL]\n",
            "                [--validation_interval VALIDATION_INTERVAL] [--fine_tuning FINE_TUNING]\n",
            "                [--local_rank LOCAL_RANK] [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n",
            "                [--distributed-port DISTRIBUTED_PORT]\n",
            "train.py: error: unrecognized arguments: --local-rank=0\n",
            "\u001b[0mE1211 23:20:04.641000 35755 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 2) local_rank: 0 (pid: 35777) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 208, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2853, in wrapper\n",
            "    return arg(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 204, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 189, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "examples.speech_to_speech_translation.train FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2024-12-11_23:20:04\n",
            "  host      : 9b0674b4b5df\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 2 (pid: 35777)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "xJvr8SCU1q3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8774d46-0e54-4a64-cd94-5346da1796ce"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==1.1.1\n",
            "aiohappyeyeballs==2.4.4\n",
            "aiohttp==3.11.9\n",
            "aiosignal==1.3.1\n",
            "alabaster==1.0.0\n",
            "albucore==0.0.19\n",
            "albumentations==1.4.20\n",
            "altair==4.2.2\n",
            "AMFM_decompy==1.0.11\n",
            "annotated-types==0.7.0\n",
            "antlr4-python3-runtime==4.8\n",
            "anyio==3.7.1\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.5.1\n",
            "arviz==0.20.0\n",
            "astropy==6.1.7\n",
            "astropy-iers-data==0.2024.12.2.0.35.34\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.3\n",
            "atpublic==4.1.0\n",
            "attrs==24.2.0\n",
            "audioread==3.0.1\n",
            "autograd==1.7.0\n",
            "babel==2.16.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.12.3\n",
            "bigframes==1.27.0\n",
            "bigquery-magics==0.4.0\n",
            "bitarray==3.0.0\n",
            "bleach==6.2.0\n",
            "blinker==1.9.0\n",
            "blis==0.7.11\n",
            "blosc2==2.7.1\n",
            "bokeh==3.6.2\n",
            "Bottleneck==1.4.2\n",
            "bqplot==0.12.43\n",
            "branca==0.8.0\n",
            "CacheControl==0.14.1\n",
            "cachetools==5.5.0\n",
            "catalogue==2.0.10\n",
            "certifi==2024.8.30\n",
            "cffi==1.17.1\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.4.0\n",
            "chex==0.1.87\n",
            "clarabel==0.9.0\n",
            "click==8.1.7\n",
            "cloudpathlib==0.20.0\n",
            "cloudpickle==3.1.0\n",
            "cmake==3.30.5\n",
            "cmdstanpy==1.2.4\n",
            "colorama==0.4.6\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.6\n",
            "contourpy==1.3.1\n",
            "cryptography==43.0.3\n",
            "cuda-python==12.2.1\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda12x==12.2.0\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.5.4\n",
            "cycler==0.12.1\n",
            "cymem==2.0.10\n",
            "Cython==3.0.11\n",
            "dask==2024.10.0\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.3.1\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.8.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "Deprecated==1.2.15\n",
            "diffusers==0.31.0\n",
            "distro==1.9.0\n",
            "dlib==19.24.2\n",
            "dm-tree==0.1.8\n",
            "docker-pycreds==0.4.0\n",
            "docstring_parser==0.16\n",
            "docutils==0.21.2\n",
            "dopamine_rl==4.0.9\n",
            "duckdb==1.1.3\n",
            "earthengine-api==1.2.0\n",
            "easydict==1.13\n",
            "ecos==2.0.14\n",
            "editdistance==0.8.1\n",
            "eerepr==0.0.4\n",
            "einops==0.8.0\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889\n",
            "entrypoints==0.4\n",
            "et_xmlfile==2.0.0\n",
            "etils==1.11.0\n",
            "etuples==0.3.9\n",
            "eval_type_backport==0.2.0\n",
            "exceptiongroup==1.2.2\n",
            "fairseq==0.12.2\n",
            "fastai==2.7.18\n",
            "fastcore==1.7.22\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.21.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.2\n",
            "ffmpeg==1.4\n",
            "filelock==3.16.1\n",
            "firebase-admin==6.5.0\n",
            "Flask==3.0.3\n",
            "flatbuffers==24.3.25\n",
            "flax==0.8.5\n",
            "folium==0.18.0\n",
            "fonttools==4.55.1\n",
            "frozendict==2.4.6\n",
            "frozenlist==1.5.0\n",
            "fsspec==2024.10.0\n",
            "future==1.0.0\n",
            "gast==0.6.0\n",
            "gcsfs==2024.10.0\n",
            "GDAL==3.6.4\n",
            "gdown==5.2.0\n",
            "geemap==0.35.1\n",
            "gensim==4.3.3\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==1.0.1\n",
            "geopy==2.4.1\n",
            "gin-config==0.5.0\n",
            "gitdb==4.0.11\n",
            "GitPython==3.1.43\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.10\n",
            "google-api-core==2.19.2\n",
            "google-api-python-client==2.151.0\n",
            "google-auth==2.27.0\n",
            "google-auth-httplib2==0.2.0\n",
            "google-auth-oauthlib==1.2.1\n",
            "google-cloud-aiplatform==1.73.0\n",
            "google-cloud-bigquery==3.25.0\n",
            "google-cloud-bigquery-connection==1.16.1\n",
            "google-cloud-bigquery-storage==2.27.0\n",
            "google-cloud-bigtable==2.27.0\n",
            "google-cloud-core==2.4.1\n",
            "google-cloud-datastore==2.20.1\n",
            "google-cloud-firestore==2.19.0\n",
            "google-cloud-functions==1.18.1\n",
            "google-cloud-iam==2.16.1\n",
            "google-cloud-language==2.15.1\n",
            "google-cloud-pubsub==2.27.1\n",
            "google-cloud-resource-manager==1.13.1\n",
            "google-cloud-storage==2.8.0\n",
            "google-cloud-translate==3.17.0\n",
            "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
            "google-crc32c==1.6.0\n",
            "google-generativeai==0.8.3\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.2\n",
            "googleapis-common-protos==1.66.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.3\n",
            "greenlet==3.1.1\n",
            "grpc-google-iam-v1==0.13.1\n",
            "grpcio==1.68.1\n",
            "grpcio-status==1.62.3\n",
            "gspread==6.0.2\n",
            "gspread-dataframe==3.3.1\n",
            "gTTS==2.5.4\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h11==0.14.0\n",
            "h5netcdf==1.4.1\n",
            "h5py==3.12.1\n",
            "holidays==0.62\n",
            "holoviews==1.20.0\n",
            "html5lib==1.1\n",
            "httpcore==1.0.7\n",
            "httpimport==1.4.0\n",
            "httplib2==0.22.0\n",
            "httpx==0.28.0\n",
            "huggingface-hub==0.26.3\n",
            "humanize==4.11.0\n",
            "hydra-core==1.0.7\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==9.2.0\n",
            "idna==3.10\n",
            "imageio==2.36.1\n",
            "imageio-ffmpeg==0.5.1\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.12.4\n",
            "imgaug==0.4.0\n",
            "immutabledict==4.2.1\n",
            "importlib_metadata==8.5.0\n",
            "importlib_resources==6.4.5\n",
            "imutils==0.5.4\n",
            "inflect==7.4.0\n",
            "iniconfig==2.0.0\n",
            "intel-cmplr-lib-ur==2025.0.3\n",
            "intel-openmp==2025.0.3\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==5.5.6\n",
            "ipyleaflet==0.19.2\n",
            "ipyparallel==8.8.0\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jax==0.4.33\n",
            "jax-cuda12-pjrt==0.4.33\n",
            "jax-cuda12-plugin==0.4.33\n",
            "jaxlib==0.4.33\n",
            "jeepney==0.7.1\n",
            "jellyfish==1.1.2\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.4\n",
            "jiter==0.8.0\n",
            "joblib==1.4.2\n",
            "jsonpatch==1.33\n",
            "jsonpickle==4.0.0\n",
            "jsonpointer==3.0.0\n",
            "jsonschema==4.23.0\n",
            "jsonschema-specifications==2024.10.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-leaflet==0.19.2\n",
            "jupyter-server==1.24.0\n",
            "jupyter_core==5.7.2\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.13\n",
            "kaggle==1.6.17\n",
            "kagglehub==0.3.4\n",
            "keras==3.5.0\n",
            "keyring==23.5.0\n",
            "kiwisolver==1.4.7\n",
            "langchain==0.3.9\n",
            "langchain-core==0.3.21\n",
            "langchain-text-splitters==0.3.2\n",
            "langcodes==3.5.0\n",
            "langsmith==0.1.147\n",
            "language_data==1.3.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-24.10.1-py3-none-manylinux_2_28_x86_64.whl\n",
            "librosa==0.10.2.post1\n",
            "lightgbm==4.5.0\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.43.0\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==5.3.0\n",
            "marisa-trie==1.2.1\n",
            "Markdown==3.7\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==3.0.2\n",
            "matplotlib==3.8.0\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==1.1.1\n",
            "mdit-py-plugins==0.4.2\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==3.0.2\n",
            "mizani==0.13.0\n",
            "mkl==2025.0.1\n",
            "ml-dtypes==0.4.1\n",
            "mlxtend==0.23.3\n",
            "more-itertools==10.5.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.1.0\n",
            "multidict==6.1.0\n",
            "multipledispatch==1.0.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.11\n",
            "music21==9.3.0\n",
            "namex==0.0.8\n",
            "natsort==8.4.0\n",
            "nbclassic==1.1.0\n",
            "nbclient==0.10.1\n",
            "nbconvert==7.16.4\n",
            "nbformat==5.10.4\n",
            "ndindex==1.9.2\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.4.2\n",
            "nibabel==5.3.2\n",
            "nltk==3.9.1\n",
            "notebook==6.5.5\n",
            "notebook_shim==0.2.4\n",
            "numba==0.60.0\n",
            "numexpr==2.10.2\n",
            "numpy==1.26.4\n",
            "nvidia-cublas-cu12==12.4.5.8\n",
            "nvidia-cuda-cupti-cu12==12.4.127\n",
            "nvidia-cuda-nvcc-cu12==12.6.85\n",
            "nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "nvidia-cuda-runtime-cu12==12.4.127\n",
            "nvidia-cudnn-cu12==9.1.0.70\n",
            "nvidia-cufft-cu12==11.2.1.3\n",
            "nvidia-curand-cu12==10.3.5.147\n",
            "nvidia-cusolver-cu12==11.6.1.9\n",
            "nvidia-cusparse-cu12==12.3.1.170\n",
            "nvidia-nccl-cu12==2.21.5\n",
            "nvidia-nvjitlink-cu12==12.4.127\n",
            "nvidia-nvtx-cu12==12.4.127\n",
            "nvtx==0.2.10\n",
            "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.10.0-py3-none-any.whl\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "omegaconf==2.0.6\n",
            "openai==1.54.5\n",
            "opencv-contrib-python==4.10.0.84\n",
            "opencv-python==4.10.0.84\n",
            "opencv-python-headless==4.10.0.84\n",
            "openpyxl==3.1.5\n",
            "opentelemetry-api==1.28.2\n",
            "opentelemetry-sdk==1.28.2\n",
            "opentelemetry-semantic-conventions==0.49b2\n",
            "opt_einsum==3.4.0\n",
            "optax==0.2.4\n",
            "optree==0.13.1\n",
            "orbax-checkpoint==0.6.4\n",
            "orjson==3.10.12\n",
            "osqp==0.6.7.post3\n",
            "packaging==24.2\n",
            "pandas==2.2.2\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.24.0\n",
            "pandas-stubs==2.2.2.240909\n",
            "pandocfilters==1.5.1\n",
            "panel==1.5.4\n",
            "param==2.1.1\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==1.0.1\n",
            "peewee==3.17.8\n",
            "peft==0.13.1\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "pillow==11.0.0\n",
            "platformdirs==4.3.6\n",
            "plotly==5.24.1\n",
            "plotnine==0.14.3\n",
            "pluggy==1.5.0\n",
            "ply==3.11\n",
            "polars==1.9.0\n",
            "pooch==1.8.2\n",
            "portalocker==3.0.0\n",
            "portpicker==1.5.2\n",
            "preshed==3.0.9\n",
            "prettytable==3.12.0\n",
            "proglog==0.1.10\n",
            "progressbar2==4.5.0\n",
            "prometheus_client==0.21.1\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.48\n",
            "propcache==0.2.1\n",
            "prophet==1.1.6\n",
            "proto-plus==1.25.0\n",
            "protobuf==4.25.5\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.10\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==17.0.0\n",
            "pyarrow-hotfix==0.6\n",
            "pyasn1==0.6.1\n",
            "pyasn1_modules==0.4.1\n",
            "pycocotools==2.0.8\n",
            "pycparser==2.22\n",
            "pydantic==2.10.3\n",
            "pydantic_core==2.27.1\n",
            "pydata-google-auth==1.9.0\n",
            "pydot==3.0.3\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.21.3\n",
            "pydub==0.25.1\n",
            "pyerfa==2.0.1.5\n",
            "pygame==2.6.1\n",
            "pygit2==1.16.0\n",
            "Pygments==2.18.0\n",
            "PyGObject==3.42.1\n",
            "PyJWT==2.10.1\n",
            "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "pylibcugraph-cu12==24.10.0\n",
            "pylibraft-cu12==24.10.0\n",
            "pymc==5.18.2\n",
            "pymystem3==0.2.0\n",
            "pynvjitlink-cu12==0.4.0\n",
            "pyogrio==0.10.0\n",
            "Pyomo==6.8.2\n",
            "PyOpenGL==3.1.7\n",
            "pyOpenSSL==24.2.1\n",
            "pyparsing==3.2.0\n",
            "pyperclip==1.9.0\n",
            "pyproj==3.7.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pyspark==3.5.3\n",
            "pytensor==2.26.4\n",
            "pytest==8.3.4\n",
            "python-apt==0.0.0\n",
            "python-box==7.2.0\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.4\n",
            "python-utils==3.9.1\n",
            "pytz==2024.2\n",
            "pyviz_comms==3.0.3\n",
            "PyYAML==6.0.2\n",
            "pyzmq==24.0.1\n",
            "qdldl==0.1.7.post4\n",
            "ratelim==0.1.6\n",
            "referencing==0.35.1\n",
            "regex==2024.9.11\n",
            "requests==2.32.3\n",
            "requests-oauthlib==1.3.1\n",
            "requests-toolbelt==1.0.0\n",
            "requirements-parser==0.9.0\n",
            "resampy==0.4.3\n",
            "rich==13.9.4\n",
            "rmm-cu12==24.10.0\n",
            "rpds-py==0.22.3\n",
            "rpy2==3.4.2\n",
            "rsa==4.9\n",
            "sacrebleu==2.4.3\n",
            "safetensors==0.4.5\n",
            "scikit-image==0.24.0\n",
            "scikit-learn==1.5.2\n",
            "scipy==1.13.1\n",
            "scooby==0.10.0\n",
            "scs==3.2.7\n",
            "seaborn==0.13.2\n",
            "SecretStorage==3.3.1\n",
            "Send2Trash==1.8.3\n",
            "sentence-transformers==3.2.1\n",
            "sentencepiece==0.2.0\n",
            "sentry-sdk==2.19.0\n",
            "setproctitle==1.3.4\n",
            "shap==0.46.0\n",
            "shapely==2.0.6\n",
            "shellingham==1.5.4\n",
            "simple-parsing==0.1.6\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "slicer==0.0.8\n",
            "smart-open==7.0.5\n",
            "smmap==5.0.1\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==2.2.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.6\n",
            "soxr==0.5.0.post1\n",
            "spacy==3.7.5\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "Sphinx==8.1.3\n",
            "sphinxcontrib-applehelp==2.0.0\n",
            "sphinxcontrib-devhelp==2.0.0\n",
            "sphinxcontrib-htmlhelp==2.1.0\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==2.0.0\n",
            "sphinxcontrib-serializinghtml==2.0.0\n",
            "SQLAlchemy==2.0.36\n",
            "sqlglot==25.1.0\n",
            "sqlparse==0.5.2\n",
            "srsly==2.4.8\n",
            "stanio==0.5.1\n",
            "statsmodels==0.14.4\n",
            "StrEnum==0.4.15\n",
            "stringzilla==3.11.0\n",
            "sympy==1.13.1\n",
            "tables==3.10.1\n",
            "tabulate==0.9.0\n",
            "tbb==2022.0.0\n",
            "tcmlib==1.2.0\n",
            "tenacity==9.0.0\n",
            "tensorboard==2.17.1\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow==2.17.1\n",
            "tensorflow-datasets==4.9.7\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.1\n",
            "tensorflow-metadata==1.13.1\n",
            "tensorflow-probability==0.24.0\n",
            "tensorstore==0.1.69\n",
            "termcolor==2.5.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.17.0\n",
            "thinc==8.2.5\n",
            "threadpoolctl==3.5.0\n",
            "tifffile==2024.9.20\n",
            "timm==1.0.12\n",
            "tinycss2==1.4.0\n",
            "tokenizers==0.20.3\n",
            "toml==0.10.2\n",
            "tomli==2.2.1\n",
            "toolz==0.12.1\n",
            "torch==2.5.1\n",
            "torchaudio==0.12.1+cu113\n",
            "torchsummary==1.5.1\n",
            "torchvision==0.13.1+cu113\n",
            "tornado==6.3.3\n",
            "tqdm==4.66.6\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.46.3\n",
            "triton==3.1.0\n",
            "tweepy==4.14.0\n",
            "typeguard==4.4.1\n",
            "typer==0.15.0\n",
            "types-pytz==2024.2.0.20241003\n",
            "types-setuptools==75.6.0.20241126\n",
            "typing_extensions==4.12.2\n",
            "tzdata==2024.2\n",
            "tzlocal==5.2\n",
            "uc-micro-py==1.0.3\n",
            "umf==0.9.1\n",
            "uritemplate==4.1.1\n",
            "urllib3==2.2.3\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wandb==0.18.7\n",
            "wasabi==1.1.3\n",
            "wcwidth==0.2.13\n",
            "weasel==0.4.1\n",
            "webcolors==24.11.1\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "Werkzeug==3.1.3\n",
            "widgetsnbextension==3.6.10\n",
            "wordcloud==1.9.4\n",
            "wrapt==1.17.0\n",
            "xarray==2024.10.0\n",
            "xarray-einstats==0.8.0\n",
            "xgboost==2.1.3\n",
            "xlrd==2.0.1\n",
            "xyzservices==2024.9.0\n",
            "yarl==1.18.3\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.50\n",
            "zipp==3.21.0\n"
          ]
        }
      ]
    }
  ]
}