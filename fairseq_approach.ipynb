{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM2mwxbOyw6V",
        "outputId": "070c3d6e-052f-4447-eae8-4823094ef697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/SLT final project/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New_zealand_data/Wave'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pip==23.3.1 #needs pip < 24.1 for whatever\n",
        "!pip install fairseq\n",
        "!git clone https://github.com/facebookresearch/fairseq.git"
      ],
      "metadata": {
        "id": "EhHp46HnznQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!apt-get install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MNHhBw2n0RxS",
        "outputId": "c12b9841-4aaa-4b77-d7b2-367f71f5eaee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#down load vocoder\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/g_00500000\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iWdwqxV-71pt",
        "outputId": "65885491-a52b-439c-e192-af7af9cd4efc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-11 04:19:26--  https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/g_00500000\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.244.202.103, 18.244.202.62, 18.244.202.25, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.244.202.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53590413 (51M) [binary/octet-stream]\n",
            "Saving to: ‘g_00500000.1’\n",
            "\n",
            "g_00500000.1        100%[===================>]  51.11M  34.6MB/s    in 1.5s    \n",
            "\n",
            "2024-12-11 04:19:28 (34.6 MB/s) - ‘g_00500000.1’ saved [53590413/53590413]\n",
            "\n",
            "--2024-12-11 04:19:28--  https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/hubert_base_100_lj/config.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.244.202.103, 18.244.202.62, 18.244.202.25, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.244.202.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1403 (1.4K) [application/json]\n",
            "Saving to: ‘config.json’\n",
            "\n",
            "config.json         100%[===================>]   1.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-11 04:19:28 (967 MB/s) - ‘config.json’ saved [1403/1403]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download hubert base + quantizer\n",
        "!wget https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\n",
        "!wget https://dl.fbaipublicfiles.com/textless_nlp/gslm/hubert/km100/km.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q-Qf5kX5DOEr",
        "outputId": "4849f20d-71a3-4e5d-fa96-ed861e942890"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-11 06:07:41--  https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.244.202.103, 18.244.202.73, 18.244.202.62, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.244.202.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1136468879 (1.1G) [application/zip]\n",
            "Saving to: ‘hubert_base_ls960.pt.2’\n",
            "\n",
            "hubert_base_ls960.p 100%[===================>]   1.06G   274MB/s    in 4.4s    \n",
            "\n",
            "2024-12-11 06:07:46 (248 MB/s) - ‘hubert_base_ls960.pt.2’ saved [1136468879/1136468879]\n",
            "\n",
            "--2024-12-11 06:07:46--  https://dl.fbaipublicfiles.com/textless_nlp/gslm/hubert/km100/km.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.244.202.103, 18.244.202.73, 18.244.202.62, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.244.202.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 308469 (301K) [application/octet-stream]\n",
            "Saving to: ‘km.bin’\n",
            "\n",
            "km.bin              100%[===================>] 301.24K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-12-11 06:07:46 (6.76 MB/s) - ‘km.bin’ saved [308469/308469]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify transcriptions.txt to be filtered\n",
        "import re\n",
        "\n",
        "def clean_text_file(input_file, output_file):\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            # Split the line into ID and text/transcription parts\n",
        "            parts = line.strip().split('\\t', maxsplit=1)\n",
        "            if len(parts) == 2:\n",
        "                identifier, text = parts\n",
        "                # Remove unwanted characters from the text\n",
        "                cleaned_text = re.sub(r'[\\-/%\\.]', '', text)  # Remove '-', '/', '%', '.'\n",
        "                cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Normalize spaces\n",
        "                # Append .wav to the identifier\n",
        "                identifier_wav = f\"{identifier}.wav\"\n",
        "                # Write to output file in the desired format\n",
        "                outfile.write(f\"{identifier_wav}\\t{cleaned_text}\\n\")\n",
        "\n",
        "# Input and output file names\n",
        "input_filename =  \"/content/drive/MyDrive/SLT final project/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New_zealand_data/ProsodyLabeling/text.txt\"\n",
        "output_filename = \"/content/drive/MyDrive/SLT final project/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New_zealand_data/ProsodyLabeling/filtered_transcriptions\"\n",
        "\n",
        "# Run the function\n",
        "clean_text_file(input_filename, output_filename)\n"
      ],
      "metadata": {
        "id": "-tJSsSIU0crA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Function to convert audio\n",
        "def convert_audio(input_file, output_file, target_sample_rate=16000):\n",
        "    # Load the .wav file\n",
        "    audio = AudioSegment.from_wav(input_file)\n",
        "\n",
        "    # Set the target sample rate and export\n",
        "    audio = audio.set_frame_rate(target_sample_rate)\n",
        "    audio.export(output_file, format=\"wav\")\n",
        "\n",
        "# Directory with .wav files\n",
        "input_dir = data_dir\n",
        "output_dir = '/content/drive/MyDrive/SLT final project/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New_zealand_data/16hz_wave'\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all files in the directory and convert\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.wav'):\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Convert to 16 kHz\n",
        "        convert_audio(input_path, output_path)\n",
        "        print(f\"Converted {filename} to 16 kHz\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8PBP-QzM0fzl",
        "outputId": "7053a44d-6b14-4b81-8b4e-3cd05495c526"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 000003.wav to 16 kHz\n",
            "Converted 000002.wav to 16 kHz\n",
            "Converted 000001.wav to 16 kHz\n",
            "Converted 000004.wav to 16 kHz\n",
            "Converted 000007.wav to 16 kHz\n",
            "Converted 000019.wav to 16 kHz\n",
            "Converted 000011.wav to 16 kHz\n",
            "Converted 000014.wav to 16 kHz\n",
            "Converted 000015.wav to 16 kHz\n",
            "Converted 000013.wav to 16 kHz\n",
            "Converted 000012.wav to 16 kHz\n",
            "Converted 000006.wav to 16 kHz\n",
            "Converted 000016.wav to 16 kHz\n",
            "Converted 000020.wav to 16 kHz\n",
            "Converted 000008.wav to 16 kHz\n",
            "Converted 000010.wav to 16 kHz\n",
            "Converted 000009.wav to 16 kHz\n",
            "Converted 000018.wav to 16 kHz\n",
            "Converted 000005.wav to 16 kHz\n",
            "Converted 000017.wav to 16 kHz\n",
            "Converted 000028.wav to 16 kHz\n",
            "Converted 000033.wav to 16 kHz\n",
            "Converted 000035.wav to 16 kHz\n",
            "Converted 000031.wav to 16 kHz\n",
            "Converted 000029.wav to 16 kHz\n",
            "Converted 000025.wav to 16 kHz\n",
            "Converted 000032.wav to 16 kHz\n",
            "Converted 000030.wav to 16 kHz\n",
            "Converted 000027.wav to 16 kHz\n",
            "Converted 000036.wav to 16 kHz\n",
            "Converted 000024.wav to 16 kHz\n",
            "Converted 000021.wav to 16 kHz\n",
            "Converted 000023.wav to 16 kHz\n",
            "Converted 000022.wav to 16 kHz\n",
            "Converted 000034.wav to 16 kHz\n",
            "Converted 000026.wav to 16 kHz\n",
            "Converted 000040.wav to 16 kHz\n",
            "Converted 000048.wav to 16 kHz\n",
            "Converted 000038.wav to 16 kHz\n",
            "Converted 000052.wav to 16 kHz\n",
            "Converted 000044.wav to 16 kHz\n",
            "Converted 000049.wav to 16 kHz\n",
            "Converted 000039.wav to 16 kHz\n",
            "Converted 000045.wav to 16 kHz\n",
            "Converted 000054.wav to 16 kHz\n",
            "Converted 000043.wav to 16 kHz\n",
            "Converted 000050.wav to 16 kHz\n",
            "Converted 000037.wav to 16 kHz\n",
            "Converted 000051.wav to 16 kHz\n",
            "Converted 000053.wav to 16 kHz\n",
            "Converted 000047.wav to 16 kHz\n",
            "Converted 000046.wav to 16 kHz\n",
            "Converted 000041.wav to 16 kHz\n",
            "Converted 000042.wav to 16 kHz\n",
            "Converted 000060.wav to 16 kHz\n",
            "Converted 000057.wav to 16 kHz\n",
            "Converted 000068.wav to 16 kHz\n",
            "Converted 000058.wav to 16 kHz\n",
            "Converted 000059.wav to 16 kHz\n",
            "Converted 000061.wav to 16 kHz\n",
            "Converted 000069.wav to 16 kHz\n",
            "Converted 000065.wav to 16 kHz\n",
            "Converted 000055.wav to 16 kHz\n",
            "Converted 000063.wav to 16 kHz\n",
            "Converted 000067.wav to 16 kHz\n",
            "Converted 000066.wav to 16 kHz\n",
            "Converted 000064.wav to 16 kHz\n",
            "Converted 000062.wav to 16 kHz\n",
            "Converted 000056.wav to 16 kHz\n",
            "Converted 000077.wav to 16 kHz\n",
            "Converted 000085.wav to 16 kHz\n",
            "Converted 000080.wav to 16 kHz\n",
            "Converted 000075.wav to 16 kHz\n",
            "Converted 000078.wav to 16 kHz\n",
            "Converted 000071.wav to 16 kHz\n",
            "Converted 000076.wav to 16 kHz\n",
            "Converted 000083.wav to 16 kHz\n",
            "Converted 000073.wav to 16 kHz\n",
            "Converted 000072.wav to 16 kHz\n",
            "Converted 000074.wav to 16 kHz\n",
            "Converted 000081.wav to 16 kHz\n",
            "Converted 000070.wav to 16 kHz\n",
            "Converted 000082.wav to 16 kHz\n",
            "Converted 000079.wav to 16 kHz\n",
            "Converted 000084.wav to 16 kHz\n",
            "Converted 000086.wav to 16 kHz\n",
            "Converted 000097.wav to 16 kHz\n",
            "Converted 000101.wav to 16 kHz\n",
            "Converted 000091.wav to 16 kHz\n",
            "Converted 000093.wav to 16 kHz\n",
            "Converted 000088.wav to 16 kHz\n",
            "Converted 000096.wav to 16 kHz\n",
            "Converted 000089.wav to 16 kHz\n",
            "Converted 000100.wav to 16 kHz\n",
            "Converted 000092.wav to 16 kHz\n",
            "Converted 000090.wav to 16 kHz\n",
            "Converted 000098.wav to 16 kHz\n",
            "Converted 000095.wav to 16 kHz\n",
            "Converted 000087.wav to 16 kHz\n",
            "Converted 000094.wav to 16 kHz\n",
            "Converted 000099.wav to 16 kHz\n",
            "Converted 000102.wav to 16 kHz\n",
            "Converted 000115.wav to 16 kHz\n",
            "Converted 000103.wav to 16 kHz\n",
            "Converted 000110.wav to 16 kHz\n",
            "Converted 000112.wav to 16 kHz\n",
            "Converted 000116.wav to 16 kHz\n",
            "Converted 000117.wav to 16 kHz\n",
            "Converted 000105.wav to 16 kHz\n",
            "Converted 000113.wav to 16 kHz\n",
            "Converted 000109.wav to 16 kHz\n",
            "Converted 000107.wav to 16 kHz\n",
            "Converted 000114.wav to 16 kHz\n",
            "Converted 000108.wav to 16 kHz\n",
            "Converted 000104.wav to 16 kHz\n",
            "Converted 000118.wav to 16 kHz\n",
            "Converted 000106.wav to 16 kHz\n",
            "Converted 000111.wav to 16 kHz\n",
            "Converted 000130.wav to 16 kHz\n",
            "Converted 000127.wav to 16 kHz\n",
            "Converted 000124.wav to 16 kHz\n",
            "Converted 000133.wav to 16 kHz\n",
            "Converted 000126.wav to 16 kHz\n",
            "Converted 000129.wav to 16 kHz\n",
            "Converted 000131.wav to 16 kHz\n",
            "Converted 000123.wav to 16 kHz\n",
            "Converted 000119.wav to 16 kHz\n",
            "Converted 000120.wav to 16 kHz\n",
            "Converted 000132.wav to 16 kHz\n",
            "Converted 000128.wav to 16 kHz\n",
            "Converted 000125.wav to 16 kHz\n",
            "Converted 000121.wav to 16 kHz\n",
            "Converted 000122.wav to 16 kHz\n",
            "Converted 000147.wav to 16 kHz\n",
            "Converted 000144.wav to 16 kHz\n",
            "Converted 000148.wav to 16 kHz\n",
            "Converted 000149.wav to 16 kHz\n",
            "Converted 000146.wav to 16 kHz\n",
            "Converted 000145.wav to 16 kHz\n",
            "Converted 000139.wav to 16 kHz\n",
            "Converted 000135.wav to 16 kHz\n",
            "Converted 000141.wav to 16 kHz\n",
            "Converted 000143.wav to 16 kHz\n",
            "Converted 000134.wav to 16 kHz\n",
            "Converted 000137.wav to 16 kHz\n",
            "Converted 000140.wav to 16 kHz\n",
            "Converted 000142.wav to 16 kHz\n",
            "Converted 000138.wav to 16 kHz\n",
            "Converted 000136.wav to 16 kHz\n",
            "Converted 000150.wav to 16 kHz\n",
            "Converted 000154.wav to 16 kHz\n",
            "Converted 000159.wav to 16 kHz\n",
            "Converted 000152.wav to 16 kHz\n",
            "Converted 000164.wav to 16 kHz\n",
            "Converted 000155.wav to 16 kHz\n",
            "Converted 000151.wav to 16 kHz\n",
            "Converted 000165.wav to 16 kHz\n",
            "Converted 000153.wav to 16 kHz\n",
            "Converted 000162.wav to 16 kHz\n",
            "Converted 000160.wav to 16 kHz\n",
            "Converted 000156.wav to 16 kHz\n",
            "Converted 000157.wav to 16 kHz\n",
            "Converted 000161.wav to 16 kHz\n",
            "Converted 000158.wav to 16 kHz\n",
            "Converted 000163.wav to 16 kHz\n",
            "Converted 000173.wav to 16 kHz\n",
            "Converted 000171.wav to 16 kHz\n",
            "Converted 000168.wav to 16 kHz\n",
            "Converted 000166.wav to 16 kHz\n",
            "Converted 000181.wav to 16 kHz\n",
            "Converted 000176.wav to 16 kHz\n",
            "Converted 000169.wav to 16 kHz\n",
            "Converted 000179.wav to 16 kHz\n",
            "Converted 000180.wav to 16 kHz\n",
            "Converted 000177.wav to 16 kHz\n",
            "Converted 000167.wav to 16 kHz\n",
            "Converted 000174.wav to 16 kHz\n",
            "Converted 000178.wav to 16 kHz\n",
            "Converted 000172.wav to 16 kHz\n",
            "Converted 000170.wav to 16 kHz\n",
            "Converted 000182.wav to 16 kHz\n",
            "Converted 000175.wav to 16 kHz\n",
            "Converted 000194.wav to 16 kHz\n",
            "Converted 000188.wav to 16 kHz\n",
            "Converted 000199.wav to 16 kHz\n",
            "Converted 000198.wav to 16 kHz\n",
            "Converted 000197.wav to 16 kHz\n",
            "Converted 000185.wav to 16 kHz\n",
            "Converted 000193.wav to 16 kHz\n",
            "Converted 000184.wav to 16 kHz\n",
            "Converted 000183.wav to 16 kHz\n",
            "Converted 000195.wav to 16 kHz\n",
            "Converted 000189.wav to 16 kHz\n",
            "Converted 000192.wav to 16 kHz\n",
            "Converted 000190.wav to 16 kHz\n",
            "Converted 000196.wav to 16 kHz\n",
            "Converted 000191.wav to 16 kHz\n",
            "Converted 000187.wav to 16 kHz\n",
            "Converted 000186.wav to 16 kHz\n",
            "Converted 000216.wav to 16 kHz\n",
            "Converted 000213.wav to 16 kHz\n",
            "Converted 000202.wav to 16 kHz\n",
            "Converted 000204.wav to 16 kHz\n",
            "Converted 000210.wav to 16 kHz\n",
            "Converted 000214.wav to 16 kHz\n",
            "Converted 000208.wav to 16 kHz\n",
            "Converted 000209.wav to 16 kHz\n",
            "Converted 000203.wav to 16 kHz\n",
            "Converted 000212.wav to 16 kHz\n",
            "Converted 000206.wav to 16 kHz\n",
            "Converted 000200.wav to 16 kHz\n",
            "Converted 000215.wav to 16 kHz\n",
            "Converted 000205.wav to 16 kHz\n",
            "Converted 000207.wav to 16 kHz\n",
            "Converted 000211.wav to 16 kHz\n",
            "Converted 000201.wav to 16 kHz\n",
            "Converted 000230.wav to 16 kHz\n",
            "Converted 000232.wav to 16 kHz\n",
            "Converted 000222.wav to 16 kHz\n",
            "Converted 000218.wav to 16 kHz\n",
            "Converted 000227.wav to 16 kHz\n",
            "Converted 000231.wav to 16 kHz\n",
            "Converted 000228.wav to 16 kHz\n",
            "Converted 000219.wav to 16 kHz\n",
            "Converted 000221.wav to 16 kHz\n",
            "Converted 000223.wav to 16 kHz\n",
            "Converted 000226.wav to 16 kHz\n",
            "Converted 000229.wav to 16 kHz\n",
            "Converted 000224.wav to 16 kHz\n",
            "Converted 000220.wav to 16 kHz\n",
            "Converted 000225.wav to 16 kHz\n",
            "Converted 000217.wav to 16 kHz\n",
            "Converted 000243.wav to 16 kHz\n",
            "Converted 000240.wav to 16 kHz\n",
            "Converted 000248.wav to 16 kHz\n",
            "Converted 000242.wav to 16 kHz\n",
            "Converted 000245.wav to 16 kHz\n",
            "Converted 000234.wav to 16 kHz\n",
            "Converted 000247.wav to 16 kHz\n",
            "Converted 000246.wav to 16 kHz\n",
            "Converted 000241.wav to 16 kHz\n",
            "Converted 000249.wav to 16 kHz\n",
            "Converted 000233.wav to 16 kHz\n",
            "Converted 000239.wav to 16 kHz\n",
            "Converted 000238.wav to 16 kHz\n",
            "Converted 000236.wav to 16 kHz\n",
            "Converted 000235.wav to 16 kHz\n",
            "Converted 000237.wav to 16 kHz\n",
            "Converted 000244.wav to 16 kHz\n",
            "Converted 000252.wav to 16 kHz\n",
            "Converted 000259.wav to 16 kHz\n",
            "Converted 000254.wav to 16 kHz\n",
            "Converted 000256.wav to 16 kHz\n",
            "Converted 000260.wav to 16 kHz\n",
            "Converted 000264.wav to 16 kHz\n",
            "Converted 000258.wav to 16 kHz\n",
            "Converted 000253.wav to 16 kHz\n",
            "Converted 000261.wav to 16 kHz\n",
            "Converted 000257.wav to 16 kHz\n",
            "Converted 000250.wav to 16 kHz\n",
            "Converted 000255.wav to 16 kHz\n",
            "Converted 000263.wav to 16 kHz\n",
            "Converted 000251.wav to 16 kHz\n",
            "Converted 000262.wav to 16 kHz\n",
            "Converted 000280.wav to 16 kHz\n",
            "Converted 000271.wav to 16 kHz\n",
            "Converted 000265.wav to 16 kHz\n",
            "Converted 000269.wav to 16 kHz\n",
            "Converted 000267.wav to 16 kHz\n",
            "Converted 000279.wav to 16 kHz\n",
            "Converted 000268.wav to 16 kHz\n",
            "Converted 000277.wav to 16 kHz\n",
            "Converted 000275.wav to 16 kHz\n",
            "Converted 000276.wav to 16 kHz\n",
            "Converted 000281.wav to 16 kHz\n",
            "Converted 000272.wav to 16 kHz\n",
            "Converted 000278.wav to 16 kHz\n",
            "Converted 000273.wav to 16 kHz\n",
            "Converted 000274.wav to 16 kHz\n",
            "Converted 000270.wav to 16 kHz\n",
            "Converted 000266.wav to 16 kHz\n",
            "Converted 000284.wav to 16 kHz\n",
            "Converted 000283.wav to 16 kHz\n",
            "Converted 000285.wav to 16 kHz\n",
            "Converted 000293.wav to 16 kHz\n",
            "Converted 000291.wav to 16 kHz\n",
            "Converted 000290.wav to 16 kHz\n",
            "Converted 000297.wav to 16 kHz\n",
            "Converted 000295.wav to 16 kHz\n",
            "Converted 000292.wav to 16 kHz\n",
            "Converted 000287.wav to 16 kHz\n",
            "Converted 000286.wav to 16 kHz\n",
            "Converted 000294.wav to 16 kHz\n",
            "Converted 000288.wav to 16 kHz\n",
            "Converted 000296.wav to 16 kHz\n",
            "Converted 000282.wav to 16 kHz\n",
            "Converted 000289.wav to 16 kHz\n",
            "Converted 000298.wav to 16 kHz\n",
            "Converted 000312.wav to 16 kHz\n",
            "Converted 000307.wav to 16 kHz\n",
            "Converted 000306.wav to 16 kHz\n",
            "Converted 000301.wav to 16 kHz\n",
            "Converted 000300.wav to 16 kHz\n",
            "Converted 000311.wav to 16 kHz\n",
            "Converted 000302.wav to 16 kHz\n",
            "Converted 000299.wav to 16 kHz\n",
            "Converted 000314.wav to 16 kHz\n",
            "Converted 000310.wav to 16 kHz\n",
            "Converted 000303.wav to 16 kHz\n",
            "Converted 000308.wav to 16 kHz\n",
            "Converted 000309.wav to 16 kHz\n",
            "Converted 000304.wav to 16 kHz\n",
            "Converted 000313.wav to 16 kHz\n",
            "Converted 000305.wav to 16 kHz\n",
            "Converted 000315.wav to 16 kHz\n",
            "Converted 000320.wav to 16 kHz\n",
            "Converted 000317.wav to 16 kHz\n",
            "Converted 000327.wav to 16 kHz\n",
            "Converted 000322.wav to 16 kHz\n",
            "Converted 000316.wav to 16 kHz\n",
            "Converted 000319.wav to 16 kHz\n",
            "Converted 000326.wav to 16 kHz\n",
            "Converted 000328.wav to 16 kHz\n",
            "Converted 000324.wav to 16 kHz\n",
            "Converted 000325.wav to 16 kHz\n",
            "Converted 000318.wav to 16 kHz\n",
            "Converted 000330.wav to 16 kHz\n",
            "Converted 000323.wav to 16 kHz\n",
            "Converted 000321.wav to 16 kHz\n",
            "Converted 000329.wav to 16 kHz\n",
            "Converted 000331.wav to 16 kHz\n",
            "Converted 000340.wav to 16 kHz\n",
            "Converted 000336.wav to 16 kHz\n",
            "Converted 000339.wav to 16 kHz\n",
            "Converted 000332.wav to 16 kHz\n",
            "Converted 000333.wav to 16 kHz\n",
            "Converted 000338.wav to 16 kHz\n",
            "Converted 000334.wav to 16 kHz\n",
            "Converted 000341.wav to 16 kHz\n",
            "Converted 000335.wav to 16 kHz\n",
            "Converted 000337.wav to 16 kHz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        "# Change to the working directory\n",
        "%cd /content\n",
        "\n",
        "def split_data(data_dir, output_dir, train_ratio=0.8, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Splits the data directory into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to the original data directory (contains .wav and transcriptions.txt).\n",
        "        output_dir (str): Path to the output directory.\n",
        "        train_ratio (float): Proportion of data to use for training (default 0.8).\n",
        "        test_ratio (float): Proportion of data to use for testing (default 0.1).\n",
        "    \"\"\"\n",
        "    # Ensure train_ratio + test_ratio <= 1\n",
        "    if train_ratio + test_ratio > 1:\n",
        "        raise ValueError(\"Train and test ratios must sum to at most 1.\")\n",
        "\n",
        "    # Path to the transcription file\n",
        "    transcription_file = os.path.join(\n",
        "        \"/content/drive/MyDrive/SLT final project/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New_zealand_data/ProsodyLabeling/filtered_transcriptions\"\n",
        "    )\n",
        "\n",
        "    # Read transcriptions\n",
        "    with open(transcription_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Shuffle the data\n",
        "    random.shuffle(lines)\n",
        "\n",
        "    # Calculate split indices\n",
        "    total_lines = len(lines)\n",
        "    train_idx = int(total_lines * train_ratio)\n",
        "    test_idx = int(total_lines * test_ratio) + train_idx\n",
        "\n",
        "    # Split the dataset\n",
        "    train_lines = lines[:train_idx]\n",
        "    test_lines = lines[train_idx:test_idx]\n",
        "    valid_lines = lines[test_idx:]\n",
        "\n",
        "    # Create output directories\n",
        "    train_dir = os.path.join(output_dir, \"train\")\n",
        "    valid_dir = os.path.join(output_dir, \"valid\")\n",
        "    test_dir = os.path.join(output_dir, \"test\")\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(valid_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Write split transcriptions\n",
        "    with open(os.path.join(train_dir, \"transcriptions.txt\"), \"w\") as f:\n",
        "        f.writelines(train_lines)\n",
        "\n",
        "    with open(os.path.join(valid_dir, \"transcriptions.txt\"), \"w\") as f:\n",
        "        f.writelines(valid_lines)\n",
        "\n",
        "    with open(os.path.join(test_dir, \"transcriptions.txt\"), \"w\") as f:\n",
        "        f.writelines(test_lines)\n",
        "\n",
        "    # Copy corresponding audio files\n",
        "    def copy_files(lines, destination_dir):\n",
        "        for line in lines:\n",
        "            audio_file = line.split(\"\\t\")[0].strip()\n",
        "            source_path = os.path.join(data_dir, audio_file)\n",
        "            dest_path = os.path.join(destination_dir, audio_file)\n",
        "            if os.path.exists(source_path):\n",
        "                copyfile(source_path, dest_path)\n",
        "\n",
        "    copy_files(train_lines, train_dir)\n",
        "    copy_files(valid_lines, valid_dir)\n",
        "    copy_files(test_lines, test_dir)\n",
        "\n",
        "    print(f\"Data successfully split into train ({train_dir}), valid ({valid_dir}), and test ({test_dir})\")\n",
        "\n",
        "# Path to the output directory\n",
        "output_dir = \"processed_data\"\n",
        "\n",
        "data_dir_16hz = '/content/drive/MyDrive/SLT final project/2 Hours- 4 Countries English Speech Synthesis Corpus/2 Hours- 4 Countries English Speech Synthesis Corpus/data/New_zealand_data/16hz_wave'\n",
        "\n",
        "# Split the data\n",
        "split_data(data_dir_16hz, output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTr6i1Ou1DCx",
        "outputId": "a85b5c10-c34a-4aad-a262-ccf618faaacc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Data successfully split into train (processed_data/train), valid (processed_data/valid), and test (processed_data/test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q6YC8k3U3-V7",
        "outputId": "8fb36042-323f-4e64-9c40-b1eb479dcc78"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.8.30)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: gtts\n",
            "Successfully installed gtts-2.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from gtts import gTTS\n",
        "\n",
        "# Ensure the folder exists\n",
        "output_folder = \"source_wav\"\n",
        "train_output_folder = \"source_wav/train\"\n",
        "test_output_folder = \"source_wav/test\"\n",
        "valid_output_folder = \"source_wav/valid\"\n",
        "os.makedirs(valid_output_folder, exist_ok=True)\n",
        "os.makedirs(test_output_folder, exist_ok=True)\n",
        "os.makedirs(train_output_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "source_transcription_path_train = '/content/processed_data/train/transcriptions.txt'\n",
        "source_transcription_path_test = '/content/processed_data/test/transcriptions.txt'\n",
        "source_transcription_path_valid = '/content/processed_data/valid/transcriptions.txt'\n",
        "# Open the transcription file\n",
        "with open(source_transcription_path_train, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Split the line into filename and text\n",
        "        parts = line.strip().split(\"\\t\")\n",
        "        if len(parts) != 2:\n",
        "            continue  # Skip invalid lines\n",
        "        filename, text = parts\n",
        "        # Generate TTS audio\n",
        "        tts = gTTS(text, lang=\"en\")\n",
        "        # Save the file\n",
        "        output_path = os.path.join(output_folder, 'train' ,filename)\n",
        "        tts.save(output_path)\n",
        "\n",
        "with open(source_transcription_path_test, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Split the line into filename and text\n",
        "        parts = line.strip().split(\"\\t\")\n",
        "        if len(parts) != 2:\n",
        "            continue  # Skip invalid lines\n",
        "        filename, text = parts\n",
        "        # Generate TTS audio\n",
        "        tts = gTTS(text, lang=\"en\")\n",
        "        # Save the file\n",
        "        output_path = os.path.join(output_folder, 'test' ,filename)\n",
        "        tts.save(output_path)\n",
        "\n",
        "with open(source_transcription_path_valid, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Split the line into filename and text\n",
        "        parts = line.strip().split(\"\\t\")\n",
        "        if len(parts) != 2:\n",
        "            continue  # Skip invalid lines\n",
        "        filename, text = parts\n",
        "        # Generate TTS audio\n",
        "        tts = gTTS(text, lang=\"en\")\n",
        "        # Save the file\n",
        "        output_path = os.path.join(output_folder, 'valid' ,filename)\n",
        "        tts.save(output_path)\n",
        "print(f\"Synthetic TTS .wav files saved in '{output_folder}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmFnfsNa3yS-",
        "outputId": "2786f7f9-793c-4b03-a4e9-f4fee4f61b3b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic TTS .wav files saved in 'source_wav'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create manifest files\n",
        "!pip install soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WJliNaagANZz",
        "outputId": "7416a7c4-93e4-4d1c-9ab8-b3039429c589"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/source_wav/test --dest /content/source_wav/test --ext wav --valid-percent 0\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/source_wav/train --dest /content/source_wav/train --ext wav --valid-percent 0\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/target_wav/test --dest /content/target_wav/test --ext wav --valid-percent 0\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/target_wav/train --dest /content/target_wav/train --ext wav --valid-percent 0\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/source_wav/valid --dest /content/source_wav/valid --ext wav --valid-percent 0\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/wav2vec/wav2vec_manifest.py /content/target_wav/valid --dest /content/target_wav/valid --ext wav --valid-percent 0"
      ],
      "metadata": {
        "id": "FgdxmI-ZAWMV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!MANIFEST=/content/target_wav/train/train.tsv\n",
        "!OUT_QUANTIZED_FILE=/content/target_wav/train.txt\n",
        "!KM_MODEL_PATH=/content/km.bin\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py \\\n",
        "    --feature_type hubert \\\n",
        "    --kmeans_model_path /content/km.bin \\\n",
        "    --acoustic_model_path /content/hubert_base_ls960.pt \\\n",
        "    --layer 6 \\\n",
        "    --manifest_path /content/target_wav/train/train.tsv \\\n",
        "    --out_quantized_file_path /content/target_wav/train.txt \\\n",
        "    --extension \".wav\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SFNrE2dgCpbe",
        "outputId": "386291ae-2910-4827-dd59-810dc1a173ca"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 06:08:00.588458: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 06:08:00.605936: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 06:08:00.626590: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 06:08:00.632869: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 06:08:00.647685: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 06:08:01.666779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-11 06:08:03 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-12-11 06:08:04 | INFO | __main__ | Namespace(feature_type='hubert', acoustic_model_path='/content/hubert_base_ls960.pt', layer=6, kmeans_model_path='/content/km.bin', features_path=None, manifest_path='/content/target_wav/train/train.tsv', out_quantized_file_path='/content/target_wav/train.txt', extension='.wav', channel_id=None, hide_fname=False)\n",
            "2024-12-11 06:08:04 | INFO | __main__ | Extracting hubert acoustic features...\n",
            "/content/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "2024-12-11 06:08:08 | INFO | fairseq.tasks.hubert_pretraining | current directory is /content\n",
            "2024-12-11 06:08:08 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
            "2024-12-11 06:08:08 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "100% 272/272 [00:04<00:00, 60.30it/s]\n",
            "2024-12-11 06:08:15 | INFO | __main__ | Features extracted for 272 utterances.\n",
            "\n",
            "2024-12-11 06:08:15 | INFO | __main__ | Dimensionality of representation = 768\n",
            "2024-12-11 06:08:15 | INFO | __main__ | Loading K-means model from /content/km.bin ...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Writing quantized predictions to /content/target_wav/train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!MANIFEST=/content/target_wav/test/test.tsv\n",
        "!OUT_QUANTIZED_FILE=/content/target_wav/test.txt\n",
        "!KM_MODEL_PATH=/content/km.bin\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py \\\n",
        "    --feature_type hubert \\\n",
        "    --kmeans_model_path /content/km.bin \\\n",
        "    --acoustic_model_path /content/hubert_base_ls960.pt \\\n",
        "    --layer 6 \\\n",
        "    --manifest_path /content/target_wav/test/train.tsv \\\n",
        "    --out_quantized_file_path /content/target_wav/test.txt \\\n",
        "    --extension \".wav\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA8rVaS-Gm4J",
        "outputId": "25a11d57-d74c-4129-fe43-50a3a96ac36f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 06:08:52.692127: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 06:08:52.710497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 06:08:52.733826: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 06:08:52.740354: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 06:08:52.756475: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 06:08:53.797154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-11 06:08:55 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-12-11 06:08:57 | INFO | __main__ | Namespace(feature_type='hubert', acoustic_model_path='/content/hubert_base_ls960.pt', layer=6, kmeans_model_path='/content/km.bin', features_path=None, manifest_path='/content/target_wav/test/train.tsv', out_quantized_file_path='/content/target_wav/test.txt', extension='.wav', channel_id=None, hide_fname=False)\n",
            "2024-12-11 06:08:57 | INFO | __main__ | Extracting hubert acoustic features...\n",
            "/content/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "2024-12-11 06:09:01 | INFO | fairseq.tasks.hubert_pretraining | current directory is /content\n",
            "2024-12-11 06:09:01 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
            "2024-12-11 06:09:01 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "100% 34/34 [00:01<00:00, 29.24it/s]\n",
            "2024-12-11 06:09:04 | INFO | __main__ | Features extracted for 34 utterances.\n",
            "\n",
            "2024-12-11 06:09:04 | INFO | __main__ | Dimensionality of representation = 768\n",
            "2024-12-11 06:09:04 | INFO | __main__ | Loading K-means model from /content/km.bin ...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Writing quantized predictions to /content/target_wav/test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!MANIFEST=/content/target_wav/valid/valid.tsv\n",
        "!OUT_QUANTIZED_FILE=/content/target_wav/valid.txt\n",
        "!KM_MODEL_PATH=/content/km.bin\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py \\\n",
        "    --feature_type hubert \\\n",
        "    --kmeans_model_path /content/km.bin \\\n",
        "    --acoustic_model_path /content/hubert_base_ls960.pt \\\n",
        "    --layer 6 \\\n",
        "    --manifest_path /content/target_wav/valid/train.tsv \\\n",
        "    --out_quantized_file_path /content/target_wav/valid.txt \\\n",
        "    --extension \".wav\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNh5HdlCOuaI",
        "outputId": "ce6781c4-0829-4e79-94ed-201c6aed1156"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 06:09:09.998549: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 06:09:10.015263: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 06:09:10.035951: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 06:09:10.042174: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 06:09:10.056891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 06:09:11.081401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-11 06:09:12 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-12-11 06:09:14 | INFO | __main__ | Namespace(feature_type='hubert', acoustic_model_path='/content/hubert_base_ls960.pt', layer=6, kmeans_model_path='/content/km.bin', features_path=None, manifest_path='/content/target_wav/valid/train.tsv', out_quantized_file_path='/content/target_wav/valid.txt', extension='.wav', channel_id=None, hide_fname=False)\n",
            "2024-12-11 06:09:14 | INFO | __main__ | Extracting hubert acoustic features...\n",
            "/content/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "2024-12-11 06:09:18 | INFO | fairseq.tasks.hubert_pretraining | current directory is /content\n",
            "2024-12-11 06:09:18 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
            "2024-12-11 06:09:18 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "100% 35/35 [00:01<00:00, 25.88it/s]\n",
            "2024-12-11 06:09:21 | INFO | __main__ | Features extracted for 35 utterances.\n",
            "\n",
            "2024-12-11 06:09:21 | INFO | __main__ | Dimensionality of representation = 768\n",
            "2024-12-11 06:09:21 | INFO | __main__ | Loading K-means model from /content/km.bin ...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Writing quantized predictions to /content/target_wav/valid.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# $SPLIT1, $SPLIT2, etc. are split names such as train, dev, test, etc.\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/speech_to_speech/preprocessing/prep_s2ut_data.py \\\n",
        "  --source-dir /content/source_wav --target-dir /content/target_wav/ --data-split train test valid \\\n",
        "  --output-root /content/data_root --reduce-unit \\\n",
        "  --vocoder-checkpoint /content/g_00500000 --vocoder-cfg /content/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNov96-C40Z5",
        "outputId": "7618fe2d-28be-4f61-c015-0481a6ff5976"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 06:09:27.351089: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 06:09:27.367611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 06:09:27.387911: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 06:09:27.394071: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 06:09:27.408422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 06:09:28.544915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-11 06:09:30 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "Generating manifest...\n",
            "Processing train\n",
            "100% 272/272 [00:00<00:00, 6863.58it/s]\n",
            "Processed 272 samples\n",
            "Writing manifest to /content/data_root/train.tsv...\n",
            "Processing test\n",
            "100% 34/34 [00:00<00:00, 6535.58it/s]\n",
            "Processed 34 samples\n",
            "Writing manifest to /content/data_root/test.tsv...\n",
            "Processing valid\n",
            "100% 35/35 [00:00<00:00, 6790.67it/s]\n",
            "Processed 35 samples\n",
            "Writing manifest to /content/data_root/valid.tsv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-train /content/data_root \\\n",
        "  --config-yaml /content/data_root/config.yaml  \\\n",
        "  --task speech_to_speech --target-is-code --target-code-size 100 --vocoder code_hifigan  \\\n",
        "  --criterion speech_to_unit --label-smoothing 0.2 \\\n",
        "  --arch s2ut_transformer_fisher --share-decoder-input-output-embed \\\n",
        "  --dropout 0.1 --attention-dropout 0.1 --relu-dropout 0.1 \\\n",
        "  --train-subset train  \\\n",
        "  --save-dir /content/models \\\n",
        "  --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-7 --warmup-updates 10000 \\\n",
        "  --optimizer adam --adam-betas \"(0.9,0.98)\" --clip-norm 10.0 \\\n",
        "  --max-update 400000 --max-tokens 20000 --max-target-positions 3000 --update-freq 1 \\\n",
        "  --seed 1 --fp16 --num-workers 8 --max-epoch 80 --save-interval 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u9aMbL54IYk1",
        "outputId": "dc7d3e3b-1f12-458b-b4fa-264af0b71c33"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 06:12:58.163412: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 06:12:58.180804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 06:12:58.201602: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 06:12:58.207917: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 06:12:58.223349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 06:12:59.262171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-11 06:13:00 | INFO | numexpr.utils | NumExpr defaulting to 12 threads.\n",
            "2024-12-11 06:13:01 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-12-11 06:13:03 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 20000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 20000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 80, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='speech_to_unit', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='speech_to_speech', num_workers=8, skip_invalid_size_inputs_valid_test=False, max_tokens=20000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=20000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2ut_transformer_fisher', max_epoch=80, max_update=400000, stop_time_hours=0, clip_norm=10.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/content/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=10, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, activation_fn='relu', data='/content/data_root', config_yaml='/content/data_root/config.yaml', max_source_positions=6000, max_target_positions=3000, target_is_code=True, target_code_size=100, n_frames_per_step=1, multitask_config_yaml=None, eval_inference=False, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='code_hifigan', spec_bwd_max_iter=8, infer_target_lang='', label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, no_seed_provided=False, encoder_embed_dim=256, encoder_attention_heads=4, encoder_freezing_updates=0, conv_kernel_sizes='5,5', conv_channels=1024, encoder_ffn_embed_dim=2048, encoder_layers=12, encoder_normalize_before=True, no_scale_embedding=False, speaker_embed_dim=256, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=True, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, quant_noise_pq=0, _name='s2ut_transformer_fisher'), 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='speech_to_unit', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='speech_to_speech', num_workers=8, skip_invalid_size_inputs_valid_test=False, max_tokens=20000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=20000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2ut_transformer_fisher', max_epoch=80, max_update=400000, stop_time_hours=0, clip_norm=10.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/content/models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=10, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, activation_fn='relu', data='/content/data_root', config_yaml='/content/data_root/config.yaml', max_source_positions=6000, max_target_positions=3000, target_is_code=True, target_code_size=100, n_frames_per_step=1, multitask_config_yaml=None, eval_inference=False, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='code_hifigan', spec_bwd_max_iter=8, infer_target_lang='', label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, no_seed_provided=False, encoder_embed_dim=256, encoder_attention_heads=4, encoder_freezing_updates=0, conv_kernel_sizes='5,5', conv_channels=1024, encoder_ffn_embed_dim=2048, encoder_layers=12, encoder_normalize_before=True, no_scale_embedding=False, speaker_embed_dim=256, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=True, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, quant_noise_pq=0, _name='speech_to_speech'), 'criterion': {'_name': 'speech_to_unit', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 10000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-12-11 06:13:03 | INFO | fairseq.tasks.speech_to_speech | dictionary size: 104\n",
            "2024-12-11 06:13:03 | INFO | fairseq_cli.train | S2UTTransformerModel(\n",
            "  (encoder): S2STransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (subsample): Conv1dSubsampler(\n",
            "      (conv_layers): ModuleList(\n",
            "        (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
            "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-11): 12 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerUnitDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): StackedEmbedding(104, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=104, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-12-11 06:13:03 | INFO | fairseq_cli.train | task: SpeechToSpeechTask\n",
            "2024-12-11 06:13:03 | INFO | fairseq_cli.train | model: S2UTTransformerModel\n",
            "2024-12-11 06:13:03 | INFO | fairseq_cli.train | criterion: SpeechToUnitMultitaskTaskCriterion\n",
            "2024-12-11 06:13:03 | INFO | fairseq_cli.train | num. shared model params: 42,771,456 (num. trained: 42,771,456)\n",
            "2024-12-11 06:13:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-12-11 06:13:03 | INFO | fairseq.data.audio.speech_to_text_dataset | 'valid' has 0.00% OOV\n",
            "2024-12-11 06:13:03 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToSpeechDataset(split=\"valid\", n_samples=35, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 06:13:03 | INFO | fairseq.data.audio.speech_to_speech_dataset | SpeechToSpeechDataset(split=\"valid\", n_samples=35, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 06:13:04 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-12-11 06:13:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-12-11 06:13:04 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.564 GB ; name = NVIDIA A100-SXM4-40GB                   \n",
            "2024-12-11 06:13:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-12-11 06:13:04 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-12-11 06:13:04 | INFO | fairseq_cli.train | max tokens per device = 20000 and max sentences per device = None\n",
            "2024-12-11 06:13:04 | INFO | fairseq.trainer | Preparing to load checkpoint /content/models/checkpoint_last.pt\n",
            "2024-12-11 06:13:04 | INFO | fairseq.trainer | No existing checkpoint found /content/models/checkpoint_last.pt\n",
            "2024-12-11 06:13:04 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-12-11 06:13:04 | INFO | fairseq.data.audio.speech_to_text_dataset | 'train' has 0.00% OOV\n",
            "2024-12-11 06:13:04 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToSpeechDataset(split=\"train\", n_samples=272, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "    SpecAugmentTransform(time_warp_w=0, freq_mask_n=1, freq_mask_f=27, time_mask_n=1, time_mask_t=100, time_mask_p=1.0)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 06:13:04 | INFO | fairseq.data.audio.speech_to_speech_dataset | SpeechToSpeechDataset(split=\"train\", n_samples=272, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "    SpecAugmentTransform(time_warp_w=0, freq_mask_n=1, freq_mask_f=27, time_mask_n=1, time_mask_t=100, time_mask_p=1.0)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 06:13:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 001:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:04 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-12-11 06:13:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "2024-12-11 06:13:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   7% 1/14 [00:02<00:26,  2.07s/it]2024-12-11 06:13:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:  50% 7/14 [00:02<00:01,  5.18it/s]2024-12-11 06:13:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:  93% 13/14 [00:03<00:00,  8.57it/s]2024-12-11 06:13:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 1/4 [00:00<00:01,  2.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 10.39it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:08 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.19 | nll_loss 10.186 | ppl 1164.88 | wps 32386.6 | wpb 1236.5 | bsz 8.8 | num_updates 11\n",
            "2024-12-11 06:13:08 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-12-11 06:13:08 | INFO | train | epoch 001 | loss 10.083 | nll_loss 10.078 | ppl 1080.83 | wps 16425.1 | ups 6.05 | wpb 2698.3 | bsz 18.9 | num_updates 11 | lr 6.4989e-07 | gnorm 19.261 | clip 100 | loss_scale 16 | train_wall 3 | gb_free 37.5 | wall 5\n",
            "2024-12-11 06:13:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 002:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:08 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-12-11 06:13:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  86% 12/14 [00:01<00:00, 10.49it/s]2024-12-11 06:13:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.42it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:10 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.246 | nll_loss 9.238 | ppl 603.86 | wps 28978.6 | wpb 1236.5 | bsz 8.8 | num_updates 25\n",
            "2024-12-11 06:13:10 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-12-11 06:13:10 | INFO | train | epoch 002 | loss 9.675 | nll_loss 9.668 | ppl 813.53 | wps 18435.3 | ups 6.67 | wpb 2762.6 | bsz 19.4 | num_updates 25 | lr 1.34975e-06 | gnorm 16.161 | clip 100 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 7\n",
            "2024-12-11 06:13:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 003:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:10 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-12-11 06:13:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  93% 13/14 [00:01<00:00, 10.77it/s]2024-12-11 06:13:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.39it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.06it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:12 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.447 | nll_loss 8.433 | ppl 345.52 | wps 28787.8 | wpb 1236.5 | bsz 8.8 | num_updates 39\n",
            "2024-12-11 06:13:12 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-12-11 06:13:12 | INFO | train | epoch 003 | loss 8.843 | nll_loss 8.833 | ppl 456.19 | wps 18988.6 | ups 6.87 | wpb 2762.6 | bsz 19.4 | num_updates 39 | lr 2.04961e-06 | gnorm 9.405 | clip 42.9 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 9\n",
            "2024-12-11 06:13:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 004:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:12 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-12-11 06:13:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  86% 12/14 [00:01<00:00, 10.63it/s]2024-12-11 06:13:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.95it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:14 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.97 | nll_loss 7.946 | ppl 246.67 | wps 31066.1 | wpb 1236.5 | bsz 8.8 | num_updates 53\n",
            "2024-12-11 06:13:14 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-12-11 06:13:14 | INFO | train | epoch 004 | loss 8.229 | nll_loss 8.211 | ppl 296.39 | wps 18842.4 | ups 6.82 | wpb 2762.6 | bsz 19.4 | num_updates 53 | lr 2.74947e-06 | gnorm 5.172 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 11\n",
            "2024-12-11 06:13:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 005:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:15 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-12-11 06:13:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  93% 13/14 [00:01<00:00, 10.55it/s]2024-12-11 06:13:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.06it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:17 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.598 | nll_loss 7.562 | ppl 188.92 | wps 34473.9 | wpb 1236.5 | bsz 8.8 | num_updates 67\n",
            "2024-12-11 06:13:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-12-11 06:13:17 | INFO | train | epoch 005 | loss 7.835 | nll_loss 7.809 | ppl 224.32 | wps 17974.9 | ups 6.51 | wpb 2762.6 | bsz 19.4 | num_updates 67 | lr 3.44933e-06 | gnorm 3.545 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 13\n",
            "2024-12-11 06:13:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 006:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:17 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-12-11 06:13:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  86% 12/14 [00:01<00:00, 10.02it/s]2024-12-11 06:13:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.52it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:19 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.337 | nll_loss 7.289 | ppl 156.38 | wps 31723.5 | wpb 1236.5 | bsz 8.8 | num_updates 81\n",
            "2024-12-11 06:13:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-12-11 06:13:19 | INFO | train | epoch 006 | loss 7.521 | nll_loss 7.484 | ppl 178.98 | wps 17672.1 | ups 6.4 | wpb 2762.6 | bsz 19.4 | num_updates 81 | lr 4.14919e-06 | gnorm 2.568 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 15\n",
            "2024-12-11 06:13:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 007:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:19 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2024-12-11 06:13:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  86% 12/14 [00:01<00:00, 10.32it/s]2024-12-11 06:13:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.77it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.13it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.14 | nll_loss 7.082 | ppl 135.48 | wps 30888.9 | wpb 1236.5 | bsz 8.8 | num_updates 95\n",
            "2024-12-11 06:13:21 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2024-12-11 06:13:21 | INFO | train | epoch 007 | loss 7.3 | nll_loss 7.251 | ppl 152.35 | wps 18245.9 | ups 6.6 | wpb 2762.6 | bsz 19.4 | num_updates 95 | lr 4.84905e-06 | gnorm 1.939 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 17\n",
            "2024-12-11 06:13:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 008:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:21 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2024-12-11 06:13:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  93% 13/14 [00:01<00:00, 10.59it/s, loss=8.382, nll_loss=8.358, ppl=328.14, wps=17934.2, ups=6.51, wpb=2750.6, bsz=19.4, num_updates=100, lr=5.099e-06, gnorm=7.631, clip=31, loss_scale=16, train_wall=11, gb_free=38.1, wall=18]2024-12-11 06:13:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.72it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:23 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.994 | nll_loss 6.928 | ppl 121.76 | wps 31235.2 | wpb 1236.5 | bsz 8.8 | num_updates 109\n",
            "2024-12-11 06:13:23 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2024-12-11 06:13:23 | INFO | train | epoch 008 | loss 7.123 | nll_loss 7.065 | ppl 133.85 | wps 19273.3 | ups 6.98 | wpb 2762.6 | bsz 19.4 | num_updates 109 | lr 5.54891e-06 | gnorm 1.493 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 19\n",
            "2024-12-11 06:13:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 009:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:23 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2024-12-11 06:13:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  86% 12/14 [00:01<00:00,  9.97it/s]2024-12-11 06:13:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.09it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:25 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.885 | nll_loss 6.812 | ppl 112.35 | wps 32512.4 | wpb 1236.5 | bsz 8.8 | num_updates 123\n",
            "2024-12-11 06:13:25 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2024-12-11 06:13:25 | INFO | train | epoch 009 | loss 6.992 | nll_loss 6.926 | ppl 121.62 | wps 18291 | ups 6.62 | wpb 2762.6 | bsz 19.4 | num_updates 123 | lr 6.24877e-06 | gnorm 1.176 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 21\n",
            "2024-12-11 06:13:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 010:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:25 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2024-12-11 06:13:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  93% 13/14 [00:01<00:00, 10.60it/s]2024-12-11 06:13:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.35it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.55it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:27 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.801 | nll_loss 6.722 | ppl 105.57 | wps 34532.6 | wpb 1236.5 | bsz 8.8 | num_updates 137\n",
            "2024-12-11 06:13:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 137 updates\n",
            "2024-12-11 06:13:27 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint10.pt\n",
            "2024-12-11 06:13:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint10.pt\n",
            "2024-12-11 06:13:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint10.pt (epoch 10 @ 137 updates, score 6.801) (writing took 1.8499128290004592 seconds)\n",
            "2024-12-11 06:13:29 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2024-12-11 06:13:29 | INFO | train | epoch 010 | loss 6.889 | nll_loss 6.816 | ppl 112.69 | wps 10065.2 | ups 3.64 | wpb 2762.6 | bsz 19.4 | num_updates 137 | lr 6.94863e-06 | gnorm 0.956 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 25\n",
            "2024-12-11 06:13:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 011:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:29 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2024-12-11 06:13:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  93% 13/14 [00:01<00:00, 10.48it/s]2024-12-11 06:13:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  75% 3/4 [00:00<00:00, 12.88it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:31 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.735 | nll_loss 6.65 | ppl 100.41 | wps 27480.8 | wpb 1236.5 | bsz 8.8 | num_updates 151 | best_loss 6.735\n",
            "2024-12-11 06:13:31 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2024-12-11 06:13:31 | INFO | train | epoch 011 | loss 6.811 | nll_loss 6.732 | ppl 106.28 | wps 17465.5 | ups 6.32 | wpb 2762.6 | bsz 19.4 | num_updates 151 | lr 7.64849e-06 | gnorm 0.799 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 28\n",
            "2024-12-11 06:13:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 012:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:31 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2024-12-11 06:13:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  86% 12/14 [00:01<00:00,  9.84it/s]2024-12-11 06:13:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.76it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:33 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.681 | nll_loss 6.59 | ppl 96.36 | wps 31543 | wpb 1236.5 | bsz 8.8 | num_updates 165 | best_loss 6.681\n",
            "2024-12-11 06:13:33 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2024-12-11 06:13:33 | INFO | train | epoch 012 | loss 6.746 | nll_loss 6.661 | ppl 101.21 | wps 17435.7 | ups 6.31 | wpb 2762.6 | bsz 19.4 | num_updates 165 | lr 8.34835e-06 | gnorm 0.678 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 30\n",
            "2024-12-11 06:13:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 013:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:33 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2024-12-11 06:13:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  86% 12/14 [00:01<00:00, 10.54it/s]2024-12-11 06:13:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:35 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.636 | nll_loss 6.54 | ppl 93.08 | wps 35706.1 | wpb 1236.5 | bsz 8.8 | num_updates 179 | best_loss 6.636\n",
            "2024-12-11 06:13:35 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2024-12-11 06:13:35 | INFO | train | epoch 013 | loss 6.693 | nll_loss 6.602 | ppl 97.16 | wps 18868.7 | ups 6.83 | wpb 2762.6 | bsz 19.4 | num_updates 179 | lr 9.04821e-06 | gnorm 0.595 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 32\n",
            "2024-12-11 06:13:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 014:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:35 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2024-12-11 06:13:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  93% 13/14 [00:01<00:00, 10.57it/s]2024-12-11 06:13:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.70it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.00it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:37 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.597 | nll_loss 6.496 | ppl 90.25 | wps 30712.2 | wpb 1236.5 | bsz 8.8 | num_updates 193 | best_loss 6.597\n",
            "2024-12-11 06:13:37 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2024-12-11 06:13:37 | INFO | train | epoch 014 | loss 6.648 | nll_loss 6.552 | ppl 93.84 | wps 18892.9 | ups 6.84 | wpb 2762.6 | bsz 19.4 | num_updates 193 | lr 9.74807e-06 | gnorm 0.528 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 34\n",
            "2024-12-11 06:13:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 015:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:38 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2024-12-11 06:13:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  93% 13/14 [00:01<00:00, 10.83it/s, loss=6.812, nll_loss=6.731, ppl=106.26, wps=16565, ups=5.99, wpb=2764.2, bsz=19.4, num_updates=200, lr=1.0098e-05, gnorm=0.824, clip=0, loss_scale=16, train_wall=9, gb_free=38.1, wall=35]2024-12-11 06:13:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.69it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:39 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.563 | nll_loss 6.456 | ppl 87.76 | wps 31198.3 | wpb 1236.5 | bsz 8.8 | num_updates 207 | best_loss 6.563\n",
            "2024-12-11 06:13:39 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2024-12-11 06:13:39 | INFO | train | epoch 015 | loss 6.612 | nll_loss 6.51 | ppl 91.14 | wps 19075.5 | ups 6.9 | wpb 2762.6 | bsz 19.4 | num_updates 207 | lr 1.04479e-05 | gnorm 0.481 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 36\n",
            "2024-12-11 06:13:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 016:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:40 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2024-12-11 06:13:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  86% 12/14 [00:01<00:00, 10.56it/s]2024-12-11 06:13:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.49it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:41 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.531 | nll_loss 6.417 | ppl 85.47 | wps 34847.6 | wpb 1236.5 | bsz 8.8 | num_updates 221 | best_loss 6.531\n",
            "2024-12-11 06:13:41 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2024-12-11 06:13:41 | INFO | train | epoch 016 | loss 6.578 | nll_loss 6.471 | ppl 88.68 | wps 18989.9 | ups 6.87 | wpb 2762.6 | bsz 19.4 | num_updates 221 | lr 1.11478e-05 | gnorm 0.445 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 38\n",
            "2024-12-11 06:13:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 017:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:42 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2024-12-11 06:13:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  86% 12/14 [00:01<00:00, 10.48it/s]2024-12-11 06:13:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.57it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.93it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:44 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.501 | nll_loss 6.38 | ppl 83.29 | wps 31076.3 | wpb 1236.5 | bsz 8.8 | num_updates 235 | best_loss 6.501\n",
            "2024-12-11 06:13:44 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2024-12-11 06:13:44 | INFO | train | epoch 017 | loss 6.545 | nll_loss 6.431 | ppl 86.28 | wps 18573.2 | ups 6.72 | wpb 2762.6 | bsz 19.4 | num_updates 235 | lr 1.18477e-05 | gnorm 0.419 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 40\n",
            "2024-12-11 06:13:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 018:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:44 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2024-12-11 06:13:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  93% 13/14 [00:01<00:00, 10.66it/s]2024-12-11 06:13:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.71it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.30it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:46 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.469 | nll_loss 6.34 | ppl 81.03 | wps 31745 | wpb 1236.5 | bsz 8.8 | num_updates 249 | best_loss 6.469\n",
            "2024-12-11 06:13:46 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2024-12-11 06:13:46 | INFO | train | epoch 018 | loss 6.514 | nll_loss 6.393 | ppl 84.03 | wps 18799.5 | ups 6.81 | wpb 2762.6 | bsz 19.4 | num_updates 249 | lr 1.25475e-05 | gnorm 0.404 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 42\n",
            "2024-12-11 06:13:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 019:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:46 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2024-12-11 06:13:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  86% 12/14 [00:01<00:00, 10.16it/s]2024-12-11 06:13:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  5.83it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 15.99it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:48 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.434 | nll_loss 6.295 | ppl 78.52 | wps 31187.1 | wpb 1236.5 | bsz 8.8 | num_updates 263 | best_loss 6.434\n",
            "2024-12-11 06:13:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2024-12-11 06:13:48 | INFO | train | epoch 019 | loss 6.482 | nll_loss 6.352 | ppl 81.67 | wps 17861.3 | ups 6.47 | wpb 2762.6 | bsz 19.4 | num_updates 263 | lr 1.32474e-05 | gnorm 0.399 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 44\n",
            "2024-12-11 06:13:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 020:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:48 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2024-12-11 06:13:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  86% 12/14 [00:01<00:00, 10.23it/s]2024-12-11 06:13:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.43it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  75% 3/4 [00:00<00:00, 12.74it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:50 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.389 | nll_loss 6.236 | ppl 75.37 | wps 25295.3 | wpb 1236.5 | bsz 8.8 | num_updates 277 | best_loss 6.389\n",
            "2024-12-11 06:13:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 277 updates\n",
            "2024-12-11 06:13:50 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint20.pt\n",
            "2024-12-11 06:13:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint20.pt\n",
            "2024-12-11 06:13:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint20.pt (epoch 20 @ 277 updates, score 6.389) (writing took 2.3092577709994657 seconds)\n",
            "2024-12-11 06:13:52 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2024-12-11 06:13:52 | INFO | train | epoch 020 | loss 6.445 | nll_loss 6.305 | ppl 79.06 | wps 8703 | ups 3.15 | wpb 2762.6 | bsz 19.4 | num_updates 277 | lr 1.39472e-05 | gnorm 0.398 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 49\n",
            "2024-12-11 06:13:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 021:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:52 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2024-12-11 06:13:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  86% 12/14 [00:01<00:00, 10.32it/s]2024-12-11 06:13:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.54it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:54 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.318 | nll_loss 6.14 | ppl 70.52 | wps 28547.4 | wpb 1236.5 | bsz 8.8 | num_updates 291 | best_loss 6.318\n",
            "2024-12-11 06:13:54 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2024-12-11 06:13:54 | INFO | train | epoch 021 | loss 6.396 | nll_loss 6.239 | ppl 75.52 | wps 18148.1 | ups 6.57 | wpb 2762.6 | bsz 19.4 | num_updates 291 | lr 1.46471e-05 | gnorm 0.415 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 51\n",
            "2024-12-11 06:13:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 022:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:54 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2024-12-11 06:13:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  93% 13/14 [00:01<00:00, 10.76it/s, loss=6.487, nll_loss=6.356, ppl=81.89, wps=16162.5, ups=5.86, wpb=2759.2, bsz=19.4, num_updates=300, lr=1.5097e-05, gnorm=0.422, clip=0, loss_scale=16, train_wall=9, gb_free=37.7, wall=52]2024-12-11 06:13:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.82it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:56 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.203 | nll_loss 5.974 | ppl 62.87 | wps 31198.5 | wpb 1236.5 | bsz 8.8 | num_updates 305 | best_loss 6.203\n",
            "2024-12-11 06:13:56 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2024-12-11 06:13:56 | INFO | train | epoch 022 | loss 6.318 | nll_loss 6.129 | ppl 69.98 | wps 19543.2 | ups 7.07 | wpb 2762.6 | bsz 19.4 | num_updates 305 | lr 1.5347e-05 | gnorm 0.46 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 53\n",
            "2024-12-11 06:13:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 023:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:56 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2024-12-11 06:13:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  86% 12/14 [00:01<00:00, 10.33it/s]2024-12-11 06:13:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.71it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.29it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:13:58 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.042 | nll_loss 5.745 | ppl 53.64 | wps 28357.4 | wpb 1236.5 | bsz 8.8 | num_updates 319 | best_loss 6.042\n",
            "2024-12-11 06:13:58 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2024-12-11 06:13:58 | INFO | train | epoch 023 | loss 6.206 | nll_loss 5.969 | ppl 62.65 | wps 18311.3 | ups 6.63 | wpb 2762.6 | bsz 19.4 | num_updates 319 | lr 1.60468e-05 | gnorm 0.502 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 55\n",
            "2024-12-11 06:13:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 024:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:13:59 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2024-12-11 06:13:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  86% 12/14 [00:01<00:00, 10.50it/s]2024-12-11 06:14:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.01it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.55it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:00 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.839 | nll_loss 5.462 | ppl 44.07 | wps 32459.2 | wpb 1236.5 | bsz 8.8 | num_updates 333 | best_loss 5.839\n",
            "2024-12-11 06:14:00 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2024-12-11 06:14:00 | INFO | train | epoch 024 | loss 6.04 | nll_loss 5.737 | ppl 53.32 | wps 19256.6 | ups 6.97 | wpb 2762.6 | bsz 19.4 | num_updates 333 | lr 1.67467e-05 | gnorm 0.573 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 57\n",
            "2024-12-11 06:14:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 025:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:01 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2024-12-11 06:14:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  93% 13/14 [00:01<00:00,  9.90it/s]2024-12-11 06:14:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  5.45it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:03 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.6 | nll_loss 5.121 | ppl 34.81 | wps 37012 | wpb 1236.5 | bsz 8.8 | num_updates 347 | best_loss 5.6\n",
            "2024-12-11 06:14:03 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2024-12-11 06:14:03 | INFO | train | epoch 025 | loss 5.85 | nll_loss 5.472 | ppl 44.38 | wps 17811.4 | ups 6.45 | wpb 2762.6 | bsz 19.4 | num_updates 347 | lr 1.74465e-05 | gnorm 0.616 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 59\n",
            "2024-12-11 06:14:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 026:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:03 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2024-12-11 06:14:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  86% 12/14 [00:01<00:00,  9.74it/s]2024-12-11 06:14:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.57it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.38it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:05 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.342 | nll_loss 4.751 | ppl 26.92 | wps 29141.4 | wpb 1236.5 | bsz 8.8 | num_updates 361 | best_loss 5.342\n",
            "2024-12-11 06:14:05 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2024-12-11 06:14:05 | INFO | train | epoch 026 | loss 5.604 | nll_loss 5.123 | ppl 34.85 | wps 18008.5 | ups 6.52 | wpb 2762.6 | bsz 19.4 | num_updates 361 | lr 1.81464e-05 | gnorm 0.683 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 61\n",
            "2024-12-11 06:14:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 027:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:05 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2024-12-11 06:14:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  86% 12/14 [00:01<00:00, 10.52it/s]2024-12-11 06:14:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  5.98it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:07 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.113 | nll_loss 4.401 | ppl 21.13 | wps 39695 | wpb 1236.5 | bsz 8.8 | num_updates 375 | best_loss 5.113\n",
            "2024-12-11 06:14:07 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2024-12-11 06:14:07 | INFO | train | epoch 027 | loss 5.359 | nll_loss 4.773 | ppl 27.34 | wps 18895 | ups 6.84 | wpb 2762.6 | bsz 19.4 | num_updates 375 | lr 1.88463e-05 | gnorm 0.69 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 63\n",
            "2024-12-11 06:14:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 028:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:07 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2024-12-11 06:14:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  93% 13/14 [00:01<00:00, 10.64it/s]2024-12-11 06:14:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.43it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.48it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:09 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.936 | nll_loss 4.108 | ppl 17.25 | wps 33816.8 | wpb 1236.5 | bsz 8.8 | num_updates 389 | best_loss 4.936\n",
            "2024-12-11 06:14:09 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2024-12-11 06:14:09 | INFO | train | epoch 028 | loss 5.138 | nll_loss 4.453 | ppl 21.91 | wps 18786 | ups 6.8 | wpb 2762.6 | bsz 19.4 | num_updates 389 | lr 1.95461e-05 | gnorm 0.709 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 65\n",
            "2024-12-11 06:14:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 029:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:09 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2024-12-11 06:14:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  86% 12/14 [00:01<00:00, 10.36it/s, loss=5.65, nll_loss=5.182, ppl=36.3, wps=18795, ups=6.77, wpb=2775.4, bsz=19.5, num_updates=400, lr=2.0096e-05, gnorm=0.631, clip=0, loss_scale=16, train_wall=9, gb_free=37.5, wall=67]2024-12-11 06:14:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.35it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.02it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:11 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.802 | nll_loss 3.921 | ppl 15.15 | wps 32455 | wpb 1236.5 | bsz 8.8 | num_updates 403 | best_loss 4.802\n",
            "2024-12-11 06:14:11 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2024-12-11 06:14:11 | INFO | train | epoch 029 | loss 4.959 | nll_loss 4.187 | ppl 18.22 | wps 18800.4 | ups 6.81 | wpb 2762.6 | bsz 19.4 | num_updates 403 | lr 2.0246e-05 | gnorm 0.719 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 67\n",
            "2024-12-11 06:14:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 030:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:11 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2024-12-11 06:14:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  86% 12/14 [00:01<00:00, 10.39it/s]2024-12-11 06:14:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.73it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.95it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:13 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.715 | nll_loss 3.773 | ppl 13.67 | wps 30408.5 | wpb 1236.5 | bsz 8.8 | num_updates 417 | best_loss 4.715\n",
            "2024-12-11 06:14:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 417 updates\n",
            "2024-12-11 06:14:13 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint30.pt\n",
            "2024-12-11 06:14:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint30.pt\n",
            "2024-12-11 06:14:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint30.pt (epoch 30 @ 417 updates, score 4.715) (writing took 2.3267685850005364 seconds)\n",
            "2024-12-11 06:14:15 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2024-12-11 06:14:15 | INFO | train | epoch 030 | loss 4.826 | nll_loss 3.992 | ppl 15.91 | wps 8724.1 | ups 3.16 | wpb 2762.6 | bsz 19.4 | num_updates 417 | lr 2.09458e-05 | gnorm 0.675 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 72\n",
            "2024-12-11 06:14:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 031:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:15 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2024-12-11 06:14:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  93% 13/14 [00:01<00:00, 10.87it/s]2024-12-11 06:14:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.14it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:17 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.647 | nll_loss 3.675 | ppl 12.77 | wps 35896.4 | wpb 1236.5 | bsz 8.8 | num_updates 431 | best_loss 4.647\n",
            "2024-12-11 06:14:17 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2024-12-11 06:14:17 | INFO | train | epoch 031 | loss 4.735 | nll_loss 3.862 | ppl 14.54 | wps 19470.4 | ups 7.05 | wpb 2762.6 | bsz 19.4 | num_updates 431 | lr 2.16457e-05 | gnorm 0.673 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 74\n",
            "2024-12-11 06:14:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 032:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:17 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2024-12-11 06:14:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  86% 12/14 [00:01<00:00, 10.42it/s]2024-12-11 06:14:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.45it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.45it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:19 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.591 | nll_loss 3.601 | ppl 12.14 | wps 33574.7 | wpb 1236.5 | bsz 8.8 | num_updates 445 | best_loss 4.591\n",
            "2024-12-11 06:14:19 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2024-12-11 06:14:19 | INFO | train | epoch 032 | loss 4.659 | nll_loss 3.745 | ppl 13.4 | wps 19007.4 | ups 6.88 | wpb 2762.6 | bsz 19.4 | num_updates 445 | lr 2.23456e-05 | gnorm 0.679 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 76\n",
            "2024-12-11 06:14:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 033:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:19 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2024-12-11 06:14:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  86% 12/14 [00:01<00:00, 10.27it/s]2024-12-11 06:14:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:21 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.562 | nll_loss 3.536 | ppl 11.6 | wps 35090.7 | wpb 1236.5 | bsz 8.8 | num_updates 459 | best_loss 4.562\n",
            "2024-12-11 06:14:21 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2024-12-11 06:14:21 | INFO | train | epoch 033 | loss 4.604 | nll_loss 3.67 | ppl 12.73 | wps 18623.8 | ups 6.74 | wpb 2762.6 | bsz 19.4 | num_updates 459 | lr 2.30454e-05 | gnorm 0.667 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 78\n",
            "2024-12-11 06:14:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 034:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:22 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2024-12-11 06:14:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  86% 12/14 [00:01<00:00, 10.14it/s]2024-12-11 06:14:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.26it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.47it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:24 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.531 | nll_loss 3.482 | ppl 11.18 | wps 30802 | wpb 1236.5 | bsz 8.8 | num_updates 473 | best_loss 4.531\n",
            "2024-12-11 06:14:24 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2024-12-11 06:14:24 | INFO | train | epoch 034 | loss 4.56 | nll_loss 3.607 | ppl 12.18 | wps 18877.2 | ups 6.83 | wpb 2762.6 | bsz 19.4 | num_updates 473 | lr 2.37453e-05 | gnorm 0.638 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 80\n",
            "2024-12-11 06:14:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 035:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:24 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-12-11 06:14:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  86% 12/14 [00:01<00:00, 10.13it/s]2024-12-11 06:14:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.63it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.55it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:26 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.499 | nll_loss 3.454 | ppl 10.96 | wps 33033.3 | wpb 1236.5 | bsz 8.8 | num_updates 487 | best_loss 4.499\n",
            "2024-12-11 06:14:26 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-12-11 06:14:26 | INFO | train | epoch 035 | loss 4.522 | nll_loss 3.551 | ppl 11.72 | wps 18243.9 | ups 6.6 | wpb 2762.6 | bsz 19.4 | num_updates 487 | lr 2.44451e-05 | gnorm 0.646 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 82\n",
            "2024-12-11 06:14:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 036:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:26 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2024-12-11 06:14:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  93% 13/14 [00:01<00:00, 10.41it/s, loss=4.636, nll_loss=3.717, ppl=13.15, wps=16205.6, ups=5.88, wpb=2753.9, bsz=19.4, num_updates=500, lr=2.5095e-05, gnorm=0.66, clip=0, loss_scale=16, train_wall=9, gb_free=37.8, wall=84]2024-12-11 06:14:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  5.97it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:28 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.475 | nll_loss 3.413 | ppl 10.65 | wps 35923.3 | wpb 1236.5 | bsz 8.8 | num_updates 501 | best_loss 4.475\n",
            "2024-12-11 06:14:28 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2024-12-11 06:14:28 | INFO | train | epoch 036 | loss 4.49 | nll_loss 3.507 | ppl 11.37 | wps 18293.2 | ups 6.62 | wpb 2762.6 | bsz 19.4 | num_updates 501 | lr 2.5145e-05 | gnorm 0.629 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 84\n",
            "2024-12-11 06:14:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 037:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:28 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2024-12-11 06:14:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  86% 12/14 [00:01<00:00, 10.37it/s]2024-12-11 06:14:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.54it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.60it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:30 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.451 | nll_loss 3.391 | ppl 10.49 | wps 33715.4 | wpb 1236.5 | bsz 8.8 | num_updates 515 | best_loss 4.451\n",
            "2024-12-11 06:14:30 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2024-12-11 06:14:30 | INFO | train | epoch 037 | loss 4.466 | nll_loss 3.472 | ppl 11.09 | wps 19063.9 | ups 6.9 | wpb 2762.6 | bsz 19.4 | num_updates 515 | lr 2.58449e-05 | gnorm 0.646 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 86\n",
            "2024-12-11 06:14:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 038:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:30 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2024-12-11 06:14:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  86% 12/14 [00:01<00:00, 10.54it/s]2024-12-11 06:14:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.25it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:32 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.442 | nll_loss 3.356 | ppl 10.24 | wps 35237.8 | wpb 1236.5 | bsz 8.8 | num_updates 529 | best_loss 4.442\n",
            "2024-12-11 06:14:32 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2024-12-11 06:14:32 | INFO | train | epoch 038 | loss 4.438 | nll_loss 3.432 | ppl 10.79 | wps 18738.5 | ups 6.78 | wpb 2762.6 | bsz 19.4 | num_updates 529 | lr 2.65447e-05 | gnorm 0.62 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 88\n",
            "2024-12-11 06:14:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 039:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:32 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2024-12-11 06:14:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  93% 13/14 [00:01<00:00, 10.77it/s]2024-12-11 06:14:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.70it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.06it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:34 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.415 | nll_loss 3.339 | ppl 10.12 | wps 30926.9 | wpb 1236.5 | bsz 8.8 | num_updates 543 | best_loss 4.415\n",
            "2024-12-11 06:14:34 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2024-12-11 06:14:34 | INFO | train | epoch 039 | loss 4.42 | nll_loss 3.409 | ppl 10.63 | wps 19845.9 | ups 7.18 | wpb 2762.6 | bsz 19.4 | num_updates 543 | lr 2.72446e-05 | gnorm 0.66 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 90\n",
            "2024-12-11 06:14:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 040:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:34 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2024-12-11 06:14:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  86% 12/14 [00:01<00:00,  9.99it/s]2024-12-11 06:14:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.36it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.23it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:36 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.404 | nll_loss 3.326 | ppl 10.03 | wps 33222.5 | wpb 1236.5 | bsz 8.8 | num_updates 557 | best_loss 4.404\n",
            "2024-12-11 06:14:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 557 updates\n",
            "2024-12-11 06:14:36 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint40.pt\n",
            "2024-12-11 06:14:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint40.pt\n",
            "2024-12-11 06:14:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint40.pt (epoch 40 @ 557 updates, score 4.404) (writing took 2.2623452129992074 seconds)\n",
            "2024-12-11 06:14:38 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2024-12-11 06:14:38 | INFO | train | epoch 040 | loss 4.4 | nll_loss 3.376 | ppl 10.38 | wps 8769.5 | ups 3.17 | wpb 2762.6 | bsz 19.4 | num_updates 557 | lr 2.79444e-05 | gnorm 0.646 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.1 | wall 95\n",
            "2024-12-11 06:14:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 041:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:38 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2024-12-11 06:14:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  86% 12/14 [00:01<00:00, 10.15it/s]2024-12-11 06:14:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.53it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.97it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:40 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.396 | nll_loss 3.301 | ppl 9.85 | wps 31383.5 | wpb 1236.5 | bsz 8.8 | num_updates 571 | best_loss 4.396\n",
            "2024-12-11 06:14:40 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2024-12-11 06:14:40 | INFO | train | epoch 041 | loss 4.389 | nll_loss 3.363 | ppl 10.29 | wps 17739.7 | ups 6.42 | wpb 2762.6 | bsz 19.4 | num_updates 571 | lr 2.86443e-05 | gnorm 0.661 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 97\n",
            "2024-12-11 06:14:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 042:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:40 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2024-12-11 06:14:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  93% 13/14 [00:01<00:00, 10.35it/s]2024-12-11 06:14:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.70it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:43 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.381 | nll_loss 3.279 | ppl 9.71 | wps 29672.7 | wpb 1236.5 | bsz 8.8 | num_updates 585 | best_loss 4.381\n",
            "2024-12-11 06:14:43 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2024-12-11 06:14:43 | INFO | train | epoch 042 | loss 4.368 | nll_loss 3.337 | ppl 10.1 | wps 18359.8 | ups 6.65 | wpb 2762.6 | bsz 19.4 | num_updates 585 | lr 2.93442e-05 | gnorm 0.634 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 99\n",
            "2024-12-11 06:14:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 043:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:43 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2024-12-11 06:14:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  93% 13/14 [00:01<00:00, 10.77it/s]2024-12-11 06:14:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.63it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.32it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:45 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.372 | nll_loss 3.274 | ppl 9.67 | wps 32160.8 | wpb 1236.5 | bsz 8.8 | num_updates 599 | best_loss 4.372\n",
            "2024-12-11 06:14:45 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2024-12-11 06:14:45 | INFO | train | epoch 043 | loss 4.356 | nll_loss 3.314 | ppl 9.95 | wps 19084.4 | ups 6.91 | wpb 2762.6 | bsz 19.4 | num_updates 599 | lr 3.0044e-05 | gnorm 0.639 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 101\n",
            "2024-12-11 06:14:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 044:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:45 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2024-12-11 06:14:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  86% 12/14 [00:01<00:00, 10.31it/s, loss=4.405, nll_loss=3.386, ppl=10.46, wps=15562.5, ups=5.62, wpb=2769.8, bsz=19.5, num_updates=600, lr=3.0094e-05, gnorm=0.643, clip=0, loss_scale=16, train_wall=9, gb_free=37.6, wall=102]2024-12-11 06:14:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.63it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.31it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:47 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.365 | nll_loss 3.261 | ppl 9.59 | wps 32143.1 | wpb 1236.5 | bsz 8.8 | num_updates 613 | best_loss 4.365\n",
            "2024-12-11 06:14:47 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2024-12-11 06:14:47 | INFO | train | epoch 044 | loss 4.344 | nll_loss 3.302 | ppl 9.86 | wps 18309.3 | ups 6.63 | wpb 2762.6 | bsz 19.4 | num_updates 613 | lr 3.07439e-05 | gnorm 0.654 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 103\n",
            "2024-12-11 06:14:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 045:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:47 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2024-12-11 06:14:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  86% 12/14 [00:01<00:00, 10.57it/s]2024-12-11 06:14:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.56it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:49 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.356 | nll_loss 3.257 | ppl 9.56 | wps 31770 | wpb 1236.5 | bsz 8.8 | num_updates 627 | best_loss 4.356\n",
            "2024-12-11 06:14:49 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2024-12-11 06:14:49 | INFO | train | epoch 045 | loss 4.33 | nll_loss 3.283 | ppl 9.73 | wps 18697.5 | ups 6.77 | wpb 2762.6 | bsz 19.4 | num_updates 627 | lr 3.14437e-05 | gnorm 0.615 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 105\n",
            "2024-12-11 06:14:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 046:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:49 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2024-12-11 06:14:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  86% 12/14 [00:01<00:00, 10.51it/s]2024-12-11 06:14:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  5.58it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:51 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.353 | nll_loss 3.233 | ppl 9.41 | wps 38161.2 | wpb 1236.5 | bsz 8.8 | num_updates 641 | best_loss 4.353\n",
            "2024-12-11 06:14:51 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2024-12-11 06:14:51 | INFO | train | epoch 046 | loss 4.323 | nll_loss 3.27 | ppl 9.65 | wps 19442.2 | ups 7.04 | wpb 2762.6 | bsz 19.4 | num_updates 641 | lr 3.21436e-05 | gnorm 0.624 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 107\n",
            "2024-12-11 06:14:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 047:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:51 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2024-12-11 06:14:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  93% 13/14 [00:01<00:00, 10.49it/s]2024-12-11 06:14:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.35it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  75% 3/4 [00:00<00:00, 10.78it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:53 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 4.349 | nll_loss 3.231 | ppl 9.39 | wps 21221.9 | wpb 1236.5 | bsz 8.8 | num_updates 655 | best_loss 4.349\n",
            "2024-12-11 06:14:53 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2024-12-11 06:14:53 | INFO | train | epoch 047 | loss 4.31 | nll_loss 3.255 | ppl 9.54 | wps 17970.5 | ups 6.5 | wpb 2762.6 | bsz 19.4 | num_updates 655 | lr 3.28435e-05 | gnorm 0.638 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.1 | wall 109\n",
            "2024-12-11 06:14:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 048:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:53 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2024-12-11 06:14:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  93% 13/14 [00:01<00:00, 10.82it/s]2024-12-11 06:14:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.84it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  75% 3/4 [00:00<00:00, 12.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:55 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 4.34 | nll_loss 3.224 | ppl 9.35 | wps 26084.2 | wpb 1236.5 | bsz 8.8 | num_updates 669 | best_loss 4.34\n",
            "2024-12-11 06:14:55 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2024-12-11 06:14:55 | INFO | train | epoch 048 | loss 4.307 | nll_loss 3.251 | ppl 9.52 | wps 18775.4 | ups 6.8 | wpb 2762.6 | bsz 19.4 | num_updates 669 | lr 3.35433e-05 | gnorm 0.653 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 111\n",
            "2024-12-11 06:14:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 049:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:55 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2024-12-11 06:14:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  93% 13/14 [00:01<00:00, 10.41it/s]2024-12-11 06:14:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.52it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.47it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:57 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 4.346 | nll_loss 3.213 | ppl 9.27 | wps 29654.8 | wpb 1236.5 | bsz 8.8 | num_updates 683 | best_loss 4.346\n",
            "2024-12-11 06:14:57 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2024-12-11 06:14:57 | INFO | train | epoch 049 | loss 4.295 | nll_loss 3.234 | ppl 9.41 | wps 17846.4 | ups 6.46 | wpb 2762.6 | bsz 19.4 | num_updates 683 | lr 3.42432e-05 | gnorm 0.671 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 38.1 | wall 113\n",
            "2024-12-11 06:14:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 050:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:14:57 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2024-12-11 06:14:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  93% 13/14 [00:01<00:00, 10.85it/s]2024-12-11 06:14:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.68it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.37it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:14:59 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 4.331 | nll_loss 3.214 | ppl 9.28 | wps 32132.8 | wpb 1236.5 | bsz 8.8 | num_updates 697 | best_loss 4.331\n",
            "2024-12-11 06:14:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 697 updates\n",
            "2024-12-11 06:14:59 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint50.pt\n",
            "2024-12-11 06:15:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint50.pt\n",
            "2024-12-11 06:15:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint50.pt (epoch 50 @ 697 updates, score 4.331) (writing took 2.2505738910003856 seconds)\n",
            "2024-12-11 06:15:01 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2024-12-11 06:15:01 | INFO | train | epoch 050 | loss 4.288 | nll_loss 3.223 | ppl 9.34 | wps 9149.1 | ups 3.31 | wpb 2762.6 | bsz 19.4 | num_updates 697 | lr 3.4943e-05 | gnorm 0.674 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 118\n",
            "2024-12-11 06:15:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 051:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:01 | INFO | fairseq.trainer | begin training epoch 51\n",
            "2024-12-11 06:15:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 051:  86% 12/14 [00:01<00:00, 10.23it/s, loss=4.312, nll_loss=3.258, ppl=9.56, wps=16324.9, ups=5.89, wpb=2769.7, bsz=19.4, num_updates=700, lr=3.5093e-05, gnorm=0.644, clip=0, loss_scale=16, train_wall=9, gb_free=37.6, wall=119]2024-12-11 06:15:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 051 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.25it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:03 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 4.331 | nll_loss 3.196 | ppl 9.16 | wps 35949.5 | wpb 1236.5 | bsz 8.8 | num_updates 711 | best_loss 4.331\n",
            "2024-12-11 06:15:03 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
            "2024-12-11 06:15:03 | INFO | train | epoch 051 | loss 4.279 | nll_loss 3.213 | ppl 9.27 | wps 18475.2 | ups 6.69 | wpb 2762.6 | bsz 19.4 | num_updates 711 | lr 3.56429e-05 | gnorm 0.648 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 120\n",
            "2024-12-11 06:15:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 052:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:03 | INFO | fairseq.trainer | begin training epoch 52\n",
            "2024-12-11 06:15:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 052:  86% 12/14 [00:01<00:00, 10.27it/s]2024-12-11 06:15:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 052 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:05 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 4.319 | nll_loss 3.197 | ppl 9.17 | wps 36079.1 | wpb 1236.5 | bsz 8.8 | num_updates 725 | best_loss 4.319\n",
            "2024-12-11 06:15:05 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
            "2024-12-11 06:15:05 | INFO | train | epoch 052 | loss 4.272 | nll_loss 3.202 | ppl 9.21 | wps 18829.3 | ups 6.82 | wpb 2762.6 | bsz 19.4 | num_updates 725 | lr 3.63428e-05 | gnorm 0.645 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.6 | wall 122\n",
            "2024-12-11 06:15:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 053:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:06 | INFO | fairseq.trainer | begin training epoch 53\n",
            "2024-12-11 06:15:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 053:  93% 13/14 [00:01<00:00, 10.69it/s]2024-12-11 06:15:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 053 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  5.79it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.60it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:07 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 4.322 | nll_loss 3.185 | ppl 9.09 | wps 33976.5 | wpb 1236.5 | bsz 8.8 | num_updates 739 | best_loss 4.322\n",
            "2024-12-11 06:15:08 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
            "2024-12-11 06:15:08 | INFO | train | epoch 053 | loss 4.262 | nll_loss 3.192 | ppl 9.14 | wps 19002.1 | ups 6.88 | wpb 2762.6 | bsz 19.4 | num_updates 739 | lr 3.70426e-05 | gnorm 0.633 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 124\n",
            "2024-12-11 06:15:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 054:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:08 | INFO | fairseq.trainer | begin training epoch 54\n",
            "2024-12-11 06:15:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 054:  86% 12/14 [00:01<00:00, 10.00it/s]2024-12-11 06:15:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 054 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.41it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.46it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:10 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 4.307 | nll_loss 3.183 | ppl 9.08 | wps 30123.9 | wpb 1236.5 | bsz 8.8 | num_updates 753 | best_loss 4.307\n",
            "2024-12-11 06:15:10 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
            "2024-12-11 06:15:10 | INFO | train | epoch 054 | loss 4.258 | nll_loss 3.181 | ppl 9.07 | wps 17820.7 | ups 6.45 | wpb 2762.6 | bsz 19.4 | num_updates 753 | lr 3.77425e-05 | gnorm 0.627 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 126\n",
            "2024-12-11 06:15:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 055:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:10 | INFO | fairseq.trainer | begin training epoch 55\n",
            "2024-12-11 06:15:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 055:  93% 13/14 [00:01<00:00, 10.72it/s]2024-12-11 06:15:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 055 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.44it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.50it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:12 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 4.313 | nll_loss 3.178 | ppl 9.05 | wps 33794.1 | wpb 1236.5 | bsz 8.8 | num_updates 767 | best_loss 4.313\n",
            "2024-12-11 06:15:12 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
            "2024-12-11 06:15:12 | INFO | train | epoch 055 | loss 4.249 | nll_loss 3.173 | ppl 9.02 | wps 17794.4 | ups 6.44 | wpb 2762.6 | bsz 19.4 | num_updates 767 | lr 3.84423e-05 | gnorm 0.603 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 128\n",
            "2024-12-11 06:15:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 056:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:12 | INFO | fairseq.trainer | begin training epoch 56\n",
            "2024-12-11 06:15:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 056:  93% 13/14 [00:01<00:00, 10.54it/s]2024-12-11 06:15:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 056 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.17it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:14 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 4.303 | nll_loss 3.168 | ppl 8.99 | wps 34827.2 | wpb 1236.5 | bsz 8.8 | num_updates 781 | best_loss 4.303\n",
            "2024-12-11 06:15:14 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
            "2024-12-11 06:15:14 | INFO | train | epoch 056 | loss 4.244 | nll_loss 3.167 | ppl 8.98 | wps 17761.8 | ups 6.43 | wpb 2762.6 | bsz 19.4 | num_updates 781 | lr 3.91422e-05 | gnorm 0.626 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 130\n",
            "2024-12-11 06:15:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 057:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:14 | INFO | fairseq.trainer | begin training epoch 57\n",
            "2024-12-11 06:15:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 057:  86% 12/14 [00:01<00:00, 10.39it/s]2024-12-11 06:15:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 057 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.70it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.08it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:16 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 4.305 | nll_loss 3.156 | ppl 8.91 | wps 31001 | wpb 1236.5 | bsz 8.8 | num_updates 795 | best_loss 4.305\n",
            "2024-12-11 06:15:16 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
            "2024-12-11 06:15:16 | INFO | train | epoch 057 | loss 4.234 | nll_loss 3.151 | ppl 8.88 | wps 18445.7 | ups 6.68 | wpb 2762.6 | bsz 19.4 | num_updates 795 | lr 3.98421e-05 | gnorm 0.612 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 133\n",
            "2024-12-11 06:15:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 058:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:16 | INFO | fairseq.trainer | begin training epoch 58\n",
            "2024-12-11 06:15:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 058:  93% 13/14 [00:01<00:00, 10.66it/s, loss=4.255, nll_loss=3.179, ppl=9.06, wps=18330.5, ups=6.67, wpb=2746.5, bsz=19.4, num_updates=800, lr=4.0092e-05, gnorm=0.629, clip=0, loss_scale=16, train_wall=9, gb_free=37.9, wall=133]2024-12-11 06:15:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 058 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.62it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.20it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:18 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 4.293 | nll_loss 3.158 | ppl 8.93 | wps 31754.9 | wpb 1236.5 | bsz 8.8 | num_updates 809 | best_loss 4.293\n",
            "2024-12-11 06:15:18 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
            "2024-12-11 06:15:18 | INFO | train | epoch 058 | loss 4.227 | nll_loss 3.14 | ppl 8.82 | wps 18535.5 | ups 6.71 | wpb 2762.6 | bsz 19.4 | num_updates 809 | lr 4.05419e-05 | gnorm 0.602 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 135\n",
            "2024-12-11 06:15:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 059:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:18 | INFO | fairseq.trainer | begin training epoch 59\n",
            "2024-12-11 06:15:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 059:  93% 13/14 [00:01<00:00, 10.65it/s]2024-12-11 06:15:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 059 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.58it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:20 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 4.293 | nll_loss 3.16 | ppl 8.94 | wps 32540 | wpb 1236.5 | bsz 8.8 | num_updates 823 | best_loss 4.293\n",
            "2024-12-11 06:15:20 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
            "2024-12-11 06:15:20 | INFO | train | epoch 059 | loss 4.224 | nll_loss 3.14 | ppl 8.82 | wps 19099.6 | ups 6.91 | wpb 2762.6 | bsz 19.4 | num_updates 823 | lr 4.12418e-05 | gnorm 0.609 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 137\n",
            "2024-12-11 06:15:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 060:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:20 | INFO | fairseq.trainer | begin training epoch 60\n",
            "2024-12-11 06:15:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 060:  86% 12/14 [00:01<00:00, 10.51it/s]2024-12-11 06:15:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 060 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.65it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:22 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 4.296 | nll_loss 3.151 | ppl 8.88 | wps 32118.9 | wpb 1236.5 | bsz 8.8 | num_updates 837 | best_loss 4.296\n",
            "2024-12-11 06:15:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 837 updates\n",
            "2024-12-11 06:15:22 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint60.pt\n",
            "2024-12-11 06:15:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint60.pt\n",
            "2024-12-11 06:15:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint60.pt (epoch 60 @ 837 updates, score 4.296) (writing took 2.2617599240002164 seconds)\n",
            "2024-12-11 06:15:25 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
            "2024-12-11 06:15:25 | INFO | train | epoch 060 | loss 4.217 | nll_loss 3.125 | ppl 8.72 | wps 8931.8 | ups 3.23 | wpb 2762.6 | bsz 19.4 | num_updates 837 | lr 4.19416e-05 | gnorm 0.61 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 141\n",
            "2024-12-11 06:15:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 061:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:25 | INFO | fairseq.trainer | begin training epoch 61\n",
            "2024-12-11 06:15:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 061:  93% 13/14 [00:01<00:00, 10.14it/s]2024-12-11 06:15:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 061 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.28it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:27 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 4.288 | nll_loss 3.148 | ppl 8.86 | wps 33131.9 | wpb 1236.5 | bsz 8.8 | num_updates 851 | best_loss 4.288\n",
            "2024-12-11 06:15:27 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
            "2024-12-11 06:15:27 | INFO | train | epoch 061 | loss 4.21 | nll_loss 3.122 | ppl 8.71 | wps 17212.7 | ups 6.23 | wpb 2762.6 | bsz 19.4 | num_updates 851 | lr 4.26415e-05 | gnorm 0.612 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 143\n",
            "2024-12-11 06:15:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 062:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:27 | INFO | fairseq.trainer | begin training epoch 62\n",
            "2024-12-11 06:15:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 062:  93% 13/14 [00:01<00:00, 10.56it/s]2024-12-11 06:15:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 062 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.41it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.62it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:29 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 4.291 | nll_loss 3.146 | ppl 8.85 | wps 30668.1 | wpb 1236.5 | bsz 8.8 | num_updates 865 | best_loss 4.291\n",
            "2024-12-11 06:15:29 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
            "2024-12-11 06:15:29 | INFO | train | epoch 062 | loss 4.205 | nll_loss 3.11 | ppl 8.63 | wps 17794.8 | ups 6.44 | wpb 2762.6 | bsz 19.4 | num_updates 865 | lr 4.33414e-05 | gnorm 0.616 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 145\n",
            "2024-12-11 06:15:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 063:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:29 | INFO | fairseq.trainer | begin training epoch 63\n",
            "2024-12-11 06:15:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 063:  93% 13/14 [00:01<00:00, 10.38it/s]2024-12-11 06:15:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 063 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.45it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:31 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 4.287 | nll_loss 3.144 | ppl 8.84 | wps 32289.4 | wpb 1236.5 | bsz 8.8 | num_updates 879 | best_loss 4.287\n",
            "2024-12-11 06:15:31 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
            "2024-12-11 06:15:31 | INFO | train | epoch 063 | loss 4.201 | nll_loss 3.107 | ppl 8.61 | wps 18354.5 | ups 6.64 | wpb 2762.6 | bsz 19.4 | num_updates 879 | lr 4.40412e-05 | gnorm 0.623 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 147\n",
            "2024-12-11 06:15:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 064:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:31 | INFO | fairseq.trainer | begin training epoch 64\n",
            "2024-12-11 06:15:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 064:  86% 12/14 [00:01<00:00, 10.30it/s]2024-12-11 06:15:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 064 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:33 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 4.285 | nll_loss 3.143 | ppl 8.84 | wps 36078.3 | wpb 1236.5 | bsz 8.8 | num_updates 893 | best_loss 4.285\n",
            "2024-12-11 06:15:33 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
            "2024-12-11 06:15:33 | INFO | train | epoch 064 | loss 4.197 | nll_loss 3.101 | ppl 8.58 | wps 18537.1 | ups 6.71 | wpb 2762.6 | bsz 19.4 | num_updates 893 | lr 4.47411e-05 | gnorm 0.668 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 150\n",
            "2024-12-11 06:15:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 065:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:33 | INFO | fairseq.trainer | begin training epoch 65\n",
            "2024-12-11 06:15:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 065:  93% 13/14 [00:01<00:00, 10.73it/s, loss=4.209, nll_loss=3.118, ppl=8.68, wps=16106.6, ups=5.83, wpb=2762.9, bsz=19.4, num_updates=900, lr=4.5091e-05, gnorm=0.623, clip=0, loss_scale=16, train_wall=9, gb_free=37.6, wall=151]2024-12-11 06:15:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 065 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.65it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.94it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:35 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 4.284 | nll_loss 3.137 | ppl 8.79 | wps 30754.2 | wpb 1236.5 | bsz 8.8 | num_updates 907 | best_loss 4.284\n",
            "2024-12-11 06:15:35 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
            "2024-12-11 06:15:35 | INFO | train | epoch 065 | loss 4.19 | nll_loss 3.095 | ppl 8.54 | wps 19252.3 | ups 6.97 | wpb 2762.6 | bsz 19.4 | num_updates 907 | lr 4.54409e-05 | gnorm 0.647 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 152\n",
            "2024-12-11 06:15:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 066:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:35 | INFO | fairseq.trainer | begin training epoch 66\n",
            "2024-12-11 06:15:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 066:  93% 13/14 [00:01<00:00, 10.90it/s]2024-12-11 06:15:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 066 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.55it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:37 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 4.28 | nll_loss 3.139 | ppl 8.81 | wps 30364.1 | wpb 1236.5 | bsz 8.8 | num_updates 921 | best_loss 4.28\n",
            "2024-12-11 06:15:37 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
            "2024-12-11 06:15:37 | INFO | train | epoch 066 | loss 4.184 | nll_loss 3.082 | ppl 8.47 | wps 19447.8 | ups 7.04 | wpb 2762.6 | bsz 19.4 | num_updates 921 | lr 4.61408e-05 | gnorm 0.619 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 154\n",
            "2024-12-11 06:15:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 067:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:37 | INFO | fairseq.trainer | begin training epoch 67\n",
            "2024-12-11 06:15:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 067:  93% 13/14 [00:01<00:00, 10.48it/s]2024-12-11 06:15:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 067 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.65it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.52it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:39 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 4.276 | nll_loss 3.136 | ppl 8.79 | wps 32829.3 | wpb 1236.5 | bsz 8.8 | num_updates 935 | best_loss 4.276\n",
            "2024-12-11 06:15:39 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
            "2024-12-11 06:15:39 | INFO | train | epoch 067 | loss 4.178 | nll_loss 3.075 | ppl 8.43 | wps 18854.7 | ups 6.83 | wpb 2762.6 | bsz 19.4 | num_updates 935 | lr 4.68407e-05 | gnorm 0.613 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 156\n",
            "2024-12-11 06:15:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 068:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:39 | INFO | fairseq.trainer | begin training epoch 68\n",
            "2024-12-11 06:15:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 068:  86% 12/14 [00:01<00:00, 10.06it/s]2024-12-11 06:15:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 068 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  5.76it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 15.59it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:41 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 4.273 | nll_loss 3.131 | ppl 8.76 | wps 30043.5 | wpb 1236.5 | bsz 8.8 | num_updates 949 | best_loss 4.273\n",
            "2024-12-11 06:15:41 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
            "2024-12-11 06:15:41 | INFO | train | epoch 068 | loss 4.174 | nll_loss 3.07 | ppl 8.4 | wps 18177.1 | ups 6.58 | wpb 2762.6 | bsz 19.4 | num_updates 949 | lr 4.75405e-05 | gnorm 0.649 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 158\n",
            "2024-12-11 06:15:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 069:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:41 | INFO | fairseq.trainer | begin training epoch 69\n",
            "2024-12-11 06:15:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 069:  93% 13/14 [00:01<00:00, 10.24it/s]2024-12-11 06:15:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 069 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  5.36it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:44 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 4.282 | nll_loss 3.129 | ppl 8.75 | wps 40632.2 | wpb 1236.5 | bsz 8.8 | num_updates 963 | best_loss 4.282\n",
            "2024-12-11 06:15:44 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
            "2024-12-11 06:15:44 | INFO | train | epoch 069 | loss 4.166 | nll_loss 3.061 | ppl 8.35 | wps 16969.6 | ups 6.14 | wpb 2762.6 | bsz 19.4 | num_updates 963 | lr 4.82404e-05 | gnorm 0.625 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 160\n",
            "2024-12-11 06:15:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 070:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:44 | INFO | fairseq.trainer | begin training epoch 70\n",
            "2024-12-11 06:15:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 070:  86% 12/14 [00:01<00:00, 10.02it/s]2024-12-11 06:15:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 070 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.44it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:46 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 4.277 | nll_loss 3.127 | ppl 8.74 | wps 29306 | wpb 1236.5 | bsz 8.8 | num_updates 977 | best_loss 4.277\n",
            "2024-12-11 06:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 977 updates\n",
            "2024-12-11 06:15:46 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint70.pt\n",
            "2024-12-11 06:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint70.pt\n",
            "2024-12-11 06:15:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint70.pt (epoch 70 @ 977 updates, score 4.277) (writing took 3.3609182159998454 seconds)\n",
            "2024-12-11 06:15:49 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
            "2024-12-11 06:15:49 | INFO | train | epoch 070 | loss 4.163 | nll_loss 3.055 | ppl 8.31 | wps 6957.7 | ups 2.52 | wpb 2762.6 | bsz 19.4 | num_updates 977 | lr 4.89402e-05 | gnorm 0.655 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 166\n",
            "2024-12-11 06:15:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 071:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:49 | INFO | fairseq.trainer | begin training epoch 71\n",
            "2024-12-11 06:15:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 071:  93% 13/14 [00:01<00:00, 10.45it/s]2024-12-11 06:15:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 071 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.74it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.36it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:51 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 4.26 | nll_loss 3.13 | ppl 8.75 | wps 28459.8 | wpb 1236.5 | bsz 8.8 | num_updates 991 | best_loss 4.26\n",
            "2024-12-11 06:15:51 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
            "2024-12-11 06:15:51 | INFO | train | epoch 071 | loss 4.158 | nll_loss 3.047 | ppl 8.27 | wps 18374.5 | ups 6.65 | wpb 2762.6 | bsz 19.4 | num_updates 991 | lr 4.96401e-05 | gnorm 0.658 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 168\n",
            "2024-12-11 06:15:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 072:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:51 | INFO | fairseq.trainer | begin training epoch 72\n",
            "2024-12-11 06:15:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 072:  86% 12/14 [00:01<00:00, 10.26it/s, loss=4.17, nll_loss=3.065, ppl=8.37, wps=15128.4, ups=5.45, wpb=2773.7, bsz=19.5, num_updates=1000, lr=5.009e-05, gnorm=0.634, clip=0, loss_scale=16, train_wall=9, gb_free=37.9, wall=169]2024-12-11 06:15:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 072 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.55it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.86it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:53 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 4.267 | nll_loss 3.122 | ppl 8.7 | wps 30913.5 | wpb 1236.5 | bsz 8.8 | num_updates 1005 | best_loss 4.267\n",
            "2024-12-11 06:15:53 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n",
            "2024-12-11 06:15:53 | INFO | train | epoch 072 | loss 4.151 | nll_loss 3.041 | ppl 8.23 | wps 18627.4 | ups 6.74 | wpb 2762.6 | bsz 19.4 | num_updates 1005 | lr 5.034e-05 | gnorm 0.629 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 170\n",
            "2024-12-11 06:15:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 073:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:53 | INFO | fairseq.trainer | begin training epoch 73\n",
            "2024-12-11 06:15:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 073:  86% 12/14 [00:01<00:00, 10.30it/s]2024-12-11 06:15:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 073 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.49it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:55 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 4.277 | nll_loss 3.116 | ppl 8.67 | wps 32269.1 | wpb 1236.5 | bsz 8.8 | num_updates 1019 | best_loss 4.277\n",
            "2024-12-11 06:15:55 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n",
            "2024-12-11 06:15:55 | INFO | train | epoch 073 | loss 4.15 | nll_loss 3.039 | ppl 8.22 | wps 18222.1 | ups 6.6 | wpb 2762.6 | bsz 19.4 | num_updates 1019 | lr 5.10398e-05 | gnorm 0.651 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 172\n",
            "2024-12-11 06:15:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 074:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:56 | INFO | fairseq.trainer | begin training epoch 74\n",
            "2024-12-11 06:15:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 074:  86% 12/14 [00:01<00:00, 10.29it/s]2024-12-11 06:15:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 074 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.27it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.94it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:15:58 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 4.268 | nll_loss 3.119 | ppl 8.69 | wps 32611.2 | wpb 1236.5 | bsz 8.8 | num_updates 1033 | best_loss 4.268\n",
            "2024-12-11 06:15:58 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n",
            "2024-12-11 06:15:58 | INFO | train | epoch 074 | loss 4.142 | nll_loss 3.027 | ppl 8.15 | wps 18414.3 | ups 6.67 | wpb 2762.6 | bsz 19.4 | num_updates 1033 | lr 5.17397e-05 | gnorm 0.642 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 174\n",
            "2024-12-11 06:15:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 075:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:15:58 | INFO | fairseq.trainer | begin training epoch 75\n",
            "2024-12-11 06:15:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 075:  86% 12/14 [00:01<00:00, 10.34it/s]2024-12-11 06:15:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 075 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.27it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.58it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:16:00 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.277 | nll_loss 3.107 | ppl 8.62 | wps 31174.6 | wpb 1236.5 | bsz 8.8 | num_updates 1047 | best_loss 4.277\n",
            "2024-12-11 06:16:00 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n",
            "2024-12-11 06:16:00 | INFO | train | epoch 075 | loss 4.138 | nll_loss 3.022 | ppl 8.13 | wps 18294.7 | ups 6.62 | wpb 2762.6 | bsz 19.4 | num_updates 1047 | lr 5.24395e-05 | gnorm 0.708 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.7 | wall 176\n",
            "2024-12-11 06:16:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 076:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:16:00 | INFO | fairseq.trainer | begin training epoch 76\n",
            "2024-12-11 06:16:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 076:  86% 12/14 [00:01<00:00, 10.34it/s]2024-12-11 06:16:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 076 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.06it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.50it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:16:02 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 4.27 | nll_loss 3.113 | ppl 8.65 | wps 32004.4 | wpb 1236.5 | bsz 8.8 | num_updates 1061 | best_loss 4.27\n",
            "2024-12-11 06:16:02 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
            "2024-12-11 06:16:02 | INFO | train | epoch 076 | loss 4.127 | nll_loss 3.006 | ppl 8.03 | wps 18229.5 | ups 6.6 | wpb 2762.6 | bsz 19.4 | num_updates 1061 | lr 5.31394e-05 | gnorm 0.681 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 178\n",
            "2024-12-11 06:16:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 077:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:16:02 | INFO | fairseq.trainer | begin training epoch 77\n",
            "2024-12-11 06:16:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 077:  86% 12/14 [00:01<00:00, 10.12it/s]2024-12-11 06:16:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 077 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.80it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:16:04 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 4.26 | nll_loss 3.107 | ppl 8.62 | wps 31204.8 | wpb 1236.5 | bsz 8.8 | num_updates 1075 | best_loss 4.26\n",
            "2024-12-11 06:16:04 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
            "2024-12-11 06:16:04 | INFO | train | epoch 077 | loss 4.122 | nll_loss 2.997 | ppl 7.98 | wps 17770.8 | ups 6.43 | wpb 2762.6 | bsz 19.4 | num_updates 1075 | lr 5.38393e-05 | gnorm 0.646 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 180\n",
            "2024-12-11 06:16:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 078:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:16:04 | INFO | fairseq.trainer | begin training epoch 78\n",
            "2024-12-11 06:16:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 078:  93% 13/14 [00:01<00:00, 10.74it/s]2024-12-11 06:16:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 078 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.23it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:16:06 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 4.262 | nll_loss 3.113 | ppl 8.65 | wps 34835.6 | wpb 1236.5 | bsz 8.8 | num_updates 1089 | best_loss 4.262\n",
            "2024-12-11 06:16:06 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
            "2024-12-11 06:16:06 | INFO | train | epoch 078 | loss 4.121 | nll_loss 3 | ppl 8 | wps 18617.6 | ups 6.74 | wpb 2762.6 | bsz 19.4 | num_updates 1089 | lr 5.45391e-05 | gnorm 0.648 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.5 | wall 182\n",
            "2024-12-11 06:16:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 079:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:16:06 | INFO | fairseq.trainer | begin training epoch 79\n",
            "2024-12-11 06:16:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 079:  86% 12/14 [00:01<00:00, 10.50it/s, loss=4.132, nll_loss=3.014, ppl=8.08, wps=18486.4, ups=6.7, wpb=2760.1, bsz=19.4, num_updates=1100, lr=5.5089e-05, gnorm=0.66, clip=0, loss_scale=16, train_wall=9, gb_free=37.7, wall=184]2024-12-11 06:16:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 079 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.52it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 16.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:16:08 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 4.256 | nll_loss 3.1 | ppl 8.58 | wps 28508 | wpb 1236.5 | bsz 8.8 | num_updates 1103 | best_loss 4.256\n",
            "2024-12-11 06:16:08 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
            "2024-12-11 06:16:08 | INFO | train | epoch 079 | loss 4.113 | nll_loss 2.988 | ppl 7.93 | wps 19036.6 | ups 6.89 | wpb 2762.6 | bsz 19.4 | num_updates 1103 | lr 5.5239e-05 | gnorm 0.653 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.8 | wall 185\n",
            "2024-12-11 06:16:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 14\n",
            "epoch 080:   0% 0/14 [00:00<?, ?it/s]2024-12-11 06:16:08 | INFO | fairseq.trainer | begin training epoch 80\n",
            "2024-12-11 06:16:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 080:  86% 12/14 [00:01<00:00, 10.49it/s]2024-12-11 06:16:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 080 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.51it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-12-11 06:16:10 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 4.262 | nll_loss 3.106 | ppl 8.61 | wps 32532.1 | wpb 1236.5 | bsz 8.8 | num_updates 1117 | best_loss 4.262\n",
            "2024-12-11 06:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 1117 updates\n",
            "2024-12-11 06:16:10 | INFO | fairseq.trainer | Saving checkpoint to /content/models/checkpoint80.pt\n",
            "2024-12-11 06:16:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/models/checkpoint80.pt\n",
            "2024-12-11 06:16:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/models/checkpoint80.pt (epoch 80 @ 1117 updates, score 4.262) (writing took 2.3862047150014405 seconds)\n",
            "2024-12-11 06:16:13 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
            "2024-12-11 06:16:13 | INFO | train | epoch 080 | loss 4.109 | nll_loss 2.984 | ppl 7.91 | wps 8764.8 | ups 3.17 | wpb 2762.6 | bsz 19.4 | num_updates 1117 | lr 5.59388e-05 | gnorm 0.673 | clip 0 | loss_scale 16 | train_wall 1 | gb_free 37.9 | wall 189\n",
            "2024-12-11 06:16:13 | INFO | fairseq_cli.train | done training in 188.3 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate /content/data_root \\\n",
        "  --config-yaml config.yaml  \\\n",
        "  --task speech_to_speech --target-is-code --target-code-size 100 --vocoder code_hifigan \\\n",
        "  --path /content/models/checkpoint_best.pt  --gen-subset test \\\n",
        "  --max-tokens 50000 \\\n",
        "  --beam 10 --max-len-a 1 \\\n",
        "  --results-path /content/results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffAP0Dy8QRw7",
        "outputId": "de8b675a-8497-4f73-f017-4eb2a312ce06"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 06:17:57.461652: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 06:17:57.478867: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 06:17:57.499603: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 06:17:57.505838: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 06:17:57.520541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 06:17:58.641855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-11 06:18:00 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-12-11 06:18:02 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': '/content/results'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 50000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 50000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'nbest': 1, 'max_len_a': 1.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='speech_to_speech', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=50000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=50000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, path='/content/models/checkpoint_best.pt', post_process=None, quiet=False, model_overrides='{}', results_path='/content/results', beam=10, nbest=1, max_len_a=1.0, max_len_b=200, min_len=1, match_source_len=False, unnormalized=False, no_early_stop=False, no_beamable_mm=False, lenpen=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, constraints=None, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, print_alignment=None, print_step=False, lm_path=None, lm_weight=0.0, iter_decode_eos_penalty=0.0, iter_decode_max_iter=10, iter_decode_force_max_iter=False, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, retain_iter_history=False, retain_dropout=False, retain_dropout_modules=None, decoding_format=None, no_seed_provided=False, eos_token=None, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, arch='wav2vec2', data='/content/data_root', config_yaml='config.yaml', max_source_positions=6000, max_target_positions=1024, target_is_code=True, target_code_size=100, n_frames_per_step=1, multitask_config_yaml=None, eval_inference=False, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='code_hifigan', spec_bwd_max_iter=8, infer_target_lang='', force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, _name='speech_to_speech'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-12-11 06:18:02 | INFO | fairseq.tasks.speech_to_speech | dictionary size: 104\n",
            "2024-12-11 06:18:03 | INFO | fairseq_cli.generate | loading model(s) from /content/models/checkpoint_best.pt\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "2024-12-11 06:18:03 | INFO | fairseq.data.audio.speech_to_text_dataset | 'test' has 0.00% OOV\n",
            "2024-12-11 06:18:03 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToSpeechDataset(split=\"test\", n_samples=34, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 06:18:03 | INFO | fairseq.data.audio.speech_to_speech_dataset | SpeechToSpeechDataset(split=\"test\", n_samples=34, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), n_frames_per_step=1\n",
            "2024-12-11 06:18:08 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-12-11 06:18:08 | INFO | fairseq_cli.generate | Translated 34 sentences (3,851 tokens) in 3.6s (9.42 sentences/s, 1066.52 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep \"^D\\-\" /content/results/generate-test.txt | \\\n",
        "  sed 's/^D-//ig' | sort -nk1 | cut -f3 \\\n",
        "  > /content/results/generate-test.unit\n",
        "\n",
        "!PYTHONPATH=/content/fairseq python /content/fairseq/examples/speech_to_speech/generate_waveform_from_code.py \\\n",
        "  --in-code-file /content/results/generate-test.unit \\\n",
        "  --vocoder /content/g_00500000 --vocoder-cfg /content/config.json  \\\n",
        "  --results-path /content/results --dur-prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etqPBnXrRkgr",
        "outputId": "e154324d-acfa-4294-c05f-131f0be402a8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-11 06:18:41.035861: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-11 06:18:41.052813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 06:18:41.073437: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 06:18:41.079658: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 06:18:41.094273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 06:18:42.151683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-11 06:18:44 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-12-11 06:18:45 | INFO | __main__ | Namespace(in_code_file='/content/results/generate-test.unit', vocoder='/content/g_00500000', vocoder_cfg='/content/config.json', results_path='/content/results', dur_prediction=True, speaker_id=-1, cpu=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/content/fairseq/fairseq/models/text_to_speech/vocoder.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(checkpoint_path)\n",
            "Removing weight norm...\n",
            "2024-12-11 06:18:45 | INFO | fairseq.models.text_to_speech.vocoder | loaded CodeHiFiGAN checkpoint from /content/g_00500000\n",
            "100% 34/34 [00:02<00:00, 11.57it/s]\n"
          ]
        }
      ]
    }
  ]
}
